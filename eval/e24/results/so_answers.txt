===Answer 273776===
wget -r --no-parent http://mysite.com/configs/.vim/

--------------
wget -r --no-parent --reject "index.html*" http://mysite.com/configs/.vim/

--------------
===Answer 4572158===
===Answer 1078539===
-P prefix
--directory-prefix=prefix
           Set directory prefix to prefix.  The directory prefix is the
           directory where all other files and sub-directories will be
           saved to, i.e. the top of the retrieval tree.  The default
           is . (the current directory).

--------------
===Answer 1432161===
# Log in to the server.  This can be done only once.                   
wget --save-cookies cookies.txt \
     --post-data 'user=foo&password=bar' \
     http://server.com/auth.php

# Now grab the page or pages we care about.
wget --load-cookies cookies.txt \
     -p http://server.com/interesting/article.php

--------------
===Answer 17094214===
===Answer 11211812===
use_proxy=yes
http_proxy=127.0.0.1:8080

--------------
wget ... -e use_proxy=yes -e http_proxy=127.0.0.1:8080 ...

--------------
===Answer 4944353===
-nc, --no-clobber              skip downloads that would download to 
                               existing files.

--------------
===Answer 13397108===
aria2c -x 16 [url]
#          |
#          |
#          |
#          ----> the number of connections 

--------------
===Answer 5335576===
wget -r -nH --cut-dirs=2 --no-parent --reject="index.html*" http://mysite.com/dir1/dir2/data

--------------
===Answer 16840827===
wget -N http://site.org/images/misc/pic.png

--------------
===Answer 2291557===
-T seconds
--timeout=seconds

--------------
--dns-timeout=seconds

--------------
--connect-timeout=seconds

--------------
--read-timeout=seconds

--------------
wget -O - -q -t 1 --timeout=600 http://www.example.com/cron/run

--------------
===Answer 13519665===
wget -e robots=off http://www.example.com/

--------------
===Answer 21039148===
curl -O <url>

--------------
wget <url>

--------------
alias wget='curl -O'

--------------
===Answer 2662955===
===Answer 7645245===
wget -r -np -N [url] &
wget -r -np -N [url] &
wget -r -np -N [url] &
wget -r -np -N [url]

--------------
===Answer 18722273===
$ export http_proxy=http://proxy_host:proxy_port

--------------
$ export http_proxy=http://username:password@proxy_host:proxy_port

--------------
$ wget fileurl

--------------
===Answer 11211792===
# You can set the default proxies for Wget to use for http, https, and ftp.
# They will override the value in the environment.
#https_proxy = http://proxy.yoyodyne.com:18023/
#http_proxy = http://proxy.yoyodyne.com:18023/
#ftp_proxy = http://proxy.yoyodyne.com:18023/

# If you do not want to use proxy at all, set this to off.
#use_proxy = on

--------------
===Answer 2291556===
-T seconds
--timeout=seconds

--------------
-t number
--tries=number

--------------
===Answer 14894734===
===Answer 24444698===
aria2c -x 16 -s 16 [url]
#          |    |
#          |    |
#          |    |
#          ---------> the number of connections enter code here

--------------
===Answer 4944374===
===Answer 19767581===
nano /usr/bin/wget

--------------
#!/bin/bash
curl -L $1 -o $2

--------------
chmod 777 /usr/bin/wget

--------------
===Answer 4572162===
curl http://127.0.0.1:8000 -o index.html

--------------
===Answer 19719875===
export http_proxy=http://your_ip_proxy:port/
export https_proxy=$http_proxy
export ftp_proxy=$http_proxy
export dns_proxy=$http_proxy
export rsync_proxy=$http_proxy
export no_proxy="localhost,127.0.0.1,localaddress,.localdomain.com"

--------------
===Answer 22286581===
===Answer 4446912===
===Answer 5308977===
===Answer 15988883===
===Answer 17337704===
wget http://example.com/textfile.txt

--------------
curl http://example.com/textfile.txt -o textfile.txt

--------------
===Answer 21983475===
wget -m http://example.com/configs/.vim/

--------------
wget -m -e robots=off --no-parent http://example.com/configs/.vim/

--------------
===Answer 273755===
wget -r http://mysite.com/configs/.vim/

--------------
===Answer 1324467===
wget http://username:password@www.domain.com/page.html

--------------
===Answer 13956225===
ignore_user_abort(TRUE);

--------------
===Answer 18551185===
aria2c -x 16 [url] 

--------------
aria2c -j 5 [url] [url2]

--------------
===Answer 23454894===
===Answer 273757===
wget -r http://stackoverflow.com/

--------------
===Answer 11500025===
===Answer 19232108===
===Answer 2663023===
===Answer 4230894===
===Answer 4572161===
===Answer 16587702===
http://<host>/downloads/good
http://<host>/downloads/bad

--------------
wget --include downloads/good --mirror --execute robots=off --no-host-directories --cut-dirs=1 --reject="index.html*" --continue http://<host>/downloads/good

--------------
===Answer 18399572===
wget --mirror www.example.com

--------------
wget --mirror example.com

--------------
wget --mirror --domains=example.com example.com

--------------
===Answer 7286374===
curl -o index.html http://127.0.0.1:8080

--------------
===Answer 24630741===
set http_proxy=http://127.0.0.1:8888
set https_proxy=http://127.0.0.1:8888

--------------
===Answer 25602166===
===Answer 2662956===
===Answer 4230854===
===Answer 7440481===
===Answer 19775257===
wget --login yourlogin --password yourpassword yoururl

--------------
===Answer 25301809===
===Answer 26478435===
wget -r --user=prayagupd --password='/aGBLB8HTPx8c' --no-parent http://pseudononymous.com/repos/projects/k2c/docs/requirements/

--------------
===Answer 26995753===
===Answer 4602181===
wget -r -P /save/location -A jpeg,jpg,bmp,gif,png http://www.domain.com

--------------
===Answer 7843316===
wget -nd -r -l 2 -A jpg,jpeg,png,gif http://t.co

--------------
wget -nd -H -p -A jpg,jpeg,png,gif -e robots=off example.tumblr.com/page/{1..2}

--------------
===Answer 3410742===
===Answer 13193540===
===Answer 4947295===
`wget -nd -r -P /save/location/ -A jpeg,jpg,bmp,gif,png http://www.domain.com`

--------------
===Answer 4527472===
wget -q -O /dev/stdout <URL>

--------------
===Answer 5888528===
wget -r -l1 --no-parent -A.cpp http://abc.com/programs/

--------------
===Answer 9314479===
wget --quiet -O - http://bit.ly/rQyhG5 \
  | sed -n -e 's!.*<title>\(.*\)</title>.*!\1!p'

--------------
wget --quiet -O - http://bit.ly/rQyhG5 \
  | paste -s -d " "  \
  | sed -e 's!.*<head>\(.*\)</head>.*!\1!' \
  | sed -e 's!.*<title>\(.*\)</title>.*!\1!'

--------------
<head profile="http://gmpg.org/xfn/11">

--------------
wget --quiet -O - http://bit.ly/rQyhG5 \
  | paste -s -d " "  \
  | sed -e 's!.*<head[^>]*>\(.*\)</head>.*!\1!' \
  | sed -e 's!.*<title>\(.*\)</title>.*!\1!'

--------------
wget --quiet -O - http://bit.ly/rQyhG5 \
  | paste -s -d " "  \
  | sed -n -e 's!.*<head[^>]*>\(.*\)</head>.*!\1!p' \
  | sed -n -e 's!.*<title>\(.*\)</title>.*!\1!p'

--------------
wget --quiet -O - http://bit.ly/rQyhG5 \
  | sed -n -e 'H;${x;s!.*<head[^>]*>\(.*\)</head>.*!\1!;T;s!.*<title>\(.*\)</title>.*!\1!p}'

--------------
wget --quiet -O - http://bit.ly/rQyhG5 \
  | sed -n -e 'H;${x;s!.*<head[^>]*>\(.*\)</head>.*!\1!;tnext;b;:next;s!.*<title>\(.*\)</title>.*!\1!p}'

--------------
wget --quiet -O - http://bit.ly/rQyhG5 \
  | sed -n -e 'H;${x;s!.*<head[^>]*>\(.*\)</head>.*!\1!;tnext};b;:next;s!.*<title>\(.*\)</title>.*!\1!p'

--------------
cat << EOF > script
H
\$x
\$s!.*<head[^>]*>\(.*\)</head>.*!\1!
\$tnext
b
:next
s!.*<title>\(.*\)</title>.*!\1!p
EOF
wget --quiet -O - http://bit.ly/rQyhG5 \
  | sed -n -f script

--------------
===Answer 10873995===
===Answer 3376524===
wget -k -m -r -q -t 1 -O - http://www.web.com/ | sed 's/cat/dog/g' > output.html

--------------
===Answer 3410780===
===Answer 16676597===
$ cygcheck -f $( which wget )
wget-1.13.4-1

--------------
===Answer 3376506===
find www.web.com -type f -exec sed -i 's/word1\|word2\|word3//ig' {} +

--------------
===Answer 4528240===
OUTPUT=$(curl www.google.com)
echo $OUTPUT

--------------
===Answer 5888471===
===Answer 15159772===
===Answer 25115620===
`wget -r -P /save/location -A jpeg,jpg,bmp,gif,png http://www.boia.de --cut-dirs=1 --cut-dirs=2 --cut-dirs=3` 

--------------
===Answer 28204496===
lynx -dump example.com | sed '2q;d'

--------------
===Answer 21089847===
===Answer 13441207===
-r: recursive retrieving
-l1: sets the maximum recursion depth to be 1
--no-parent: does not ascend to the parent; only downloads from the specified subdirectory and downwards hierarchy

--------------
===Answer 1078475===
linkdiscoverer | wget -i - ./directory

--------------
===Answer 16718718===
open "......" r+

--------------
===Answer 18014631===
===Answer 21875199===
===Answer 1078487===
‘-i file’
‘--input-file=file’
    Read urls from file. If ‘-’ is specified as file, urls are read from the standard input. (Use ‘./-’ to read from a file literally named ‘-’.)

--------------
===Answer 12756121===
===Answer 20309938===
===Answer 21271724===
===Answer 25942448===

wget -r -l1 --no-parent http://www.domain.com/subdirectory/

--------------

wget -r --no-parent http://www.domain.com/subdirectory/

--------------
===Answer 26143792===
===Answer 27060952===
===Answer 27627866===
while [ true ] ; do
    urlfile=$( ls /root/wget/wget-download-link.txt | head -n 1 )
    dir=$( cat /root/wget/wget-dir.txt )
    if [ "$urlfile" = "" ] ; then
        sleep 180
        continue
    fi

    url=$( head -n 1 $urlfile )
    if [ "$url" = "" ] ; then
        mv $urlfile $urlfile.invalid
        continue
    fi

    mv $urlfile $urlfile.busy
    wget $url -P $dir -o /www/wget.log -c -t 100 -nc
    mv $urlfile.busy $urlfile.done
done

--------------
===Answer 17699948===
 --post-data=string
       --post-file=file
           Use POST as the method for all HTTP requests and send the specified data
           in the request body.  --post-data sends string as data, whereas
           --post-file sends the contents of file.  Other than that, they work in
           exactly the same way. In particular, they both expect content of the
           form "key1=value1&key2=value2", with percent-encoding for special
           characters; the only difference is that one expects its content as a
           command-line parameter and the other accepts its content from a file. In
           particular, --post-file is not for transmitting files as form
           attachments: those must appear as "key=value" data (with appropriate
           percent-coding) just like everything else. Wget does not currently
           support "multipart/form-data" for transmitting POST data; only
           "application/x-www-form-urlencoded". Only one of --post-data and
           --post-file should be specified.

--------------
===Answer 6323906===
===Answer 9430998===
string page;
using(var client = new WebClient()) {
    client.Credentials = new NetworkCredential("user", "password");
    page = client.DownloadString("http://www.example.com/");
}

--------------
===Answer 6310225===
===Answer 9809896===
GET /oneiric/lt-origen-oneiric/20120321/0/images/hwpack/hwpack_linaro-lt-origen_20120321-0_armel_supported.tar.gz HTTP/1.1

--------------
Set-Cookie: downloadrequested=/oneiric/lt-origen-oneiric/20120321/0/images/hwpack/hwpack_linaro-lt-origen_20120321-0_armel_supported.tar.gz; path=/; domain=.snapshots.linaro.org
Location: http://snapshots.linaro.org/licenses/samsung-v2.html

--------------
GET /licenses/samsung-accepted.html HTTP/1.1

--------------
Set-Cookie: samsunglicenseaccepted-v1=true; path=/oneiric/lt-origen-oneiric/20120321/0/images/hwpack/; domain=.snapshots.linaro.org; expires=Wed, 21-Mar-2012 17:37:57 GMT
Location: http://snapshots.linaro.org/oneiric/lt-origen-oneiric/20120321/0/images/hwpack/hwpack_linaro-lt-origen_20120321-0_armel_supported.tar.gz

--------------
===Answer 16496406===
wget "http://osu.ppy.sh/pages/include/profile-history.php?u=2330158&m=0"

--------------
===Answer 6323749===
===Answer 7913813===
===Answer 26976653===
===Answer 6264834===
wget -q --spider address
echo $?

--------------
===Answer 6264849===
===Answer 17638586===
 --reject jpg,png  --accept html

--------------
===Answer 7623766===
wget http://www.icerts.com/images/logo.jpg --header "Referer: www.icerts.com"

--------------
--2011-10-02 02:00:18--  http://www.icerts.com/images/logo.jpg
Résolution de www.icerts.com (www.icerts.com)... 97.74.86.3
Connexion vers www.icerts.com (www.icerts.com)|97.74.86.3|:80...connecté.
requête HTTP transmise, en attente de la réponse...200 OK
Longueur: 6102 (6,0K) [image/jpeg]
Sauvegarde en : «logo.jpg»

--------------
===Answer 12661261===
wget -q "http://blah.meh.com/my/path" -O /dev/null

--------------
===Answer 15465184===
wget -4 http://www.php.net/get/php-5.4.13.tar.gz/from/this/mirror

--------------
===Answer 6264924===
wget --spider www.bluespark.co.nz

--------------
Resolving www.bluespark.co.nz... 210.48.79.121
Connecting to www.bluespark.co.nz[210.48.79.121]:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: unspecified [text/html]
200 OK

--------------
===Answer 6293305===
wget -E http://whatever.url.example.com/x/y/z/foo.aspx

--------------
wget -Ep --convert-links http://whatever.url.example.com/x/y/z/foo.aspx

--------------
===Answer 10879403===
===Answer 14894098===
import urllib2
with open('list.txt') as my_list:
    for line in my_list:
        response = urllib2.urlopen(line)
        html = response.read()
        # now process the page's source

--------------
===Answer 20576549===
wget -v -t 2 --timeout 10

--------------
timeout 10  wget -v -t 2

--------------
( cmdpid=$BASHPID; (sleep 10; kill $cmdpid) & exec wget -v -t 2 )

--------------
===Answer 6293200===
===Answer 10548212===
--reject swf, mpeg

--------------
===Answer 10885479===
$ wget \
     --recursive \
     --no-clobber \
     --page-requisites \
     --html-extension \
     --convert-links \
     --restrict-file-names=windows \
     --domains example.org \
     --no-parent \
         www.example.org/tutorials/html/

--------------
===Answer 26620104===
   --timeout=seconds
       Set the network timeout to seconds seconds.  This is equivalent to
       specifying --dns-timeout, --connect-timeout, and --read-timeout,
       all at the same time.

       When interacting with the network, Wget can check for timeout and
       abort the operation if it takes too long.  This prevents anomalies
       like hanging reads and infinite connects.  The only timeout enabled
       by default is a 900-second read timeout.  Setting a timeout to 0
       disables it altogether.  Unless you know what you are doing, it is
       best not to change the default timeout settings.

       All timeout-related options accept decimal values, as well as sub-
       second values.  For example, 0.1 seconds is a legal (though unwise)
       choice of timeout.  Subsecond timeouts are useful for checking
       server response times or for testing network latency.

--------------
===Answer 8574081===
url=http://pics.sitename.com/images/191211/mxKL17DdgUhcr.jpg
filename=$(basename "$url")
wget "$url"

--------------
===Answer 14313360===
wget --server-response -q -O - "https://very.long/url/here" 2>&1 | 
  grep "Content-Disposition:" | tail -1 | 
  awk 'match($0, /filename=(.+)/, f){ print f[1] }' )

--------------
===Answer 8574079===
wget -O myfile.html http://www.example.com/

--------------
===Answer 10415296===
===Answer 2497809===
wget 'http://www.google.com/' -O my-output-file.html

--------------
===Answer 8961877===
===Answer 12978956===
===Answer 8574084===
url='http://pics.sitename.com/images/191211/mxKL17DdgUhcr.jpg'
file=`basename "$url"`
wget "$url" -O "$file"

--------------
===Answer 2497836===
$data=file_get_contents('http://www.google.com/');
file_put_contents($data,"./downloads/output.html");

--------------
===Answer 2965710===
===Answer 8574100===
~ $ URL='http://pics.sitename.com/images/191211/mxKL17DdgUhcr.jpg'
~ $ echo ${URL##*/}
mxKL17DdgUhcr.jpg
~ $ wget $URL -O ${URL##*/}
--18:34:26--  http://pics.sitename.com/images/191211/mxKL17DdgUhcr.jpg
           => `mxKL17DdgUhcr.jpg'

--------------
===Answer 16980461===
URL="http://www.example.com/ESTAD%C3%8DSTICA(2012).pdf"
BASE=$(basename ${URL})             # ESTAD%C3%8DSTICA(2012).pdf
FILE=$(printf '%b' ${BASE//%/\\x})  # ESTADÍSTICA(2012).pdf
wget ${URL}

--------------
===Answer 18328193===
 wget --no-check-certificate https://dl.dropboxusercontent.com/u/60455970/litecoin-0.6.3c-linux.tar.gz

--------------
===Answer 2497837===
system('/usr/bin/wget -q --directory-prefix="./downloads/" http://www.google.com/');
$filename = system('ls -tr ./downloads'); // $filename is now index.html

--------------
===Answer 5913507===
<?php
system('/usr/bin/wget -q --directory-prefix="./downloads/" http://www.google.com/');
$dir = "./downloads";

$newstamp = 0;
$newname = "";
$dc = opendir($dir);
while ($fn = readdir($dc)) {
  # Eliminate current directory, parent directory
  if (ereg('^\.{1,2}$',$fn)) continue;
  $timedat = filemtime("$dir/$fn");
  if ($timedat > $newstamp) {
    $newstamp = $timedat;
    $newname = $fn;
  }
}
// $newname contains the name of the most recently updated file
// $newstamp contains the time of the update to $newname
?>

--------------
===Answer 8574099===
echo ${url##*/}

--------------
===Answer 15644326===
===Answer 23041177===
===Answer 24326399===
0 * * * * curl -o "/Users/me/Downloads/example.txt" "http://www.example.com/example.txt"

--------------
===Answer 29253805===
wget --mirror --convert-links --adjust-extension \
--page-requisites --span-hosts \
--accept-regex '^http://www\.example\.com/docs|\.(js|css|png|jpeg|jpg|svg)$' \
http://www.example.com/docs

--------------
^http://www\.example\.org/docs|\.([Jj][Ss]|[Cc][Ss][Ss]|[Pp][Nn][Gg]|[Jj]
[Pp][Ee]?[Gg]|[Ss][Vv][Gg]|[Gg][Ii][Ff]|[Tt][Tt][Ff]|[Oo][Tt][Ff]|[Ww]
[Oo][Ff][Ff]2?|[Ee][Oo][Tt])(\?.*)?$

--------------
===Answer 8575028===
echo -n "Give me the name of file in http://pics.sitename.com/images/191211/ :"

read $string

sudo wget http://pics.sitename.com/images/191211/$string ;;

--------------
===Answer 20318469===
(ulimit -f 102400; wget $url)

--------------
(ulimit -f 204800; wget $url)

--------------
===Answer 9830332===
===Answer 20318140===
===Answer 16979558===
$ host www.d-centralize.nl
www.d-centralize.nl is an alias for ghs.google.com.
ghs.google.com is an alias for ghs.l.google.com.
ghs.l.google.com has address 173.194.69.121
ghs.l.google.com has IPv6 address 2a00:1450:4008:c01::79

--------------
===Answer 12501542===
===Answer 16765470===
===Answer 18948228===
===Answer 19895411===
for /f %%a in (proxylist.txt) do wget http://www.google.com -U Mozilla/5.0 (Windows NT 6.1; WOW64; rv:20.0) Gecko/20100101 Firefox/20.0 -P "" --cookies=off --no-http-keep-alive -e use_proxy=yes --max-redirect 0 -t 1 --timeout=5 -e http_proxy=%%a

--------------
===Answer 22013384===
===Answer 17671183===
wget -d --header="X-Auth-Token: eaaafd18-0fed-4b3a-81b4-663c99ec1cbb" http://ipadress/get/v1/AUTH_test/test/test.jpg

--------------
===Answer 17671192===
wget --header='X-Auth-Token: eaaafd18-0fed-4b3a-81b4-663c99ec1cbb' -A ...

--------------
===Answer 20317994===
===Answer 26826818===
# wget http://google.com/
Connecting to google.com (212.188.7.49:80)
Connecting to www.google.ru (64.233.161.94:80)
index.html           100% |*****************| 18381   0:00:00 ETA
# wget http://google.com/
Connecting to google.com (212.188.7.50:80)
Connecting to www.google.ru (64.233.161.94:80)
wget: can't open 'index.html': File exists
#

--------------
===Answer 2467633===
===Answer 6429118===
===Answer 13389501===
 import urllib
 urllib.urlretrieve("http://google.com/index.html", filename="local/index.html")

--------------
===Answer 2467634===
===Answer 2467646===


#!/usr/bin/python

import sys
import threading
import urllib
from Queue import Queue
import logging

class Downloader(threading.Thread):
    def __init__(self, queue):
        super(Downloader, self).__init__()
        self.queue = queue

    def run(self):
        while True:
            download_url, save_as = queue.get()
            # sentinal
            if not download_url:
                return
            try:
                urllib.urlretrieve(download_url, filename=save_as)
            except Exception, e:
                logging.warn("error downloading %s: %s" % (download_url, e))

if __name__ == '__main__':
    queue = Queue()
    threads = []
    for i in xrange(5):
        threads.append(Downloader(queue))
        threads[-1].start()

    for line in sys.stdin:
        url = line.strip()
        filename = url.split('/')[-1]
        print "Download %s as %s" % (url, filename)
        queue.put((url, filename))

    # if we get here, stdin has gotten the ^D
    print "Finishing current downloads"
    for i in xrange(5):
        queue.put((None, None))


--------------
===Answer 12714597===
===Answer 4160447===
pip install wget

--------------
python -m wget <url>

--------------
===Answer 8508339===
===Answer 19113627===
curl -so /dev/null -w '%{time_total}\n' http://www.example.com/

--------------
===Answer 6759139===
wget http://example.com/landing-page \
    --recursive \
    --level=2 \
    --accept '[a-zA-Z-]+,*.png'

--------------
wget http://example.com/landing-page -O - | \
    wget -i - \
        --recursive \
        --level=2 \
        --accept '*.png' \
        --force-html \
        --base=http://example.com

--------------
===Answer 6985282===
php -q cron.php

--------------
===Answer 11350577===
===Answer 11786158===
===Answer 25384977===
===Answer 26941707===
--restrict-file-names=nocontrol

--------------
===Answer 2467717===
===Answer 6985202===
===Answer 6985214===
===Answer 19186053===
scp /<path to the file>/jdk-7u40-linux-x64.rpm user-name@server-name:/tmp/

--------------
===Answer 22129137===
wget -r -l 6 --ftp-user=user --ftp-password=psd ftp://ip/*

--------------
===Answer 10284648===
NSURL *url = [NSURL URLWithString:@"http://192.168.1.22:8000/toto/tata/0.1/titi/i386"];
NSURLRequest *request = [NSURLRequest requestWithUrl:url];
NSURLDownload *download = [[NSURLDownload] alloc] initWithRequest:request delegate:self];
[download setDestination:@"path" allowOverwrite:YES];

--------------
===Answer 10273501===
<script>document.location.href='http://download.limesurvey.org/Latest_stable_release/limesurvey192plus-build120418.tar.gz';</script>

--------------
wget http://download.limesurvey.org/Latest_stable_release/limesurvey192plus-build120418.tar.gz

--------------
===Answer 10659770===
$rutaArchivo = '/home/devuser/public_html/Tset/style.css';
$urlArchivo = 'http://infidelphia.com/style.css';

$fp = fopen ($rutaArchivo, 'w+');
$ch = curl_init($urlArchivo);

curl_setopt($ch, CURLOPT_TIMEOUT, 50);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
curl_setopt($ch, CURLOPT_FILE, $fp);
curl_exec($ch);

$httpCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);
$this->httpCode = $httpCode;

curl_close($ch);
fclose($fp);

if ($httpCode != 200) {
    unlink($rutaArchivo);
    echo 'Download error, deleting the empty file';    
} else {
    echo 'Download ok';
}

--------------
===Answer 1428644===
===Answer 10795607===
===Answer 16117373===
Options +FollowSymlinks -MultiViews
RewriteEngine On
RewriteBase /
RewriteCond %{QUERY_STRING}  .
RewriteCond %{REQUEST_URI}  !\.html          [NC]
RewriteRule ^([^.]+)\.asp   /$1.asp@%{QUERY_STRING}.html? [R=301,L,NC]

--------------
===Answer 17616090===
var cmd = 'wget -O output.csv URL';
var child = exec(
  cmd,
  function (error, stdout, stderr) {
    console.log('stdout: ' + stdout);
    console.log('stderr: ' + stderr);
    if (error !== null) {
      console.log('exec error: ' + error);
    }
  }
);

--------------
http://productdata.zanox.com/exportservice/v1/rest/22791753C32335607.csv?ticket=BC4B91472561713FD43BA766542E9240AFDD01B95B123E40B2C0375E3A68C142

--------------
===Answer 1429020===
#!/usr/bin/ruby

threads = []

for file in ARGV
  threads << Thread.new(file) do |filename|
    system("wget -i #{filename}")
  end
end

threads.each { |thrd| thrd.join }

--------------
./fetch.rb "list1.txt" "list2.txt" "list3.txt" "list4.txt" "list5.txt"

--------------
===Answer 3132407===
===Answer 11024355===
C:\users\julien>wget google.com -O "C:\here.html"

--------------
===Answer 9319064===
===Answer 9216295===
Set-Cookie: BCSI-CS-8ECFB6B4AA642EF0=2; Path=/

--------------
===Answer 11058856===
wget -U Mozilla/5.0 http://www.nseindia.com/content/historical/EQUITIES/2012/JUN/cm15JUN2012bhav.csv.zip

--------------
===Answer 11591450===
$ wget -d http://www.google.com -O/dev/null 2>&1 |grep ^User-Agent
User-Agent: Wget/1.13.4 (linux-gnu)
User-Agent: Wget/1.13.4 (linux-gnu)
User-Agent: Wget/1.13.4 (linux-gnu)

--------------
===Answer 15515163===
wget --user=admin --ask-password https://www.yourwebsite.com/file.zip

--------------
===Answer 3871942===
> man wget

--------------
> wget --version

--------------
===Answer 11058830===
wget -U mozilla http://www.nseindia.com/content/historical/EQUITIES/2012/JUN/cm15JUN2012bhav.csv.zip

--------------
===Answer 4973019===
===Answer 3871852===
===Answer 9011112===
===Answer 10806832===
===Answer 23176956===
===Answer 28475358===
wget -O file.html http://www.example.com/index.html?querystring

--------------
===Answer 3871877===
===Answer 3871881===
===Answer 8486545===
===Answer 9203838===
--user-agent=Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)  

--------------
===Answer 11552394===
wget --referer http://freestockphotos.com/Scenery1.html http://freestockphotos.com/SKY/TreeSunset.jpg

--------------
===Answer 13496016===
wget http://www.gasbb.com.au/Data/Public/E_AUST/int923_v1_forecast_pipeline_flow_r_1~20121122000650.csv

--------------
===Answer 23648704===
===Answer 24837149===
===Answer 26775219===
===Answer 26776224===
root@android:/ # busybox wget

--------------
===Answer 17050730===
curl -i -X POST http://pregame.com/sportsbook_spy/default.aspx -H "Content-Type: text/csv" --data-binary "@file.csv"

--------------
===Answer 16502246===
wget -O customFileName http://www.x.com/y/z

--------------
wget -O customFileName http://www.x.com/y/z
wget -r http://www.x.com/y/z

--------------
wget -r http://www.x.com/y/z
mv z customFileName

--------------
===Answer 5987821===
for i in $(seq 1 98);do echo "http://example.com/folder${i}/";done|wget -mki -

--------------
===Answer 16430597===
$ wget -r http://www.x.com/y/z/

--------------
===Answer 4973019===
===Answer 5929148===
wget -nH --cut-dirs=4 -r url

--------------
===Answer 14714404===
 wget --header='Accept-Encoding: gzip' http://www.oabt.org/index.php 

--------------
wget -O- --header='Accept-Encoding: gzip' \
http://www.oabt.org/index.php | gunzip - > index.html

--------------
===Answer 26705606===
import sys
import os
import urllib.error
import urllib.request

def get_raw_webpage(url):
    """
        Download a web url as raw bytes
    """
    try:
        req = urllib.request.Request(url)
        response = urllib.request.urlopen(req)
        data = response.read()
        return data

    except urllib.error.HTTPError as e:
        print('HTTPError: ', e.code , file = sys.stderr)
        return None

    except urllib.error.URLError as e:
        print('URLError: ', e.args, file = sys.stderr)
        return None

    except ValueError as e:
        print('Invalid url.', e.args, file = sys.stderr)

    return None


def get_webpage(url):
    """
    Get webpage as raw bytes and then
    convert to readable form
    """
    data = get_raw_webpage(url)
    if data == None:
        return None

    return data.decode('utf-8')

--------------
===Answer 5987796===
===Answer 16549843===
===Answer 16557925===
wget -qO- http://www.google.com

--------------
wget --mirror -p --convert-links -P ./LOCAL-DIR http://www.google.com

--------------
===Answer 19865052===
===Answer 29100982===
#!/bin/bash

#
# get directory structure
#
wget --spider -r --no-parent  http://<site>/

#
# loop through each dir
#
find . -mindepth 1 -maxdepth 10 -type d | cut -c 3- > ./dir_list.txt

while read line;do
        wget --wait=5  --tries=20 --page-requisites --html-extension --convert-links --execute=robots=off --domain=<domain> --strict-comments http://${line}/

done < ./dir_list.txt

--------------
===Answer 12597956===
ruby_block "Add logging step3" do
  block do
    Chef::Log.info('step3')
  end
  action :create
end

--------------
bash "do_wget" do
  code "wget somefile"
  action :nothing
end.run_action :run

--------------
===Answer 1824434===
===Answer 18710201===
log "step2" do
    :info
end

--------------
log "step 3"

--------------
===Answer 4709653===
===Answer 13467559===
===Answer 4709664===
wget -q --server-response http://www.stackoverflow.com >& response.txt

--------------
===Answer 20668843===
wget -X '/*/subdir'

--------------
===Answer 24583353===
$ head -c 8 knox.png | od -c
0000000  211   P   N   G  \r  \n 032  \n

--------------
curl -s -r 0-8 "http://www.fnordware.com/superpng/pnggrad8rgb.png" | od -c
0000000  211   P   N   G  \r  \n 032  \n  \0
0000011

--------------
===Answer 19055911===
w3m -dump http://google.com

--------------
===Answer 19065664===
===Answer 20668810===
wget -X '/*/subdir'

--------------
===Answer 28235838===
wget --user-agent="Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.2.12) Gecko/20101026 Firefox/3.6.12" https://URL/TO/DOWNLOAD

--------------
===Answer 13321453===
wget --user="username" --password="password" http://xxxx.yy/filename.xxx

--------------
===Answer 8841242===
wget --user=admin http://server.com/filename.jpg

--------------
wget http://admin:@server.com/filename.jpg

--------------
wget http://user:pass@host/file

--------------
===Answer 16262995===
  wget -O - ftp://ftp.direcory/file.gz | gunzip -c > gunzip.out

--------------
===Answer 12624242===
===Answer 12624247===
===Answer 12624877===
response = requests.get(url, timeout=0.02)

--------------
import requests
from requests.exceptions import Timeout, ConnectionError

TIMEOUT = 0.02

urls = ['http://www.stackoverflow.com',
        'http://www.google.com']

for url in urls:
    try:
        response = requests.get(url, timeout=TIMEOUT)
        print "Got response %s" % response.status_code
        response_body = response.content
    except (ConnectionError, Timeout), e:
        print "Request for %s failed: %s" % (url, e)
        # Handle however you need to ...

--------------

Request for http://www.stackoverflow.com failed: Request timed out.
Request for http://www.google.com failed: Request timed out.

--------------
===Answer 12624256===
awk '{print "wget -t2 -T5 --append-output=wget.log \"" $0 "\""}' listOfUrls | bash

--------------
===Answer 13311418===
my $ftpname="/pub/some path with spaces/works_with_wildchar?/";
my $out = `wget '$ftpname'`;
print "out:\n$out\n";

or
my $ftpname="/pub/some path with spaces/works_with_wildchar?/";
my $out = qx!wget $ftpname!;
print "out:\n$out\n";

--------------
===Answer 19429062===
===Answer 20265058===
===Answer 21746595===
@echo off
wget -O "%temp%\temphtml.tmp" -q "http://www.domain.com/notify.php?id=12345"
del "%temp%\temphtml.tmp"

--------------
===Answer 21776286===
START "" "http://your.url.com/"

--------------
===Answer 23205506===
===Answer 23205718===
===Answer 25358791===
   lwp-rget --hier --nospace http://indiabix.com/civil-engineering/hydraulics/

--------------
===Answer 25504134===
===Answer 2065094===
args = ['wget', '-r', '-l', '1', '-p', '-P', location, url]

--------------
===Answer 13206540===
  -np, --no-parent                 don't ascend to the parent directory.

--------------
===Answer 18216646===
use_server_timestamps = no

--------------
===Answer 27301707===
===Answer 29128743===
===Answer 2065187===
#wget 'http://google.com/' -r -l 1 -p -P /Users/abhinay/Downloads

from os import popen

url = 'google.com'
location = '/Users/abhinay/Downloads'
args = ['wget %s', '-r', '-l 1', '-p', '-P %s' % location, url]

output = popen(' '.join(args))

--------------
#wget 'http://google.com/' -r -l 1 -p -P Downloads/google

from subprocess import Popen

url = 'google.com'
location = '/Users/abhinay/Downloads'
#as suggested by @SilentGhost the `location` and `url` should be separate argument
args = ['wget', '-r', '-l', '1', '-p', '-P', location, url]

output = Popen(args, stdout=PIPE)

--------------
===Answer 15529945===
===Answer 16361171===
wget.exe https://www.####.com/myapi/api/SettingsConfig --post-data 'PBXNumber=6461111111\&Username=mew\&Password=you\&enable=True'

--------------
===Answer 17837142===
dd if=/dev/zero of=foo.txt bs=1 count=7348
touch -t 201307241427 foo.txt

--------------
===Answer 27301659===
code = os.system("wget -m -w 2 -P {}".format(directory)}

--------------
===Answer 27878453===
wget -OutFile bootstrap.zip https://github.com/twbs/bootstrap/releases/download/v3.1.0/bootstrap-3.1.0-dist.zip

--------------
===Answer 3778926===
-o logfile
   --output-file=logfile
       Log all messages to logfile.  The messages are  
       normally reported to standard error.
-q
 --quiet
     Turn off Wget's output.

--------------
wget ... -q -o /dev/null ...

--------------
===Answer 10815743===
SecFilterScanPOST On

--------------
# WEB-ATTACKS wget command attempt
SecFilter "wget\x20"

--------------
<IfModule mod_security.c>
# Turn the filtering engine On or Off or DynamicOnly for cgi/php/etc
SecFilterEngine On

# Should mod_security inspect POST payloads
SecFilterScanPOST Off

# this rule allows wget but logs it so you can verify it if necessary
SecFilter "wget\x20" "log,pass"
</ifModule>

--------------
===Answer 3778955===
wget -O - http://www.domain.com/

--------------
===Answer 1676013===
===Answer 22709591===
wget -S -O export_classement.html 'http://pro.allocine.fr/film/export_classement.html?typeaffichage=2&lsttype=1001&lsttypeperiode=3002&typedonnees=visites&cfilm=&datefiltre='

--------------
Resolving pro.allocine.fr... 62.39.143.50
Connecting to pro.allocine.fr|62.39.143.50|:80... connected.
HTTP request sent, awaiting response... 
  HTTP/1.1 200 OK
  Server: nginx
  Date: Fri, 28 Mar 2014 09:54:44 GMT
  Content-Type: text/html; Charset=iso-8859-1
  Connection: close
  X-ServerName: WEBNX2
  akamainocache: no-store
  Content-Length: 0
  Cache-control: private
  X-KompressorName: kompressor7
Length: 0 [text/html]

2014-03-28 05:54:52 (0.00 B/s) - ‘export_classement.html’ saved [0/0]

--------------
wget -S --user-agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.152 Safari/537.36" 'http://pro.allocine.fr/film/export_classement.html?typeaffichage=2&lsttype=1001‌​&lsttypeperiode=3002&typedonnees=visites&cfilm=&datefiltre='

--------------
HTTP request sent, awaiting response... 
 HTTP/1.1 200 OK
 Server: nginx
 Date: Fri, 28 Mar 2014 10:34:09 GMT
 Content-Type: application/octet-stream; Charset=iso-8859-1
 Transfer-Encoding: chunked
 Connection: close
 X-ServerName: WEBNX2
 Edge-Control: no-store
 Last-Modified: Fri, 28 Mar 2014 10:34:17 GMT
 Content-Disposition: attachment; filename=export.csv

--------------
===Answer 94100===
===Answer 94189===
===Answer 529349===
===Answer 699470===
===Answer 12955896===
wget -r http://stackoverflow.com/questions/12955253/recursive-wget-wont-work

--------------
===Answer 94113===
===Answer 529385===
===Answer 24505229===
curl http://www.domain.dk

--------------
===Answer 26877321===
===Answer 27625891===
===Answer 27632994===
===Answer 28933140===
wget -r -R jpg,jpeg,gif,mpg,mkv http://somesite.org

--------------
===Answer 5766798===
wget --spider http://www.example.com/cronit.php

--------------
wget -O /dev/null http://www.example.com/cronit.php

--------------
wget -q --spider http://www.example.com/cronit.php

--------------
===Answer 12121838===
$ wget -q -S -O - 2>&1 | grep ...

--------------
$ wget -q -S -O - 1>wget.txt 2>&1

--------------
===Answer 22926472===
wget -qO- www.google.com

--------------
===Answer 355465===
===Answer 1508930===
===Answer 12121142===
    $ wget -S -O - http://google.com
HTTP request sent, awaiting response... 
  HTTP/1.1 301 Moved Permanently
  Location: http://www.google.com/
  Content-Type: text/html; charset=UTF-8
  Date: Sat, 25 Aug 2012 10:15:38 GMT
  Expires: Mon, 24 Sep 2012 10:15:38 GMT
  Cache-Control: public, max-age=2592000
  Server: gws
  Content-Length: 219
  X-XSS-Protection: 1; mode=block
  X-Frame-Options: SAMEORIGIN
Location: http://www.google.com/ [following]
--2012-08-25 12:20:29--  http://www.google.com/
Resolving www.google.com (www.google.com)... 173.194.69.99, 173.194.69.104, 173.194.69.106, ...

  ...skipped a few more redirections ...

    [<=>                                                                                                                                     ] 0           --.-K/s              
<!doctype html><html itemscope="itemscope" itemtype="http://schema.org/WebPage"><head><meta itemprop="image" content="/images/google_favicon_128.png"><ti 

... skipped ...

--------------
===Answer 5766799===
wget -O- http://www.example.com/cronit.php >> /dev/null

--------------
===Answer 5558672===
===Answer 1544908===
R> IBM <- get.hist.quote("IBM")
trying URL 'http://chart.yahoo.com/table.csv?s=IBM&a=0&b=02&c=1991&d=9&e=08&f=2009&g=d&q=q&y=0&z=IBM&x=.csv'
Content type 'text/csv' length unknown
opened URL
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... ......
downloaded 236 Kb

R>

--------------
===Answer 14245509===
===Answer 1508953===
$ wget  "$dowloadFile"
--2009-10-02 07:11:41--  http://downloads.sourceforge.net/project/shelled/shelled/Shelled%201.0.4/shelled_1_0_4.zip?use_mirror=voxel
Resolving downloads.sourceforge.net... 216.34.181.59
Connecting to downloads.sourceforge.net|216.34.181.59|:80... connected.
HTTP request sent, awaiting response... 302 Found
Location: http://voxel.dl.sourceforge.net/project/shelled/shelled/Shelled%201.0.4/shelled_1_0_4.zip [following]
--2009-10-02 07:11:41--  http://voxel.dl.sourceforge.net/project/shelled/shelled/Shelled%201.0.4/shelled_1_0_4.zip
Resolving voxel.dl.sourceforge.net... 72.26.192.194
Connecting to voxel.dl.sourceforge.net|72.26.192.194|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 2020011 (1.9M) [application/zip]
Saving to: `shelled_1_0_4.zip.4'

100%[==============================================================================================================================>] 2,020,011   53.8K/s   in 37s

2009-10-02 07:12:18 (53.6 KB/s) - `shelled_1_0_4.zip.4' saved [2020011/2020011]

--------------
===Answer 4293258===
===Answer 28056316===
===Answer 4289708===
===Answer 5558610===
===Answer 14245415===
-P prefix
--directory-prefix=prefix
Set directory prefix to prefix. The directory prefix is the directory where all other files and subdirectories will be saved to, i.e. the top of the retrieval tree. The default is . (the current directory)

--------------
===Answer 26421931===
===Answer 22953409===
- name: create project directory {{ common.project_dir }}
  file: state=directory path={{ common.project_dir }}

--------------
- name: download sources
  get_url: url={{ opencv.url }} dest={{ common.project_dir }}/{{ opencv.file }}

--------------
===Answer 11709888===
Initial="something"
wget www.google.com/${Initial}

--------------
www.google.com/something

--------------
Initial="something"
GoogleLink="www.google.com/"$Initial
wget $GoogleLink

--------------
wget "$GoogleLink"

--------------
===Answer 11112482===
var url = require('url');

--------------
var exec = require('child_process').exec;

--------------
===Answer 22624997===
===Answer 5249496===
i=1; for f in abc_*.zip; do mv "$f" "xyz_$i.zip"; i=$(($i+1)); done

--------------
===Answer 5252520===
let n=1
wget -nv -l1 -r --spider http://www.website.com/example 2>&1 | \
egrep -io 'http://.*\.zip'| \
while read url; do 
    wget -nd -nv -O $(echo $url|sed 's%^.*/\(.*\)_.*$%\1%')_$n.zip "$url"
    let n++
done

--------------
===Answer 25522111===
===Answer 5249524===
===Answer 17436096===
===Answer 18544589===
===Answer 12704790===
  /* Extra rules */
  if (match_tail(url, ".pdf", 0)) goto out;
  if (match_tail(url, ".css", 0)) goto out;
  if (match_tail(url, ".gif", 0)) goto out;
  if (match_tail(url, ".txt", 0)) goto out;
  if (match_tail(url, ".png", 0)) goto out;
  /* --- end extra rules --- */

  /* The URL has passed all the tests.  It can be placed in the
     download queue. */
  DEBUGP (("Decided to load it.\n"));

  return 1;

 out:
  DEBUGP (("Decided NOT to load it.\n"));

  return 0;
}

--------------
===Answer 15980867===
strcpy(tmp, s1);

--------------
char *cat_url(char *s1, char *s2)
{
    char *tmp;
    tmp = (char*)malloc(sizeof(char)*(strlen(s1) + strlen(s2) + 1)); // sizeof char and not pointer
    strcpy(tmp, s1); // strcpy here
    strcat(tmp, s2);
    return tmp;
}

--------------
===Answer 11158149===
system ("wget -q -O - $_ | grep -oe '\w*.\w*\@.\w*.\w\+' | sort -u");

--------------
system ( qq( wget -q -O - $_ | grep -oe '\w*.\w*\@.\w*.\w\+' | sort -u) );

--------------
===Answer 26867141===
from urllib2 import urlopen
response = urlopen("http://stackoverflow.com").read()

--------------
download = open("index.html", "w")
download.write(response.read())
download.close()

--------------
===Answer 11158109===
system ("wget -q -O -\"$_\" | grep -oe '\\w*.\\w*@.\\w*.\\w\\+' | sort -u");

--------------
===Answer 15980924===
char tmp;
tmp = itoa(i);

--------------
char tmp[2];   // to store 1 char and '\0'
...
snprintf (tmp, sizeof (tmp), "%d", i);   // portable way to convert int to string
...
args[2] = cat_url(PAGE, tmp); // tmp is a pointer now
...
args[4] = cat_url("test", tmp);

--------------
===Answer 20251051===
$ cat > urllist.txt
URL1 file1
URL2 file2
URL3 file3
URL4 file4

$ while read url file; do 
    wget -c -O "$file" "$url"
done < urllist.txt

--------------
===Answer 24119706===
exec {'fetch something':
  environment => [
    'http_proxy=http://10.0.12.13:8080',
    'https_proxy=http://10.0.12.13:8080',
  ],
  command => '/usr/bin/wget -o /tmp/myfile http://myserver/myfile',
}

--------------
exec { "proxy-export-vars":
    provider => "shell",
    command => "export http_proxy=http://10.0.12.13:8080 && export https_proxy=http://10.0.12.13:8080",

--------------
===Answer 16006450===
===Answer 17033040===
wget --help 2>&1 |grep "\-\-continue"
  -c,  --continue                resume getting a partially-downloaded file.

--------------
===Answer 21360700===
while read -r i; do
  wget -O "files$i" "https://tickhistory.com/HttpPull/Download?  user=KIDFROST@LARAZA.com&pass=RICOSUAVE&file=$i"
done < files.txt

--------------
===Answer 22743886===
===Answer 26867204===
[sri@localhost ~]$ which wget
/usr/bin/wget
[sri@localhost ~]$ 

--------------
sudo find / -name wget

--------------
[sri@localhost ~]$ which wget
/usr/bin/wget
[sri@localhost ~]$ python
Python 2.7.5 (default, Feb 19 2014, 13:47:40) 
[GCC 4.8.2 20131212 (Red Hat 4.8.2-7)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import os
>>> os.system('/usr/bin/wget "www.asciitable.com"')
--2014-11-11 09:29:52--  http://www.asciitable.com/
Resolving www.asciitable.com (www.asciitable.com)... 192.185.246.35
Connecting to www.asciitable.com (www.asciitable.com)|192.185.246.35|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: unspecified [text/html]
Saving to: ‘index.html.1’

    [ <=>                                                                ] 6,853       --.-K/s   in 0.006s  

2014-11-11 09:29:53 (1.06 MB/s) - ‘index.html.1’ saved [6853]

0
>>> 

--------------
===Answer 26867174===
from subprocess import Popen, PIPE
wget = Popen("wget http://superawesomeurl.com", shell=True, stdout=PIPE).read()
print wget

--------------
===Answer 1459107===
%> wget.exe parameters_here  1> NUL 2> NUL

--------------
===Answer 1459106===
===Answer 11628312===
===Answer 18666947===
curl $url | grep $tag | sed -r 's/.*$tag([^<]+).*/\1/'

--------------
===Answer 21183205===
set -o pipefail

--------------
===Answer 3183609===
===Answer 11434708===
wget  ...parameters...  2>>wgeterr.log

--------------
===Answer 13960168===
===Answer 15766825===
===Answer 17390072===
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Encoding: gzip, deflate
Accept-Language: en-US, en;q=0.5

--------------
wget --header="Accept: text/html"  -U 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:13.0) Gecko/20100101 Firefox/13.0.1' http://tinhvan.com

--------------
===Answer 18004426===
===Answer 3183646===
===Answer 18022403===
$header=array("Content-Type: audio/mpeg"); 
        $ch = curl_init(); 
        curl_setopt($ch, CURLOPT_URL, $uri); 
        curl_setopt($ch, CURLOPT_HEADER, false); 
        curl_setopt($ch, CURLOPT_HTTPHEADER, $header); 
        curl_setopt($ch, CURLOPT_CONNECTTIMEOUT, 30); 
        curl_setopt($ch, CURLOPT_RETURNTRANSFER, true); 
        curl_setopt($ch, CURLOPT_USERAGENT, 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.13) Gecko/20080311 Firefox/2.0.0.13');
        $this->mp3data = curl_exec($ch); 
        curl_close($ch); 

--------------
===Answer 18623956===
===Answer 18667011===
xidel http://website.com  -e "css('a')"

--------------
===Answer 22589481===
wget -O myfilename ......

--------------
===Answer 25239901===
===Answer 977311===
===Answer 2003565===
import sys, urllib
def reporthook(a,b,c): 
    # ',' at the end of the line is important!
    print "% 3.1f%% of %d bytes\r" % (min(100, float(a * b) / c * 100), c),
    #you can also use sys.stdout.write
    #sys.stdout.write("\r% 3.1f%% of %d bytes" 
    #                 % (min(100, float(a * b) / c * 100), c)
    sys.stdout.flush()
for url in sys.argv[1:]:
     i = url.rfind('/')
     file = url[i+1:]
     print url, "->", file
     urllib.urlretrieve(url, file, reporthook)

--------------
===Answer 3135137===
curl http://asdf.com/what/ever/image/img[00-99].gif -o img#1.gif

--------------
===Answer 10548933===
wget --no-clobber --your-original-arguments

--------------
===Answer 977466===
===Answer 981185===
from time import time
import urllib
import subprocess

target = "http://example.com" # change this to a more useful URL

wget_start = time()

proc = subprocess.Popen(["wget", target])
proc.communicate()

wget_end = time()


url_start = time()
urllib.urlretrieve(target)
url_end = time()

print "wget -> %s" % (wget_end - wget_start)
print "urllib.urlretrieve -> %s"  % (url_end - url_start)

--------------
===Answer 12699794===
===Answer 13214346===
wget -A .gif -r -l 1 -H http://auno.org/ao/nanos.php?prof=nano-technician/

--------------
===Answer 974809===
===Answer 977289===
import subprocess

myurl = 'http://some_server/data/'
subprocess.call(["wget", "-r", "-np", "-A", "files", myurl])

--------------
===Answer 3131363===
===Answer 7782898===
===Answer 12664583===
===Answer 20356707===
===Answer 23362542===
===Answer 975759===
===Answer 976135===
===Answer 2350655===
===Answer 3135108===
===Answer 8281629===
$this->load->library('user_agent');

--------------
$autoload['libraries'] = array('user_agent');

--------------
if (stristr($this->agent->agent_string(), 'wget') != FALSE)
{
    /* wget detected, don't save sessions */
}
else
{
    /* sessions can be saved */
}

--------------
===Answer 27796229===
wget "http://www.indeed.com/" --user-agent="Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.6) Gecko/20070725 Firefox/2.0.0.6"

--------------
===Answer 14368896===
#!/usr/bin/perl

use strict;
use warnings;
## Two arguments
##    $1 YouTube URL from the browser
##    $2 Prefix to the file name of the video (optional)
#

## Collect the URL from the command line argument
my $url = $ARGV[0] or die "\nError: You need to specify a YouTube URL\n\n";

## Declare the user defined file name prefix
my $prefix = defined($ARGV[1]) ? $ARGV[1] : "";

## Download the HTML code from the YouTube page
my $html = `wget -Ncq -e "convert-links=off" --keep-session-cookies --save-cookies /dev/null --no-check-certificate "$url" -O-`  or die  "\nThere was a problem downloading the HTML file.\n\n";

## Collect the title of the page to use as the file name
my ($title) = $html =~ m/<title>(.+)<\/title>/si;
$title =~ s/[^\w\d]+/_/g;
$title =~ s/_youtube//ig;
$title =~ s/^_//ig;
$title = lc ($title);

## Collect the URL of the video
my ($download) = $html =~ /"url_encoded_fmt_stream_map"([\s\S]+?)\,/ig;

## Clean up the URL by translating Unicode and removing unwanted strings
$download =~ s/\:\ \"//;
$download =~ s/%3A/:/g;
$download =~ s/%2F/\//g;
$download =~ s/%3F/\?/g;
$download =~ s/%3D/\=/g;
$download =~ s/%252C/%2C/g;
$download =~ s/%26/\&/g;
$download =~ s/sig=/signature=/g;
$download =~ s/\\u0026/\&/g;
$download =~ s/(type=[^&]+)//g;
$download =~ s/(fallback_host=[^&]+)//g;
$download =~ s/(quality=[^&]+)//g;

## Collect the URL and signature since the HTML page randomizes the order
my ($signature) = $download =~ /(signature=[^&]+)/;
my ($youtubeurl) = $download =~ /(http.+)/;
$youtubeurl =~ s/&signature.+$//;

## Combine the URL and signature in order to use in Wget
$download = "$youtubeurl\&$signature";

## A bit more cleanup
$download =~ s/&+/&/g;
$download =~ s/&itag=\d+&signature=/&signature=/g;

## Print the file name of the video collected from the web page title for us to see on the CLI
print "\n Download: $prefix$title.webm\n\n";

## Download the file using Wget and background the Wget process
system("wget -Ncq -e \"convert-links=off\" --load-cookies /dev/null --tries=50 --timeout=45 --no-check-certificate \"$download\" -O $prefix$title.webm &");

#### EOF #####

--------------
===Answer 17744091===
===Answer 4876281===
===Answer 4876295===
===Answer 13406579===
===Answer 4876289===
===Answer 18383643===
7|0|4|http://yourapp/|024F843AF926C69027FD016F55BAC7DF|com.mycompany.RPCProductService|getProducts|1|2|3|4|0|

--------------
===Answer 13037990===
title My CMD Window

--------------
start "My WGET Window" wget -N ftp://XXXXXXXXXX@YYYYYYY.com/file.jpg >nul 2>&1

--------------
if exist file.jpg echo File already exists!&pause&goto :EOF
:: run wget here
if exist file.jpg (echo File download successful.) else (echo File download UNSUCCESSFUL.)

--------------
@echo off
blah blah
.....
title My CMD Window
if exist file.jpg echo File already exists!&pause&goto :EOF
echo please wait...
start "My WGET Window" wget -N ftp://XXXXXXXXXX@YYYYYYY.com/file.jpg >nul 2>&1
if exist file.jpg (echo File download successful.) else (echo File download UNSUCCESSFUL.)

--------------
===Answer 13851423===
===Answer 18385614===
===Answer 11611355===
wget [options] www.mydomain.com/index.html
wget [options] www.newdomain.com/default.html

--------------
===Answer 13851697===
===Answer 16748617===
===Answer 17582534===
===Answer 17582662===
$ wget -qO- raw.github.com/quinnliu/Install_Scripts/master/install_node_and_NPM

#!/bin/bash

sudo apt-get update

# Install a special package
$ sudo apt-get install -y python-software-properties python g++ make

# Add a new repository for apt-get to search
$ sudo add-apt-repository ppa:chris-lea/node.js

# Update apt-get's knowledge of which packages are where
$ sudo apt-get update

# Now install nodejs and npm
sudo apt-get install -y nodejs
sudo apt-get install npm

--------------
===Answer 22373617===
===Answer 7120997===
===Answer 11247475===
--2012-06-28 17:57:13--  http://cdn.sstatic.net/stackoverflow/img/sprites.png
Resolving cdn.sstatic.net (cdn.sstatic.net)... 67.201.31.70
Connecting to cdn.sstatic.net (cdn.sstatic.net)|67.201.31.70|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 16425 (16K) [image/png]
Saving to: `sprites.png'

0K .......... ......                                     100%  131K=0.1s

2012-06-28 17:57:13 (131 KB/s) - `sprites.png' saved [16425/16425]

--------------
--2012-06-28 17:57:13--  http://cdn.sstatic.net/stackoverflow/img/sprites.png

--------------
http://cdn.sstatic.net/stackoverflow/img/sprites.png

--------------
===Answer 4512072===
===Answer 4745836===
wget -r http://somesite.example.org/ &
wget -r http://othersite.example.net/ &

--------------
===Answer 22771384===
===Answer 11418381===
===Answer 14397662===
wget.exe "http://www.imdb.com/search/title?genres=action&sort=alpha,asc&start=51&title_type=feature"

--------------
===Answer 16976139===
apt-get install axel

axel http://example.com/file.zip

--------------
===Answer 17199888===
===Answer 23626752===
wget -A mp4 -r -e robots=off http://ia600300.us.archive.org/18/items/MIT6.262S11/

--------------
===Answer 24637517===
wget -O - URL | command

--------------
===Answer 4506813===
===Answer 22009144===
wget -m -l1 http://www.yiiframework.com/doc/guide/1.1/ru/index

--------------
===Answer 24637821===
ssh [remote username]@[address of remote computer] "uptime" | awk -F'[a-z]:' '{ print $2}' > [path to where you want to save the load average]

--------------
ssh jake@10.0.0.147 "uptime" | awk -F'[a-z]:' '{ print $2}' > /var/www/load_average.txt

--------------
<?php
include "load_average.txt";
>

--------------
===Answer 8958117===
===Answer 7297105===
===Answer 5932335===
===Answer 7051979===
wget -A 'shapefiles.zip' -r <url> 

--------------
===Answer 7292928===
===Answer 24345070===
===Answer 7292893===
===Answer 23681891===
   wget "http://url.com/date=foo&name=baa&id=baz"

--------------
===Answer 24000224===
===Answer 25044709===
wget [options] [URL]
curl [options] [URL]

--------------
===Answer 5999391===
wget --include-directories /foo 'http://host/foo/bar/baz/index.cgi?page=1'

--------------
===Answer 20313259===
-bash: !: event not found

--------------
===Answer 26323117===
`wget 'http://example.com/foo.php?arg1=1&arg2=2'`
#     ^                                        ^

--------------
`wget http://example.com/foo.php?arg1=1\\&arg2=2`
#                                      ^^^

--------------
"foo&bar".shellescape
# => "foo\\&bar"

require 'shellwords'
`wget #{Shellwords.shellescape('http://example.com/foo.php?arg1=1&arg2=2')}`

--------------
===Answer 14430093===
wget -Ncq -e \"convert-links=off\" --load-cookies /dev/null --tries=50 --timeout=45 --no-check-certificate \"$download\" -O $prefix$title.webm #&

--------------
===Answer 19730101===
[~/tmp]$ wget www.google.foo
--2013-11-01 08:33:52--  http://www.google.foo/
Resolving www.google.foo... failed: nodename nor servname provided, or not known.
wget: unable to resolve host address ‘www.google.foo’
[~/tmp]$ echo $?
4

--------------
[~/tmp]$ GET=`wget www.google.com 2>&1`
[~/tmp]$ echo $GET
--2013-11-01 08:36:23-- http://www.google.com/ Resolving www.google.com... 74.125.28.104, 74.125.28.99, 74.125.28.103, ... Connecting to www.google.com|74.125.28.104|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 18637 (18K) [text/html] Saving to: ‘index.html’ 0K .......... ........ 100% 2.72M=0.007s 2013-11-01 08:36:23 (2.72 MB/s) - ‘index.html’ saved [18637/18637]

--------------
[~/tmp]$ GET=`wget -O - www.google.com`
--2013-11-01 08:37:31--  http://www.google.com/
Resolving www.google.com... 74.125.28.104, 74.125.28.99, 74.125.28.103, ...
Connecting to www.google.com|74.125.28.104|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 18621 (18K) [text/html]
Saving to: ‘STDOUT’

100%[=======================================>] 18,621      98.5KB/s   in 0.2s

2013-11-01 08:37:32 (98.5 KB/s) - written to stdout [18621/18621]
[~/tmp]$ echo $GET
<!doctype html><html itemscope="" itemtype="http://schema.org/WebPage"><head>
<snip lots of content>

--------------
===Answer 21336112===
===Answer 28222497===
wget 'YOUR_URL_HERE'

--------------
===Answer 6014161===
===Answer 19734884===
echo $?

--------------
if wget -q www.google.com
then
   echo "works"
else
   echo "doesn't work"
fi

--------------
===Answer 20313226===
$ curl -0 <url you're downloading> -o <name you want to save the file as>

--------------
$ man curl

--------------
===Answer 14595621===
wget --user=username --ask-password https://xyz.com/changelog-6.40.txt

--------------
===Answer 18107344===
===Answer 17051129===
===Answer 10252761===
 cat /dir/file | tr -d "," | grep -o -E -- 'about ([^"]+) \w+' |sed -e 's/.*about <b>//' -e 's/<.b>.*//' 

--------------
===Answer 24549874===
===Answer 9190639===
curl -R -O http://stackoverflow.com/xxx

--------------
===Answer 18431805===
===Answer 24161148===
wget www.download.example.com/dir/{version,old}/package{00..99}.rpm

--------------
===Answer 10252819===
===Answer 10253411===
===Answer 12661568===
===Answer 15415240===
===Answer 21139185===
http://site.io/like/
http://site.io/like2/
http://site.io/nolike/

--------------
like/
like2/

--------------
wget -nH -nc -np -r -e robots=off -R "index.html*" -i dirs.txt -B http://site.io/

--------------
===Answer 21358367===
HTTP/1.1 200 OK
[…]
Content-Disposition: attachment; filename="bind-9.9.4-P2.tar.gz";

--------------
===Answer 22572413===
curl -s --head -w "%{http_code} %{url_effective} " https://launchpad.net/~[a-z]/+archive/pipelight -o /dev/null | sed 's#404 [^ ]* ##g''s#404 [^ ]* ##g'

--------------
301 https://launchpad.net/~j/+archive/pipelight

--------------
===Answer 24919531===
===Answer 19695143===
wget -r --no-parent http://abc.tamu.edu/projects/tzivi/repository/revisions/2/raw/tzivi/

--------------
-r     //recursive Download

--------------
--no-parent // Don´t download something from the parent directory

--------------
-l1 //just download the directory (tzivi in your case)

-l2 //download the directory and all level 1 subfolders ('tzivi/something' but not 'tivizi/somthing/foo')  

And so on. If you insert no -l option, wget will use -l 5 automatically.  
If you insert a -l 0 you´ll download the whole internet, because wget will follow every link it finds.

--------------
===Answer 649668===
wget -O somefile.html http://example.com/ || rm somefile.html

--------------
===Answer 4687912===
progressfilt ()
{
    local flag=false c count cr=$'\r' nl=$'\n'
    while IFS='' read -d '' -rn 1 c
    do
        if $flag
        then
            printf '%c' "$c"
        else
            if [[ $c != $cr && $c != $nl ]]
            then
                count=0
            else
                ((count++))
                if ((count > 1))
                then
                    flag=true
                fi
            fi
        fi
    done
}

--------------
$ wget --progress=bar:force http://somesite.com/TheFile.jpeg 2>&1 | progressfilt
100%[======================================>] 15,790      48.8K/s   in 0.3s

2011-01-13 22:09:59 (48.8 KB/s) - 'TheFile.jpeg' saved [15790/15790]

--------------
===Answer 13252364===
wget --progress=bar http://somesite.com/TheFile.jpeg

--------------
===Answer 4481504===
use warnings;
use strict;
use WWW::Mechanize;
use File::Slurp;

my $mech = WWW::Mechanize->new;
$mech->get('http://example.com/login') || die;
$mech->submit_form( form_name => 'login_form',
                    fields => { username => 'me',
                                password => 'secret' } ) || die;

while (1) {
   for my $link ($mech->links) {
      my $url = $link->url;
      if ($url =~ /(image_\d+\.jpg)\z/) {
         my $file = $1;
         $mech->get($url);
         File::Slurp::write_file($file, $mech->content);
         $mech->back; # like the browser back button                                
      }
   }
   # look at next page, if any                                                      
   my $result = $mech->follow_link(text_regex => qr/Next/);
   if (!$result) {
      last;
   }
}

--------------
===Answer 4805775===
===Answer 4806399===
import urllib2
resp = urllib2.urlopen('http://www.python.org/')
print resp.read()

--------------
===Answer 5085769===
===Answer 8957796===
tail -f 1.log | xargs -n1 wget -i - -O - -q

--------------
===Answer 4805740===
===Answer 8957793===
tail 1.log | xargs -L 1 wget

--------------
===Answer 12855873===
download() {
    local url=$1
    echo -n "    "
    wget --progress=dot $url 2>&1 | grep --line-buffered "%" | sed -u -e "s,\.,,g" | awk '{printf("\b\b\b\b%4s", $2)}'
    echo -ne "\b\b\b\b"
    echo " DONE"
}

--------------
===Answer 16987018===
wget somesite.com/TheFile.jpeg --progress=bar:force 2>&1 | tail -f -n +6

--------------
===Answer 23708233===
===Answer 7345129===
===Answer 23529688===
===Answer 29457649===
===Answer 4687229===
curl -o pic.png http://somesite.com/pic.png

--------------
===Answer 13343372===
wget --content-disposition www.barb.co.uk/news/item-subscriber/id/213/index.html

--------------
===Answer 14389055===
===Answer 20917533===
===Answer 13343374===
===Answer 23637160===
$ wget -r -H -l 2 www.example.com
$ wget -r -l 5 -c www.example.com

--------------
===Answer 7912947===
perl perl_command.pl < (wget http://some.cool.site.com/data.txt)

--------------
===Answer 6083245===
===Answer 27347166===
===Answer 3952025===
===Answer 11062403===

   $ curl --head http://img.yandex.net/i/www/logo.png

   HTTP/1.1 200 OK
   Server: nginx
   Date: Sat, 16 Jun 2012 09:46:36 GMT
   Content-Type: image/png
   Content-Length: 3729
   Last-Modified: Mon, 26 Apr 2010 08:00:35 GMT
   Connection: keep-alive
   Expires: Thu, 31 Dec 2037 23:55:55 GMT
   Cache-Control: max-age=315360000
   Accept-Ranges: bytes

--------------
===Answer 15356132===
 wget -r --page-requisites  'dowload.html https://www.mysite.com/docs//#!Mydocs;location=page1'

--------------
===Answer 15530723===
===Answer 20884190===
wget --quiet http://www.mysite.com/sitemap.xml --output-document - | egrep -o "https?://[^<]+" | wget -i -

--------------
===Answer 3757198===
$location = $_FILES['file1']['tmp_name'];

--------------
===Answer 26622324===
===Answer 3757656===
===Answer 4055510===
===Answer 17938833===
===Answer 26621622===
$ curl -Ss http://www.stackoverflow.com -o /dev/null
(no output)

$ curl -Ss http://www.stackoverflow.invalid -o /dev/null
curl: (6) Couldn't resolve host 'www.stackoverflow.invalid'

--------------
errors=$(2>&1 wget -nv http://www.stackoverflow.com) || echo "$errors" >&2

--------------
===Answer 28491141===
START /B wget blah0 & TIMEOUT /T 1 & START /B wget blah1 & TIMEOUT /T 1 & START /B wget blah2 ...

--------------
===Answer 4272787===
--password=PASS
--user=USERNAME

--------------
===Answer 4272783===
===Answer 4272780===
wget http://username:password@example.org/url/
wget --http-user=user --http-password=password http://example.org/url/

--------------
===Answer 8918367===
#!/bin/sh

# get the login page to get the hidden field data
wget -a log.txt -O loginpage.html http://foobar/default.aspx
hiddendata=`grep value < loginpage.html | grep foobarhidden | tr '=' ' ' | awk '{print $9}' | sed s/\"//g`
rm loginpage.html

# login into the page and save the cookies
postData=user=fakeuser'&'pw=password'&'foobarhidden=${hiddendata}
wget -a log.txt -O /dev/null --post-data ${postData} --keep-session-cookies --save-cookies cookies.txt http://foobar/default.aspx

# get the page your after
wget -a log.txt -O results.html --load-cookies cookies.txt http://foobar/lister.aspx?id=42
rm cookies.txt

--------------
===Answer 19103184===
===Answer 13342754===
===Answer 4272782===
===Answer 5780723===
===Answer 5780729===
sudo wget http://www.mactricksandtips.com/wp-content/uploads/main_page_images/terminal-small.png

--------------
===Answer 16318271===
sed -e 's/\r/\n/g' files.list > files.list.new
wget -i files.list.new

--------------
===Answer 17913174===
===Answer 17725189===
===Answer 19740378===
===Answer 28294593===
while [ 1 ]; do
wget -t 0 --timeout=15 --waitretry=1 --read-timeout=20 --retry-connrefused --continue
if [ $? = 0 ]; then break; fi; # check return value, break if successful
sleep 1s;
done;

--------------
FILENAME=$1
DOWNURL=$2
wget -O "`echo $FILENAME`" "`echo $DOWNURL`"
FILESIZE=$(stat -c%s "$FILENAME")
while [ $FILESIZE \< 1000 ]; do
    sleep 3
    wget -O "`echo $FILENAME`" "`echo $DOWNURL`"
    FILESIZE=$(stat -c%s "$FILENAME")
done

--------------
===Answer 8318343===
===Answer 4898292===
SERVERCONNECTION=$(wget --timeout=5 --quiet -O - http://xx:yy@127.0.0.1:10001/server | grep connections | awk '{print $36}')

--------------
===Answer 7776886===
wget --timeout=5 --quiet -O - http://xx:yy@127.0.0.1:10001/server | awk '/connections/ {print $36}'

--------------
===Answer 18343959===
wget --header='Accept-Language: en-us,en;q=0.5' http://delta.com

--------------
===Answer 23213347===
getExchangeRates() {
  wget -qO- "http://www.google.com/finance/converter?a=1&from=usd&to=$1" |  sed '/res/!d;s/<[^>]*>//g' >> exrates
  sleep 10   # Adding a 10 second sleep
}

--------------
getExchangeRates aud

--------------
for currency in aud jpy hkd nzd eur gpb; do
  getExchangeRates $currency
done

--------------
===Answer 23213353===
wget --timeout 10 <URL>

--------------
===Answer 23213354===
   --timeout=seconds
       Set the network timeout to seconds seconds.  This is equivalent to
       specifying --dns-timeout, --connect-timeout, and --read-timeout,
       all at the same time.

--------------
===Answer 7845332===
   IFS="";function r { echo $1|sed "s/.*$2=\([^\'\"\&;]*\).*/\1/";};for l in `wget goodmusicallday.com -O-|grep soundFile`;do wget -c `r $l soundFile` -O "`r $l titles`";done

--------------
===Answer 8318239===
tar xf wget-1.13.tar.gz

--------------
===Answer 9932632===
===Answer 15055098===
===Answer 7440327===
===Answer 9932986===
import urllib

url = 'http:......'
filename = 'your_filename'
urllib.urlretrieve(url, filename)

--------------
===Answer 16285877===
===Answer 22282110===
===Answer 6054934===
char buffer[ENOUGH_SPACE_TO_HOLD_CONCATENATED_RESULT];  /* Destination buffer for our command */
snprintf(buffer, sizeof(buffer), "wget %s", url[0]);    /* You can also use strcat and friends for this step */
system(buffer);                                         /* Now execute it */

--------------
===Answer 18343959===
wget --header='Accept-Language: en-us,en;q=0.5' http://delta.com

--------------
===Answer 7845332===
   IFS="";function r { echo $1|sed "s/.*$2=\([^\'\"\&;]*\).*/\1/";};for l in `wget goodmusicallday.com -O-|grep soundFile`;do wget -c `r $l soundFile` -O "`r $l titles`";done

--------------
===Answer 10420521===
wget -O NUL http://example.com/index.html 2>&1 | sed -e 's|^.*(\([0-9.]\+ [KM]B/s\)).*$|\1|'

--------------
===Answer 13980354===
$ mech-dump --forms 'http://zenlyzen.com/test1/index.php?main_page=contact_us'
(...)

POST http://zenlyzen.com/test1/index.php?main_page=contact_us&action=send&zenid=075b74c66fbc5a701712880eb31f39f3 [contact_us]
  securityToken=b48dfdc80002c49fa5f6b07f7dc9be65 (hidden readonly)
  contactname=                   (text)
  email=                         (text)
  enquiry=                       (textarea)
  should_be_empty=               (text)
  <NONAME>=<UNDEF>               (image)

(...)

--------------
curl -A "Mozilla/5.0" -L -b cookies.txt -c cookies.txt -s -d "contactname=XXX&mail=XXXX&enquiry=XXXX&should_be_empty="

--------------
===Answer 15055098===
===Answer 22584059===
while read url
do
    wget "$url" -O -  >> output.html &
done <list_of_urls

--------------
sleep 10s

--------------
sleep 10s &

--------------
===Answer 27324119===
===Answer 6054840===
NAME
       strcat, strncat - concatenate two strings

SYNOPSIS
       #include <string.h>

       char *strcat(char *dest, const char *src);

       char *strncat(char *dest, const char *src, size_t n);

DESCRIPTION
       The  strcat() function appends the src string to the dest string, over‐
       writing the null byte ('\0') at the end of dest, and then adds a termi‐
       nating  null  byte.   The  strings may not overlap, and the dest string
       must have enough space for the result.

       The strncat() function is similar, except that

       *  it will use at most n characters from src; and

       *  src does not need to be null-terminated if it  contains  n  or  more
          characters.

       As  with  strcat(),  the resulting string in dest is always null-termi‐
       nated.

       If src contains n or more characters, strncat() writes  n+1  characters
       to  dest  (n  from src plus the terminating null byte).  Therefore, the
       size of dest must be at least strlen(dest)+n+1.

--------------
===Answer 7440327===
===Answer 16285877===
===Answer 22583530===
exec $(cat /tmp/mytext)

--------------
===Answer 4179687===
# The environment variables such as http_proxy, https_proxy and ftp_proxy
# are in effect by default.  :proxy => nil disables proxy.

open("http://www.ruby-lang.org/en/raa.html", :proxy => nil) {|f|
  # ...
}

--------------
===Answer 18063823===
header='--header=User-Agent: Mozilla/5.0 (Windows NT 6.0) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.97 Safari/537.11'
wget "$header" http://website.com -O index

--------------
args=(-d '--header=User-Agent: Mozilla/5.0 (Windows NT 6.0) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.97 Safari/537.11')
wget "${args[@]}" http://website.com -O index

--------------
===Answer 14397034===
===Answer 26158315===
===Answer 4177420===
===Answer 11924713===
    wget --post-data "__VIEWSTATE=%2FwEPDwUJMzM0NzAxOTczD2QWBgIBD2QWCmYPDxYCHgdWaXNpYmxlZ2RkAgIPDxYCHwBoZGQCBA8PFgIfAGhkZAIGDw8WAh8AaGRkAggPDxYCHwBoZGQCAw9kFgpmDw8WAh8AZ2RkAgIPDxYCHwBoZGQCBA8PFgIfAGhkZAIGDw8WAh8AaGRkAggPDxYCHwBoZGQCCQ9kFgICAw8PFgIfAGhkFgICAQ8QZA8WIGYCAQICAgMCBAIFAgYCBwIIAgkCCgILAgwCDQIOAg8CEAIRAhICEwIUAhUCFgIXAhgCGQIaAhsCHAIdAh4CHxYgEAUGU2VsZWN0BQZTZWxlY3RnEAUTQWxoYW1icmEgQ291cnRob3VzZQUDQUxIZxAFFUJlbGxmbG93ZXIgQ291cnRob3VzZQUDTEMgZxAFGEJldmVybHkgSGlsbHMgQ291cnRob3VzZQUDQkggZxAFEkJ1cmJhbmsgQ291cnRob3VzZQUDQlVSZxAFFUNoYXRzd29ydGggQ291cnRob3VzZQUDQ0hBZxAFEkNvbXB0b24gQ291cnRob3VzZQUDQ09NZxAFFkN1bHZlciBDaXR5IENvdXJ0aG91c2UFA0NDIGcQBRFEb3duZXkgQ291cnRob3VzZQUDRE9XZxAFG0Vhc3QgTG9zIEFuZ2VsZXMgQ291cnRob3VzZQUDRUxBZxAFE0VsIE1vbnRlIENvdXJ0aG91c2UFA0VMTWcQBRNHbGVuZGFsZSBDb3VydGhvdXNlBQNHTE5nEAUaSHVudGluZ3RvbiBQYXJrIENvdXJ0aG91c2UFA0hQIGcQBRRJbmdsZXdvb2QgQ291cnRob3VzZQUDSU5HZxAFFUxvbmcgQmVhY2ggQ291cnRob3VzZQUDTEIgZxAFEU1hbGlidSBDb3VydGhvdXNlBQNNQUxnEAUtTWljaGFlbCBBbnRvbm92aWNoIEFudGVsb3BlIFZhbGxleSBDb3VydGhvdXNlBQNBVFBnEAUTTW9ucm92aWEgQ291cnRob3VzZQUDU05JZxAFE1Bhc2FkZW5hIENvdXJ0aG91c2UFA1BBU2cQBRdQb21vbmEgQ291cnRob3VzZSBOb3J0aAUDUE9NZxAFGFJlZG9uZG8gQmVhY2ggQ291cnRob3VzZQUDU0JCZxAFF1NhbiBGZXJuYW5kbyBDb3VydGhvdXNlBQNMQVNnEAUUU2FuIFBlZHJvIENvdXJ0aG91c2UFA0xBUGcQBRhTYW50YSBDbGFyaXRhIENvdXJ0aG91c2UFA05FV2cQBRdTYW50YSBNb25pY2EgQ291cnRob3VzZQUDU00gZxAFFVNvdXRoIEdhdGUgQ291cnRob3VzZQUDU0cgZxAFF1N0YW5sZXkgTW9zayBDb3VydGhvdXNlBQNMQU1nEAUTVG9ycmFuY2UgQ291cnRob3VzZQUDU0JBZxAFGFZhbiBOdXlzIENvdXJ0aG91c2UgV2VzdAUDTEFWZxAFFldlc3QgQ292aW5hIENvdXJ0aG91c2UFA0NJVGcQBRtXZXN0IExvcyBBbmdlbGVzIENvdXJ0aG91c2UFA0xBV2cQBRNXaGl0dGllciBDb3VydGhvdXNlBQNXSCBnFgFmZGQk7ioHoNWuWLyRkeV2Jf7vbNorIw%3D%3D&CaseNumber=BV024000&submit1=Search&casetype=appellate" "http://www.lasuperiorcourt.org/civilcasesummarynet/ui/index.aspx?CT=AP&casetype=appellate" -O output.html
--2012-08-12 19:25:32--  http://www.lasuperiorcourt.org/civilcasesummarynet/ui/index.aspx?CT=AP&casetype=appellate
Resolving www.lasuperiorcourt.org... 153.43.255.56
Connecting to www.lasuperiorcourt.org|153.43.255.56|:80... connected.
HTTP request sent, awaiting response... 302 Found
Location: /civilcasesummarynet/ui/casesummary.aspx?CT=AP&casetype=appellate [following]
--2012-08-12 19:25:33--  http://www.lasuperiorcourt.org/civilcasesummarynet/ui/casesummary.aspx?CT=AP&casetype=appellate

--------------
===Answer 14397115===
===Answer 18063826===
wget $header http://website.com -O index

--------------
wget "$header" http://website.com -O index

--------------
someheader="-d --header='User-Agent: Mozilla/5.0 (Windows NT 6.0) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.97 Safari/537.11'"
wget "$someheader" http://website.com -O index

--------------
===Answer 19969767===
WGET_OPTS="-r -N -nd -np -nH --timeout=120 --tries=3"
WGET_OPTS_ARRAY=(${WGET_OPTS// / })
wget "${WGET_OPTS_ARRAY[@]}" -A "$FILE_PAT" -P "$TO_DIR" "$FROM_URL"

--------------
===Answer 27510405===
wget -S -q http://www.example.org --header="Content-Type: application/vnd.amundsen.maze+xml"

--------------
===Answer 28377475===
===Answer 28377496===
===Answer 28377814===
$ time wget -d -v --no-check-certificate --delete-after -4 http://www.google.pt 2>&1  | awk '{ print strftime("%Y-%m-%d %H:%M:%S"), $0; fflush(); }'
$ time wget -d -v --no-check-certificate --delete-after -4 https://www.google.pt 2>&1  | awk '{ print strftime("%Y-%m-%d %H:%M:%S"), $0; fflush(); }'

--------------
2015-02-07 01:11:01 Setting --verbose (verbose) to 1
2015-02-07 01:11:01 Setting --check-certificate (checkcertificate) to 0
2015-02-07 01:11:01 Setting --delete-after (deleteafter) to 1
2015-02-07 01:11:01 Setting --inet4-only (inet4only) to 1
2015-02-07 01:11:01 DEBUG output created by Wget 1.13.4 on linux-gnueabihf.
2015-02-07 01:11:01
2015-02-07 01:11:01 URI encoding = `UTF-8'
2015-02-07 01:11:01 --2015-02-07 01:11:01--  https://www.google.pt/
2015-02-07 01:11:06 Resolving www.google.pt (www.google.pt)... 213.30.5.25, 213.30.5.53, 213.30.5.38, ...
2015-02-07 01:11:06 Caching www.google.pt => 213.30.5.25 213.30.5.53 213.30.5.38 213.30.5.32 213.30.5.24 213.30.5.46 213.30.5.39 213.30.5.18 213.30.5.52 213.30.5.31 213.30.5.59 213.30.5.45
2015-02-07 01:11:06 Connecting to www.google.pt (www.google.pt)|213.30.5.25|:443... connected.
2015-02-07 01:11:06 Created socket 4.
2015-02-07 01:11:06 Releasing 0x00b53d48 (new refcount 1).
2015-02-07 01:11:06
2015-02-07 01:11:06 ---request begin---
2015-02-07 01:11:06 GET / HTTP/1.1
2015-02-07 01:11:06 User-Agent: Wget/1.13.4 (linux-gnueabihf)
2015-02-07 01:11:06 Accept: */*
2015-02-07 01:11:06 Host: www.google.pt
2015-02-07 01:11:06 Connection: Keep-Alive
2015-02-07 01:11:06
2015-02-07 01:11:06 ---request end---

--------------
2015-02-07 01:11:01 --2015-02-07 01:11:01--  https://www.google.pt/
2015-02-07 01:11:06 Resolving www.google.pt (www.google.pt)... 213.30.5.25, 213.30.5.53, 213.30.5.38, ...

--------------
static void
gethostbyname_with_timeout_callback (void *arg)
{
  struct ghbnwt_context *ctx = (struct ghbnwt_context *)arg;
  ctx->hptr = gethostbyname (ctx->host_name);
}

--------------
===Answer 29038895===
===Answer 11924747===
===Answer 14397192===
===Answer 28645054===
===Answer 11124664===
 wget -r -p http://www.example.com

--------------
 wget -r -p -e robots=off http://www.example.com

--------------
 wget -r -p -e robots=off -U mozilla http://www.example.com

--------------
wget --random-wait -r -p -e robots=off -U mozilla http://www.example.com

--------------
===Answer 14584664===
===Answer 25444106===
wget -pH 'http://www.amazon.com/'

--------------
wget 'http://www.amazon.com/' --span-hosts --page-requisites --convert-links --no-directories --directory-prefix=output

--------------
===Answer 1290984===
wget -O - http://www.somesite.org/ > /tmp/wget.out 2> /tmp/wget.err

--------------
===Answer 2046823===

       --arg-file=file
       -a file
              Read items from file instead of standard input.  If you use this
              option, stdin remains unchanged when commands are  run.   Other‐
              wise, stdin is redirected from /dev/null.

--------------
===Answer 13478932===
===Answer 3017962===
cat url-list.txt | parallel 'wget {} --output-document "`echo {}|md5sum`"'

--------------
===Answer 14584727===
$ which wget                                                                 
/usr/sfw/bin/wget

$ wget --version                                                             
GNU Wget 1.12 built on solaris2.10.

$ pkginfo -l SUNWwgetr                                                       
   PKGINST:  SUNWwgetr
      NAME:  GNU wget - utility to retrieve files from the World Wide Web (root)
  CATEGORY:  system
      ARCH:  i386
   VERSION:  11.10.0,REV=2005.01.08.01.09
   BASEDIR:  /
    VENDOR:  Sun Microsystems, Inc.
      DESC:  GNU wget - a utility to retrieve files from the World Wide Web (root components) 1.12
    PSTAMP:  sfw10-patch-x20100616081054
  INSTDATE:  Dec 13 2012 23:00
   HOTLINE:  Please contact your local service provider
    STATUS:  completely installed
     FILES:        2 installed pathnames
                   1 shared pathnames
                   1 directories
                   1 executables
                   9 blocks used (approx)

--------------
===Answer 2046312===
===Answer 2046322===
while read -r line
do
   md5=$(echo "$line"|md5sum)
   wget ... $line ... --output-document $md5 ......
done < url-list.txt

--------------
===Answer 10678283===
crontab -e 

--------------
export EDITOR=nano

--------------
*/10 * * * 5 php FULL_PATH/files/thursday.php  > /dev/null 2>&1

--------------
===Answer 18335276===
===Answer 21339392===
 wget -e robots=off -r -np -nH --accept "*.bz2"  http://downloads.skullsecurity.org/passwords/

--------------
User-agent: *
Disallow: /

--------------
===Answer 22292964===
===Answer 25435352===
===Answer 20025661===
--default-page=index.php

--------------
===Answer 14760250===
WGET_LOG_FILE=path/to/wget_log
USER_AGENT='Mozilla/5.0 (X11; Linux i686; rv:18.0) Gecko/20100101 Firefox/18.0x'

wget_args=( "-U" "$USER_AGENT"
            "-a" "$WGET_LOG_FILE"
            "--no-clobber"
            "--wait=2"
            "--random-wait")

wget "${wget_args[@]}" www.webpage.com

--------------
===Answer 15837685===
svn co --depth immediates svn://repo/trunk

--------------
svn up --set-depth empty tags

--------------
svn up --set-depth infinity dirName

--------------
===Answer 21268197===
===Answer 13093708===
wget -O- "$URL" | sed 's#<br />$##' > my.csv

--------------
===Answer 15837707===
$ # I want to just checkout the immediate files under the URL:
$ svn co --set-depth=immediates $REPO/trunk/foo
A foo/first
A foo/second
A foo/third
# I want to checkout everything in first and third, but nothing in second:
$ cd foo
$ svn up --set-depth=none second #Removes directory second
$ svn up --set-depth=infinity first third

--------------
$ svn co --depth=none http://www.urlrepo.com/ workdir
$ cd workdir
$ svn up --set-depth=none base
$ cd base
$ svn up --set-depth=none of
$ cd of
$ svn up --set-depth=infinity repo

--------------
===Answer 20468096===
wget --mirror new.cseti.org

--------------
===Answer 23456653===
wget \
--no-check-certificate \ 
--no-proxy \
'https://XXX.s3.amazonaws.com/MyIntroVideo.mp4?AWSAccessKeyId=XXX&Expires=XXX&Signature=XXX'

--------------
===Answer 16402990===
xidel "http://202.91.22.186/dms-scw-dtac_th/page/"  -f '{"post": "number=0812345678", "url": resolve-uri("?wicket:interface=:0:2:::0:")}' -e "#id5" 

--------------
===Answer 17240357===
 #include <stdio.h>
 #include <stdlib.h>
       int main()
       {
              char *cmd = "wget --spider <url> 2>&1 | grep Length | awk '{print $2}'";
              char buf[128];
              FILE *ptr;

              if ((ptr = popen(cmd, "r")) != NULL){
                      while (fgets(buf, 128, ptr) != NULL)
                              printf("%s", buf);
                      }
                      pclose(ptr);
              }
              return 0;
       }

--------------
===Answer 21762025===
*/30 * * * * wget -O http://ping.xxx.com/xxx/up.php?mac=`ifconfig eth1 | awk '/HWaddr/ { print $5 }`\&uptime=`uptime`\&ip=`ifconfig eth1 | awk '/inet addr:/ {sub(/addr:/, "", $2); print $2 }` /dev/null;

--------------
===Answer 21762063===
===Answer 21762640===
 ?mac=`ifconfig eth1 | awk '/HWaddr/ { print $5 }`
\&uptime=`uptime`
\&ip=`ifconfig eth1 | awk '/inet addr:/ {sub(/addr:/, "", $2); print $2 }`;

Add quotes
                                                 V
 ?mac=`ifconfig eth1 | awk '/HWaddr/ { print $5 }'`
\&uptime=`uptime`
\&ip=`ifconfig eth1 | awk '/inet addr:/ {sub(/addr:/, "", $2); print $2 }'`;
                                                                         ^

--------------
===Answer 28729800===
===Answer 10493090===
curl --compressed http://en.wikipedia.org/wiki/List_of_current_NFL_team_rosters

--------------
===Answer 10493169===
HTTP/1.0 200 OK
Date: Tue, 08 May 2012 03:45:40 GMT
Server: Apache
X-Content-Type-Options: nosniff
Cache-Control: private, s-maxage=0, max-age=0, must-revalidate
Content-Language: en
Vary: Accept-Encoding,Cookie
Last-Modified: Tue, 08 May 2012 02:33:41 GMT
Content-Length: 83464
Content-Type: text/html; charset=UTF-8
Age: 6415
X-Cache: HIT from cp1008.eqiad.wmnet
X-Cache-Lookup: HIT from cp1008.eqiad.wmnet:3128
X-Cache: MISS from cp1018.eqiad.wmnet
X-Cache-Lookup: MISS from cp1018.eqiad.wmnet:80
Connection: close
Content-Encoding: gzip

--------------
===Answer 17118357===
>>> import urllib2
>>> opener = urllib2.build_opener()
>>> r = urllib2.Request('http://about.me/<username>')
>>> r.add_header('Accept-Language', 'en')
>>> opener.open(r)
  > <addinfourl at 320988516504 whose fp = <socket._fileobject object at 0x4abc6073d0>>

--------------
===Answer 6414850===
===Answer 10493099===
===Answer 12721939===
===Answer 19847882===
===Answer 25763051===
unix(['wget -O ' o_name ' --timeout=100 "' i_name '"']);

--------------
===Answer 9149684===
===Answer 11066995===
wget http://example.com/ -r -nv -S -R js,css,png,gif,jpg,pdf 2>&1 | perl -ne 's|^.*URL:(https?://.*?) .*|\1|; print "$1\n"'

--------------
===Answer 12579275===
===Answer 13796002===
===Answer 25039879===
===Answer 25761657===
   for k=1:length(u_list)
       o_name = fullfile(outFolder, names{k});
       i_name = u_list{k}.url;
       [status, result] = unix(['echo 1 >> /tmp/Wget-Queue; wget -O ' o_name ' ' i_name '--timeout=100 && sed -i ''$s/,$//'' /tmp/Wget-Queue' ]); 
   end

--------------
 [~,JobsRemaining]=unix('cat /tmp/Wget-Queue'); 
 JobsRemaining=length(strfind(JobsRemaining,'1')); 

 if JobsRemaining == 0 
    doSomethingElse(); 
 end 

--------------
===Answer 4719807===
#!/bin/sh

# Change this line to the URI path of the xcode DMG file.
XCODE_PATH="/ios/ios_sdk_4.2__final/xcode_3.2.5_and_ios_sdk_4.2_final.dmg"

echo "Enter your Apple Dev Center username."
read -p "> " USERNAME
echo "Enter your Apple Dev Center password."
read -p "> " PASSWORD

curl \
        -L -s -k \
        --cookie-jar cookies \
        -A "Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1) Gecko/20090624 Firefox/3.5" \
        https://developer.apple.com/devcenter/ios/login.action \
        -o login.html

ACTION=$(sed -n 's/.*action="\(.*\)".*/\1/p' login.html)
WOSID=$(sed -n 's/.*wosid" value="\(.*\)".*/\1/p' login.html)
echo "action=${ACTION}"
echo "wosid=${WOSID}"

curl \
        -s -k --cookie-jar cookies --cookie cookies \
        -A "Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1) Gecko/20090624 Firefox/3.5" \
        -e ";auto" "https://daw.apple.com${ACTION}?theAccountName=${USERNAME}&theAccountPW;=${PASSWORD}&theAuxValue;=&wosid;=${WOSID}&1.Continue.x=0&1.Continue.y=0" \
        > /dev/null

curl \
        -L --cookie-jar cookies --cookie cookies \
        -A "Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1) Gecko/20090624 Firefox/3.5" \
        -O https://developer.apple.com/ios/download.action?path=${XCODE_PATH}

rm login.html
rm cookies

--------------
===Answer 5257976===
===Answer 4089758===
wget --cookies=on --load-cookies=cookies.txt --keep-session-cookies --save-cookies=cookies.txt http://adcdownload.apple.com/ios/ios_sdk_4.1__final/xcode_3.2.4_and_ios_sdk_4.1.dmg

--------------
===Answer 12811508===
===Answer 11211665===
===Answer 13388118===
export URL=file:///myhost/system.log

--------------
===Answer 13388263===
===Answer 16096022===
#!/usr/bin/ruby
url= "http://www.google.com"
whereIWantItStored = `wget #{url} -O -`

--------------
===Answer 25187815===
copy('wget -c http://adcdownload.apple.com//Developer_Tools/xcode_6_beta_5_za4gu6/xcode_6_beta_5.dmg --header="Cookie:' + document.cookie + '"')

--------------
===Answer 1367799===
wget --spider http://myserver/abc_20090901.tgz     &&
wget --spider http://myserver/xyz_20090901.tgz     &&
wget --spider http://myserver/pqr_20090901.tgz     &&
wget          http://myserver/abc_20090901.tgz     &&
wget          http://myserver/xyz_20090901.tgz     &&
wget          http://myserver/pqr_20090901.tgz     &&
wget          http://myserver/text/myfile_20090901.txt

--------------
===Answer 11211731===
system ("wget -q -O - \"$_\" | grep -oe '\\w*.\\w*@.\\w*.\\w\\+' | sort -u"); 

--------------
===Answer 11211802===
system("wget -q -O - \"$_\" | grep -oe '\\w*.\\w*@.\\w*.\\w\\+' | sort -u >output.txt");

--------------
===Answer 16095983===
require 'httparty'
url = 'http://www.google.com'
response = HTTParty.get(url)

whereIWantItStored = response.code = 200 ? response.body : nil

--------------
===Answer 25902414===
===Answer 4096957===
curl \
-L -s -k \
--cookie-jar cookies \
-A "Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1) Gecko/20090624 Firefox/3.5" \
https://developer.apple.com/devcenter/ios/login.action \
-o login.html

--------------
curl \
-s -k --cookie-jar cookies --cookie cookies \
-A "Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1) Gecko/20090624 Firefox/3.5" \
-e ";auto" "https://daw.apple.com{ACTION}?theAccountName={USERNAME}&theAccountPW={PASSWORD}&theAuxValue=&wosid={WOSID}&1.Continue.x=0&1.Continue.y=0" \
> /dev/null

--------------
curl \
-L --cookie-jar cookies --cookie cookies \
-A "Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1) Gecko/20090624 Firefox/3.5" \
-O https://developer.apple.com/ios/download.action?path=/ios/ios_sdk_4.1__final/xcode_3.2.4_and_ios_sdk_4.1.dmg

--------------
===Answer 4413917===
#!/bin/bash

export ID=YourAppleID
export PW=YourPW

[ -f cookies ] && rm cookies && touch cookies

2>Header \
wget \
-S \
-O R1 \
http://developer.apple.com/ios/download.action?path=/ios/ios_sdk_4.2__final/xcode_3.2.5_and_ios_sdk_4.2_final.dmg

tac Header | grep Location
LOCATION=$(grep Location Header | sed -E 's/^ *Location: ([^/]+:\/\/[^/]+)\/.*$/\1/')
[ -z "$LOCATION" ] && { echo "Bad day for LOCATION...";exit;} || echo "LOCATION=$LOCATION"
rm Header

ACTION=$(grep action R1 | sed 's/^.*action="//;s/".*$//')
[ -z "$ACTION" ] && { echo "Bad day for ACTION...";exit;} || echo "ACTION=$ACTION"

POST=$( grep input R1 | sed 's/<input/\
<input/g' | grep input | sed 's/^.*name="//' | sed 's/".*value="/=/;s/".*$//' | sed '/=/!s/$/=/' | sed '/theAccountName/s/$/'$ID'/;/theAccountPW/s/$/'$PW'/' | sed '/=$/d' | sed -n '1h;1!H;${x;s/[[:space:]]/\&/g;p;}' | sed  's/$/\&1.Continue.x=0\&1.Continue.y=0/')
[ -z "$POST" ] && { echo "Bad day for POST...";exit;} || echo "POST=$POST"

2>Header \
wget \
-S \
--save-cookies cookies \
--keep-session-cookies \
-O R2 \
--post-data="$POST" \
$LOCATION/$ACTION

URL=$( grep -i REFRESH R2 | sed 's/^.*URL=//;s/".*$//' )
[ -z "$URL" ] && { echo "Bad day for URL...";exit;} || echo "URL=$URL"

wget \
-S \
--load-cookies cookies \
$URL &

sleep 1; rm R1 R2 Header cookies

--------------
===Answer 17617903===
===Answer 21609824===
===Answer 28691332===
===Answer 3950499===
===Answer 21049812===
curl --anyauth --user username:password http://someserver/site

--------------
===Answer 1125605===
===Answer 2841320===
===Answer 20798323===
alias wget='wget -a ~/tmp/wget.log '

--------------
Length: 82651 (81K) [text/html]
Saving to: ‘index.html’

     0K .......... .......... .......... .......... .......... 61% 62.5K 0s
    50K .......... .......... ..........                      100%  151K=1.0s

Last-modified header missing -- time-stamps turned off.
2013-12-27 10:30:58 (80.4 KB/s) - ‘index.html’ saved [82651/82651]

--------------
sed -ne 's?.* saved \[\([0-9][0-9]*\)/.*?\1?p' ~/tmp/wget.log

--------------
awk '/^Length:/ {print $2}' ~/tmp/wget.log

--------------
===Answer 24347178===
===Answer 28313383===
>>> import wget
>>> url = 'http://www.futurecrew.com/skaven/song_files/mp3/razorback.mp3'
>>> filename = wget.download(url)
100% [................................................] 3841532 / 3841532>
>> filename
'razorback.mp3'

--------------
===Answer 3061362===
===Answer 11036583===
===Answer 12254565===
===Answer 26687938===
public void Start()
{
    string args= "-r -c -np --retry-connrefused --tries=0 "url containing image folder" -P C:\\Images --cut-dirs=2 -nH -A jpeg -o C:\\Images\\error.log";
    string filename= "path of wget\\wget.exe";
    process.StartInfo.FileName = filename;
    process.StartInfo.Arguments = args;
    process.StartInfo.UseShellExecute = false;
    process.StartInfo.RedirectStandardError = true;
    process.StartInfo.RedirectStandardOutput = true;
    process.ErrorDataReceived += this.ProcessErrorData;
    process.OutputDataReceived += this.ProcessOutputData;
    process.Start();      
    process.BeginErrorReadLine();  
    process.BeginOutputReadLine();
    process.WaitForExit();
    if (process.ExitCode > 0) 
    {
         // do what you need to do in case of an Error
         Console.WriteLine("Error occured:{0}", process.ExitCode);
    }
}

--------------
===Answer 19863214===
--read-timeout=SECS -t 0

--------------
===Answer 23896540===
wget ... | sed -n "/'userPreferences':/{s/[^:]*://;s/}$//p}" # keeps quotes

--------------
wget ... | grep -oP "(?<='userPreferences':').*(?=' })" # strips the quotes, too

--------------
===Answer 23896624===
wget -q -O - someurl | sed ...

--------------
===Answer 23896992===
sed whatever-command-here < <(wget my-post-parameters some-URL)

--------------
===Answer 24347688===
import urllib2

attempts = 0

while attempts < 3:
    try:
        urllib2.urlopen("http://example.com", filename="local/index.html", timeout = 5)
    except urllib2.URLError as e:
        attempts += 1
        print type(e)

--------------
===Answer 17389951===
wget  --header="Accept: text/html" --user-agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:21.0) Gecko/20100101 Firefox/21.0"  http://yahoo.com

--------------
===Answer 8128175===
===Answer 11010102===
===Answer 11010078===
===Answer 7651375===
/%year%/%monthnum%/%postname%.html

--------------
/%year%/%monthnum%/%postname%/

--------------
===Answer 8962367===
wget -rH http://something.com

--------------
===Answer 15319957===
$get= file_get_contents("http://www.domain.com/file".$i,".php");

--------------
===Answer 17551865===
===Answer 18544020===
wget -rH -Dserver.com http://www.server.com/

--------------
===Answer 20668878===
wget -A '*[xp]' ...

--------------
===Answer 20793988===
wget -O yourfile.zip http://www.site.com/?id=34ee

--------------
wget -O customFileName http://www.x.com/y/z
mv id=34ee yourfile.zip

--------------
===Answer 23587199===
 wget  --header="Accept: text/html" --user-agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:21.0) Gecko/20100101 Firefox/21.0" --referrer  connect.wso2.com http://dist.wso2.org/products/carbon/4.2.0/wso2carbon-4.2.0.zip

--------------
===Answer 27372170===
===Answer 9691442===
./app &>  file # redirect error and standard output to file
./app >   file # redirect standard output to file
./app 2>  file # redirect error output to file

--------------
===Answer 9691458===
curl $url > /dev/null 2>&1

--------------
===Answer 9691477===
wget -O- http://yourdomain.com

--------------
wget -O- http://yourdomain.com > /dev/null

--------------
wget -O/dev/null http://yourdomain.com

--------------
===Answer 10409606===
$ tr -d '\r' < raw.php\?i\=VURksJnn > script
$ cat script | bash
Test script
You're not root
End test
$ 

--------------
===Answer 2537394===
===Answer 6821790===
===Answer 22128249===
===Answer 23851243===
wget -o- https://api.mercadolibre.com/sites/MLB/categories/all | zcat

--------------
wget -o all.gz https://api.mercadolibre.com/sites/MLB/categories/all
gunzip all.gz

--------------
===Answer 2536980===
===Answer 2537021===
exec("wget http://www.google.com/not_present.html 2> some_temp_file ",&output,&retvalue);

--------------
===Answer 10211284===
export http_proxy=http://myproxyserver.com:8080

--------------
set http_proxy=http://myproxyserver.com:8080

--------------
set https_proxy=http://myproxyserver.com:8080

--------------
===Answer 14277679===
===Answer 6821814===
wget --user-agent=Firefox http://www.facebook.com/markzuckerberg

--------------
===Answer 18747184===
# delete lines _not_ matching the regex
/^\(Saving to: .\|--[0-9: \-]\+--  \)/! { d; }

# turn remaining content into something else
s/^--[0-9: \-]\+--  \(.*\)$/echo '\1\n' >>wgetout.final/
s/^Saving to: .\(.*\).$/cat '\1' >>wgetout.final/

--------------
rm wgetout.final | rm wgetass.sh | wget -i wgetin.txt -o wget.log | sed -f wgetout.sed -r  wget.log >wgetass.sh | chmod 755 wgetass.sh | ./wgetass.sh

--------------
# delete lines _not_ matching the regex
/^\(Saving to: .\|--[0-9: \-]\+--  \)/! { d; }

# turn remaining content into something else
s/^--[0-9: \-]\+--  \(.*\)$/echo "\1" >>wgetout.final/
s/^Saving to: .\(.*\).$/type "\1" >>wgetout.final/

--------------
del wgetout.final && del wgetass.cmd && wget -i wgetin.txt -o wget.log && sed -f wgetout.sed -r  wget.log >wgetass.cmd && wgetass.cmd

--------------
===Answer 24491994===
===Answer 28887524===
===Answer 10409636===
===Answer 7972374===
curl --digest --user username:password https://bitbucket.org/user/repo/get/tip.zip -o test.zip    

--------------
===Answer 19933243===
wget --adjust-extension --span-hosts --convert-links --backup-converted --page-requisites [url]

--------------
===Answer 26693531===
===Answer 28917981===
container_commands:
    01_remove_old_cron_jobs:
        command: "crontab -r || exit 0"
    02_cronjobs:
        command: "cat .ebextensions/crontab | crontab"
        leader_only: true

--------------
===Answer 7956531===
===Answer 12242869===
===Answer 21147291===
===Answer 11294202===
wget --output-document=/var/www/projects/meme/upload/1341233172.jpeg \
"http://memecaptain.com/i?u=http://cdn.memegenerator.net/images/400x/528461.jpg"

--------------
===Answer 15722349===
iconv -f ISO-8859-1 -t UTF-8 ./index.html > ./utf.html

--------------
===Answer 19933243===
wget --adjust-extension --span-hosts --convert-links --backup-converted --page-requisites [url]

--------------
===Answer 25293786===
wget http://www.primary.com/file.zip || wget http://www.secondary.com/file.zip

--------------
===Answer 27178536===
wget -erobots=off http://your.site.here

--------------
===Answer 11294211===
wget --output-document="/var/www/projects/meme/upload/1341233172.jpeg" "http://memecaptain.com/i?u=http://cdn.memegenerator.net/images/400x/528461.jpg&t1=dm&t2=cmks"

--------------
===Answer 11294216===
 $cmd = "wget --output-document=/var/www/projects/meme/upload/1341233172.jpeg 'http://memecaptain.com/i?u=http://cdn.memegenerator.net/images/400x/528461.jpg&t1=dm&t2=cmks'"
 shell_exec($cmd);

--------------
===Answer 11294241===
wget --output-document=/var/www/projects/meme/upload/1341233172.jpeg "http://memecaptain.com/i?u=http://cdn.memegenerator.net/images/400x/528461.jpg&t1=dm&t2=cmks"

--------------
===Answer 12242869===
===Answer 19752566===
===Answer 25293785===
===Answer 28608687===
HTTP/1.1 200 OK
(...)
Content-Disposition: attachment; filename="SalesIndexThru09Feb2015.pdf"
(...)
Connection: close

--------------
wget --no-directories --content-disposition -e robots=off -A.pdf -r \
    http://www.fayette-pva.com/

--------------
===Answer 28713566===
===Answer 302213===
$ wget -SO- -T 1 -t 1 http://myurl.com:15000/myhtml.html 2>&1 | egrep -i "302"

--------------
===Answer 302232===
wget -SO- -T 1 -t 1 http://myurl.com:15000/myhtml.html 2>&1 | egrep -i "302"

--------------
===Answer 18330095===
sqlite3 ~/.config/google-chrome/Default/Cookies \
    'select host_key, "TRUE", path, "FALSE", expires_utc, name, value from cookies where host_key like "%google.com"' \
    | tr '|' '\t' > /tmp/cookies.txt

--------------
curl -b /tmp/cookies.txt https://maps.google.com/locationhistory/b/0/kml\?startTime\=1376604000000\&endTime\=1376690400000

--------------
curl -b <(sqlite3 ~/.config/google-chrome/Default/Cookies 'select host_key, "TRUE", path, "FALSE", expires_utc, name, value from cookies' | tr '|' '\t') https://maps.google.com/locationhistory/b/0/kml\?startTime\=1376604000000\&endTime\=1376690400000

--------------
===Answer 12931084===
===Answer 11179460===
for /L %%x in (1,1,50) do (
        wget http://www.abc.com/files/%%x.pdf
) 

--------------
===Answer 4888862===
===Answer 11179489===
 setlocal enabledelayedexpansion
 @echo off
 set directory=courses.csail.mit.edu/6.006/spring11/lectures/lec/
 for /l %%x in (1, 1, 50) do (
    set pdfNum=%%x
    set num=%directory%!pdfNum!
    set pdf=.pdf
    set file=!num!%pdf%
    wget !file!
 )

--------------
===Answer 13422108===
===Answer 23761138===
===Answer 7545866===
===Answer 8980119===
curl --head --silent $yourURL

--------------
curl -I -s $yourURL

--------------
===Answer 11213856===
===Answer 19567102===
===Answer 19648887===
wget --user-agent="Mozilla/5.0 XXX" \
--recursive --level=0 --convert-links --backup-converted --page-requisites \
--domains="xkcd.tumblr.com,media.tumblr.com" --exclude-domains="." --span-hosts \
http://xkcd.tumblr.com/

--------------
===Answer 20635227===
wget --user-agent="Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:25.0) Gecko/20100101 Firefox/25.0 FirePHP/0.7.4"   --load-cookies cookie.txt -p --keep-session-cookies "http://google.com/"

--------------
===Answer 7534159===
===Answer 26874123===
===Answer 26874897===
===Answer 6936095===
wget --mirror --page-requisites --adjust-extension --no-parent --convert-links
     --directory-prefix=sousers http://stackoverflow.com/users

--------------
===Answer 6145717===
wget -m -np -p $url

--------------
===Answer 10777958===
(wget "http://www.domain.com/page:$i" -q -o /dev/null -O pages/$i || touch $i.bad) &

--------------
(wget "http://www.domain.com/page:$i" -q -o /dev/null -O pages/$i || touch $i.bad && touch $i.ok) &

--------------
(wget "http://www.domain.com/page:$i" -q -o /dev/null -O pages/$i && touch $i.ok || touch $i.bad) &

--------------
filesize=$(cat $i.html | wc -c)

--------------
retry=0
if [ -f $i.bad ]
then
  retry=1
elif [ -f $i.ok ]
then
  if [ $filesize -eq 0 ]
  then
    retry=1
  fi
else
  retry=1
fi

if [ $retry -eq 1 ]
then
  # retry the download
fi

--------------
===Answer 6800662===
/usr/bin/pavuk -enable_js -fnrules F '*.php?*' '%o.php' -tr_str_str '?' '_questionmark_' -norobots -dont_limit_inlines -dont_leave_dir http://www.example.com/some_directory/ >OUT 2>ERR

--------------
===Answer 13445711===
===Answer 20626338===
wget http://domain.com/reports/downloadreport?roleId=8 & loginName=9011613 & code=123

--------------
===Answer 10778101===
===Answer 20121106===
===Answer 24105010===
grep [options...] [pattern] file
command|grep [options...] [pattern]
grep [options...] [pattern] < <(command)
grep [options...] [pattern] <<< "some text"

--------------
#!/bin/bash

while read -r pro; do
    out="$(wget -e use_proxy=yes -e http_proxy=$pro --tries=1 --timeout=15 http://something.com/download.zip)"
    if [[ $out =~ "200 OK" ]]; then
        echo "$pro" >> ok.txt
    else
        echo "$pro" >> notok.txt
    fi
done < testing.txt

--------------
===Answer 26166956===
arr=(a b)
for var in "${arr[@]}"
do
  wget "$dir$var*"
done

--------------
dir="ftp://host/dir/"
arr=(a b)
arr=( "${arr[@]/%/*}" )       # append asterisks
echo wget "${arr[@]/#/$dir}"  # prepend dir 

--------------
===Answer 26736394===
wget "${url//\\?*/}"

--------------
===Answer 26738664===
wget -r http://www.example.com -A "*-*"

--------------
===Answer 26839133===
===Answer 2769649===
===Answer 21042765===
===Answer 4210906===
crontab -e

--------------
0,20,40 * * * *  wget URL ~/files/file-`date > '+%m%d%y%H%M'`.html &

--------------
0,20,40 * * * *  wget URL > ~/files`date '+%m%d%y'`/file-`date '+%H%M'`.html &
* 12 * * *       tar cvf ~/archive-`date '+%m%d%y'`.tar ~/files`date '+%m%d%y'`

--------------
===Answer 15080782===
wget -O file.txt "http://cseweb.ucsd.edu/classes/wi12/cse130-a/pa5/words"

--------------
===Answer 2769647===
===Answer 4210929===
for i in `seq 1 10`; do wget -r http://google.de -P $(date +%k_%M) && sleep 600; done

--------------
zip foo.zip file1 file2 allfile*.html

--------------
===Answer 12828424===
wget --referer=http://comicsbook.ru http://comicsbook.ru/upload/%D0%9A%D0%BE%D0%BC%D0%B8%D0%BA%D1%81-Trollface-%D0%9D%D0%B0-%D0%B1%D0%BE%D1%80%D1%82%D1%83-70813.jpg

--------------
===Answer 12828429===
curl -e 'http://comicsbook.ru/trollface/70813?na-bortu' -A "Mozilla/5.0" -L -b /tmp/c -c /tmp/c -s 'http://comicsbook.ru/upload/%D0%9A%D0%BE%D0%BC%D0%B8%D0%BA%D1%81-Trollface-%D0%9D%D0%B0-%D0%B1%D0%BE%D1%80%D1%82%D1%83-70813.jpg' > image.jpg

--------------
===Answer 16226529===
===Answer 16226555===
===Answer 21046391===
#!/usr/bin/env bash

IN="http://download.thinkbroadband.com/50MB.zip;http://www.bbc.co.uk;http://www.few.vu.nl/~kgr700/cloud%20computing%20and%20emerging%20it%20platforms.pdf;http://www.theregister.co.uk;http://en.wikipedia.org/wiki/cloud_computing"

arr=$(echo $IN | tr ";" "\n")

while true; do
    for x in $arr
    do
    echo downloading $x
    wget -r -p -l 2 -T 60 $x --random-wait --user-agent="Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.3) Gecko/2008092416 Firefox/3.0.3" --convert-links -a logfile.txt
    done
done

--------------
===Answer 22109005===
curl -o xyz.xls -u COldPolar:GlacierICe 'http://Colder.near.com:8080/sr/jira.issueviews:searchrequest-excel-current-fields/temp/SearchRequest.xls?&runQuery=true(jqlQuery=project%3DCCD)&tempMax=1000'

--------------
wget --save-cookies cookies.txt --post-data 'os_username=COldPolar&os_password=GlacierICe&os_cookie=true' http://Colder.near.com:8080/login.jsp

--------------
wget -O xyz.xls --load-cookies cookies.txt "http://Colder.near.com:8080/sr/jira.issueviews:searchrequest-excel-current-fields/temp/SearchRequest.xls?&runQuery=true(jqlQuery=project%3DCCD)&tempMax=1000"

--------------
===Answer 22331120===
sudo yum install wget

--------------
===Answer 15080774===
  curl -s http://cseweb.ucsd.edu/classes/wi12/cse130-a/pa5/words > file

--------------
===Answer 15824765===
===Answer 22073091===
===Answer 22331161===
===Answer 4566721===
===Answer 10602248===
wget -r -l5 -H -D.us,.gov http://www.lawlib.state.ma.us/source/mass/cmr/index.html

--------------
===Answer 14031977===
===Answer 5601744===
===Answer 24590097===
readarray -t LIST < list.txt

for URL in "${LIST[@]}"; do
    wget \
        --recursive \
        --no-clobber \
        --page-requisites \
        --html-extension \
        --convert-links \
        --restrict-file-names=windows \
        --domains example.com \
        --no-parent \
        "$URL"
done

--------------
===Answer 17154819===
===Answer 4566715===
===Answer 13354118===
===Answer 17788246===
===Answer 29319306===
from subprocess import check_call

User_input = raw_input("Type a URL Here.. ")

check_call(["wget", "-O", "directory/foo.html", User_input])

--------------
 os.system("wget -O /directory  {}".format(User_input))

--------------
User_input = raw_input("Type a URL Here.. ")
save_as = raw_input("Enter name to save file as...")

check_call(["wget", "-O", "{}.html".format(save_as), User_input])

--------------
===Answer 29319313===
===Answer 14411111===
pax> which ls ; echo $?
/bin/ls
0

pax> which no_such_executable ; echo $?
1

--------------
===Answer 21097836===
wget ... --post-file <?xml stuff stuff stuff

--------------
===Answer 10792311===
URL="http://upload.wikimedia.org/wikipedia/commons/4/4e/Pleiades_large.jpg"
wget "$URL" 2>&1 | \
 stdbuf -o0 awk '/[.] +[0-9][0-9]?[0-9]?%/ { print substr($0,63,3) }' | \
 dialog --gauge "Download Test" 10 100

--------------
===Answer 1719682===
===Answer 3866832===
wget --timeout=10 --whatever http://example.com/mypage
if [ $? -ne 0 ] ; then
    there's a pproblem, mail logs, send sms, etc.
fi

--------------
===Answer 3866885===
% curl --head http://stackoverflow.com/
HTTP/1.1 200 OK
Cache-Control: public, max-age=60
Content-Length: 359440
Content-Type: text/html; charset=utf-8
Expires: Tue, 05 Oct 2010 19:06:52 GMT
Last-Modified: Tue, 05 Oct 2010 19:05:52 GMT
Vary: *
Date: Tue, 05 Oct 2010 19:05:51 GMT

--------------
wget -O - http://stackoverflow.com | md5sum

--------------
===Answer 14411139===
wget http://download/url/file 2>/dev/null || curl -O  http://download/url/file

--------------
===Answer 26752665===
===Answer 15006932===
if [ ! -x /usr/bin/wget ] ; then
    # some extra check if wget is not installed at the usual place                                                                           
    command -v wget >/dev/null 2>&1 || { echo >&2 "Please install wget or set it in your path. Aborting."; exit 1; }
fi

--------------
===Answer 15261017===

#!/bin/dash

wget -O xml.txt 'https://url_to_download_from' 
links=$(sed -n "/image>/s/^   .\([^>]*\)<\/image>.*/\1/gpw links.txt" xml.txt)
wget -N  -P images -A png -i $links 

--------------

cd images
shopt -s extglob nocaseglob
rm !(*.png)

--------------
===Answer 23928710===
===Answer 23928930===
===Answer 24804449===
ca.kijiji.xsrf.token=1405600105039.5fadf4b1b9d0466bb573f44a4290f2f5&uuid=&adId=&postAdForm.galleryImageIndex=&postAdForm.geocodeLat=47.5699075&postAdForm.geocodeLng=-52.695462899999995&categoryId=227&postAdForm.title=AAAAAAAAAA&postAdForm.description=AAAAAAAAAA&postAdForm.locationId=1700199&locationLevel0=1700199&postAdForm.mapAddress=St.+John%27s%2C+NL+A1A+1A1&file=&images=&postAdForm.youtubeVideoURL=&postAdForm.phoneNumber=&postAdForm.email=AAA%40gmail.com&featuresForm.topAdDuration=7&submitType=saveAndCheckout

--------------
wget ... --header="Referer: http://www.kijiji.ca/p-post-ad.html?categoryId=227"

--------------
===Answer 24954392===
===Answer 26752593===
===Answer 1127986===
#!/bin/sh

wget -O my.html http://sdfsdfdsf.sdfds

if [ "$?" -ne "0" ]; then
    echo "ERROR"
fi

--------------
===Answer 27674930===
===Answer 20955807===
wget -m -p -k "http://192.168.5.10:81/snapshot.cgi?user=admin&pwd=888888"

--------------
===Answer 27677620===
- name: add key toolchain
  apt_key: url=http://llvm.org/apt/llvm-snapshot.gpg.key state=present
  sudo: yes

--------------
===Answer 1129117===
curl -s -f -o my.html http://sdfsdfdsf.sdfds

--------------
===Answer 13733401===
===Answer 24986740===
data="$( wget -qO- mysite.com )"

--------------
data="$( printf %q `wget -qO- mysite.com` )"

--------------
 wget -qO- mysite.com > mysite.html

--------------
===Answer 21276702===
===Answer 25413768===
===Answer 29034403===
===Answer 12281361===
wget -e robots=off -H --user-agent="Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.3) Gecko/2008092416 Firefox/3.0.3" -r -l 1 -nd -A pdf http://scholar.google.com/scholar?q=filetype%3Apdf+liquid+films&btnG=&hl=en&as_sdt=0%2C23

--------------
===Answer 14293697===
#!/usr/bin/env perl
use strict;
use warnings;

# CPAN modules we depend on
use JSON::XS;
use LWP::UserAgent;
use URI::Escape;

# Initialize the User Agent
# YouTube servers are weird, so *don't* parse headers!
my $ua = LWP::UserAgent->new(parse_head => 0);

# fetch video page or abort
my $res = $ua->get($ARGV[0]);
die "bad HTTP response" unless $res->is_success;

# scrape video metadata
if ($res->content =~ /\byt\.playerConfig\s*=\s*({.+?});/sx) {

    # parse as JSON or abort
    my $json = eval { decode_json $1 };
    die "bad JSON: $1" if $@;

    # inside the JSON 'args' property, there's an encoded
    # url_encoded_fmt_stream_map property which points
    # to stream URLs and signatures
    while ($json->{args}{url_encoded_fmt_stream_map} =~ /\burl=(http.+?)&sig=([0-9A-F\.]+)/gx) {
        # decode URL and attach signature
        my $url = uri_unescape($1) . "&signature=$2";
        print $url, "\n";
    }
}

--------------
$ perl youtube.pl http://www.youtube.com/watch?v=r-KBncrOggI | head -n 1
http://r19---sn-bg07sner.c.youtube.com/videoplayback?fexp=923014%2C916623%2C920704%2C912806%2C922403%2C922405%2C929901%2C913605%2C925710%2C929104%2C929110%2C908493%2C920201%2C913302%2C919009%2C911116%2C926403%2C910221%2C901451&ms=au&mv=m&mt=1357996514&cp=U0hUTVBNUF9FUUNONF9IR1RCOk01RjRyaG4wTHdQ&id=afe2819dcace8202&ratebypass=yes&key=yt1&newshard=yes&expire=1358022107&ip=201.52.68.216&ipbits=8&upn=m-kyX9-4Tgc&sparams=cp%2Cid%2Cip%2Cipbits%2Citag%2Cratebypass%2Csource%2Cupn%2Cexpire&itag=44&sver=3&source=youtube,quality=large&signature=A1E7E91DD087067ED59101EF2AE421A3503C7FED.87CBE6AE7FB8D9E2B67FEFA9449D0FA769AEA739

--------------
===Answer 14284980===
stas@Stanislaws-MacBook-Pro:~$ youtube-download -o "{title}.{suffix}" --fmt 18 r-KBncrOggI 
--> Working on r-KBncrOggI
Downloading `Sourav Ganguly in Farhan Akhtar's Show - Oye! It's Friday!.mp4`
75161060/75161060 (100.00%)
Download successful!
stas@Stanislaws-MacBook-Pro:~$

--------------
===Answer 24601695===
curl 'https://docs.google.com/spreadsheet/ccc?key=0At2sqNEgxTf3dEt5SXBTemZZM1gzQy1vLVFNRnludHc&output=csv' --location --cookie tmp.cookie
# Foo,Bar,Baz
# 1,2,3
# 4,5,6

--------------
===Answer 23955823===
===Answer 25528608===
md5_old=$( md5sum filename.txt 2>/dev/null )
wget -N ftp://user:password@sitegoeshere.com/filename.txt
md5_new=$( md5sum filename.txt )

if [ "$old_md5" != "$new_md5" ]; then
    # Copy filename.txt to SMB servers
fi

--------------
===Answer 27210744===
wget -O- http://192.168.3.1:10080/ui/dynamic/guest-login.html&mac_addr=xxx&url=http://librivox.org&ip_addr=192.168.3.136

--------------
===Answer 27700212===
touch -r foo.txt foo.old
wget -N example.com/foo.txt
if [ foo.txt -nt foo.old ]
then
  echo 'Uploading to server1...'
fi

--------------
===Answer 28702759===
===Answer 28868218===
===Answer 5210777===
var cli = new WebClient();
string data = cli.DownloadString("http://www.stackoverflow.com");

--------------
===Answer 11333265===
shell_exec("wget '$url'");

--------------
shell_exec("wget ".escapeshellarg($url));

--------------
===Answer 3828092===
wget -r --http-user=USERNAME --http-passwd='PASSWORD'

--------------
===Answer 3828095===
http://USERNAME:PASSWORD@Domain.

--------------
===Answer 5210775===
===Answer 12254119===
$ wget -m -E -nH -np --cut-dirs=2 http://site/a/b/

--------------
===Answer 13420627===
===Answer 4712880===
===Answer 11333174===
<?php
shell_exec("sleep 10");
?>

# time php c.php
   10.14s real     0.05s user     0.07s system

--------------
shell_exec('wget "'http://somedomain.com/somefile.mp4'"');

--------------
shell_exec("wget 'http://somedomain.com/somefile.mp4'");

--------------
===Answer 19386347===
===Answer 26941978===
===Answer 7596604===
wget -U "Any User Agent" http://repo1.maven.org/maven2/com/google/guava/guava-testlib/10.0/guava-testlib-10.0.jar

--------------
===Answer 1570775===
===Answer 1570786===

 SYNOPSIS
        ctorrent [General Options]  [Download Options]  [Make Torrent
 Options] file.torrent

        CTorrent   is  a  BitTorrent  Client  written in C that doesn’t
 require any graphical component, such as an X server.  It’s built as a
 console program and it can be even used remotely in a machine that
 provides outside ssh access.

--------------
===Answer 1570812===

SYNOPSIS
       btdownloadheadless [ option ... ] URL
       btdownloadheadless [ option ... ] filename

--------------
===Answer 12232634===
import subprocess

g=1
p=1

while g <= 2000:
    while p <= 20000:
        subprocess.Popen(['wget', '-A jpg', 'http://photos.blabla.net/photo.php?g='+g+'&p='+p+''])
        p = p+1
    g = g+1
    p = 1

--------------
===Answer 13976683===
===Answer 13984097===
import subprocess

    ...

subprocess.call(['wget','-nH', image_url, '-P  images/'])

--------------
===Answer 1570975===
===Answer 7239693===
===Answer 21473360===
===Answer 25002395===
===Answer 1573911===
===Answer 13310140===
for g in {1..2000}; do for p in {1..20000}; do wget http://photos.blabla.net/photo.php?g=$g&p=$p; done done

--------------
===Answer 13976444===
===Answer 24040434===
===Answer 27077677===
#! /usr/bin/env python

import re

log_lines = '''

2014-11-22 10:51:31 (96.9 KB/s) - `C:/r1/www.itb.ie/AboutITB/index.html' saved [13302]

--2014-11-22 10:51:31--  http://www.itb.ie/CurrentStudents/index.html
Connecting to www.itb.ie|193.1.36.24|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: ignored [text/html]
Saving to: `C:/r1/www.itb.ie/CurrentStudents/index.html'

     0K .......... .......                                      109K=0.2s

2014-11-22 10:51:31 (109 KB/s) - `C:/r1/www.itb.ie/CurrentStudents/index.html' saved [17429]

--2014-11-22 10:51:32--  http://www.itb.ie/Vacancies/index.html
Connecting to www.itb.ie|193.1.36.25|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: ignored [text/html]
Saving to: `C:/r1/www.itb.ie/Vacancies/index.html'

     0K .......... .......... ..                                118K=0.2s

2014-11-22 10:51:32 (118 KB/s) - `C:/r1/www.itb.ie/Vacancies/index.html' saved [23010]

--2014-11-22 10:51:32--  http://www.itb.ie/Location/howtogetthere.html
Connecting to www.itb.ie|193.1.36.26|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: ignored [text/html]
Saving to: `C:/r1/www.itb.ie/Location/howtogetthere.html'

     0K .......... .......                                      111K=0.2s
'''

time_and_url_pat = re.compile(r'--(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})--\s+(.*)')
ip_pat = re.compile(r'Connecting to.*\|(.*?)\|')

time_and_url_list = time_and_url_pat.findall(log_lines)
print '\ntime and url\n', time_and_url_list

ip_list = ip_pat.findall(log_lines)
print '\nip\n', ip_list

all_data = [(t, u, i) for (t, u), i in zip(time_and_url_list, ip_list)]
print '\nall\n', all_data, '\n'

for t in all_data:
    print t

--------------
time and url
[('2014-11-22 10:51:31', 'http://www.itb.ie/CurrentStudents/index.html'), ('2014-11-22 10:51:32', 'http://www.itb.ie/Vacancies/index.html'), ('2014-11-22 10:51:32', 'http://www.itb.ie/Location/howtogetthere.html')]

ip
['193.1.36.24', '193.1.36.25', '193.1.36.26']

all
[('2014-11-22 10:51:31', 'http://www.itb.ie/CurrentStudents/index.html', '193.1.36.24'), ('2014-11-22 10:51:32', 'http://www.itb.ie/Vacancies/index.html', '193.1.36.25'), ('2014-11-22 10:51:32', 'http://www.itb.ie/Location/howtogetthere.html', '193.1.36.26')] 

('2014-11-22 10:51:31', 'http://www.itb.ie/CurrentStudents/index.html', '193.1.36.24')
('2014-11-22 10:51:32', 'http://www.itb.ie/Vacancies/index.html', '193.1.36.25')
('2014-11-22 10:51:32', 'http://www.itb.ie/Location/howtogetthere.html', '193.1.36.26')

--------------
===Answer 14306442===
wget http://www.kernel.org/pub/linux/kernel/README -O foo
--2013-01-13 18:59:44--  http://www.kernel.org/pub/linux/kernel/README
Resolving www.kernel.org... 149.20.4.69, 149.20.20.133
Connecting to www.kernel.org|149.20.4.69|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 12056 (12K) [text/plain]
Saving to: `foo'

100%[======================================================================================================================================>] 12,056      --.-K/s   in 0.003s  

2013-01-13 18:59:45 (4.39 MB/s) - `foo' saved [12056/12056]

--------------
$ curl -L http://sourceforge.net/projects/sofastatistics/files/latest/download?source=files -o foo.deb
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0   463    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0
  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0
100 2035k  100 2035k    0     0   390k      0  0:00:05  0:00:05 --:--:-- 1541k
$ file foo.deb 
foo.deb: gzip compressed data, was "sofastats-1.3.1.tar", last modified: Thu Jan 10 00:30:44 2013, max compression

--------------
===Answer 11707430===
wget -r -l 1 -A jpg,jpeg,png,gif,bmp -nd -H http://reddit.com/some/path

--------------
===Answer 11716320===
===Answer 16544386===
open (CMDOUT,"wget some_thing 2>&1 |");
while (my $line = <CMDOUT>)
{
    ### do something with each line of hte command output/eror;
}

--------------
#!/usr/bin/perl -w
use strict;
open (CMDOUT,"wget ftp://ftp.redhat.com/pub/redhat/jpp/6.0.0/en/source/MD5SUM 2>&1 |");
while (my $line = <CMDOUT>)
{
    ;
}

--------------
===Answer 27816321===
wget --content-disposition http://sourceforge.net/projects/sofastatistics/files/latest/download?source=dlp

--------------
       --content-disposition
           If this is set to on, experimental (not fully-functional) support for "Content-Disposition" headers is enabled. This can currently result in extra round-trips to the server
           for a "HEAD" request, and is known to suffer from a few bugs, which is why it is not currently enabled by default.

           This option is useful for some file-downloading CGI programs that use "Content-Disposition" headers to describe what the name of a downloaded file should be.

--------------
===Answer 29306797===
setlocal enabledelayedexpansion
if %errorlevel% gtr 0 (
     ....
     wget url
     if !errorlevel! gtr 0 (
     )
    ....
)

--------------
if errorlevel 1 (
    ....
)

--------------
wget url || echo failed
wget url && echo it works
wget url && (echo it works) || (echo failed)

--------------
===Answer 11708015===
use WWW::Mechanize;

$mech = WWW::Mechanize->new();
$mech->get("URL");
$mech->dump_links(undef, 'absolute' => 1);

--------------
===Answer 16536451===
open my $input, "-|", "wget -O - $wgetVal 2>/dev/null";
while (<$input>) { 
    print "Line $_";
}
close $input;

--------------
open my $input, "-|", "wget -O - $wgetVal 2>&1";
while (<$input>) { 
    print "Good\n" and last if /Connecting to.*connected/;
}
close $input;

--------------
===Answer 26785301===
===Answer 2627453===
===Answer 13337278===
@url= 'http://example.com/firstpage.html'
@file = @url.scan(/[^\/]*$/i)

puts @file  #firstpage.html

--------------
===Answer 14306439===
===Answer 16860611===
===Answer 16861163===
#!/bin/ksh
#use your wget command in place of echo below
echo "connected" 2>&1 | grep connected >/dev/null
retcode=$?
if [ $retcode = 0 ]
then
   echo "Running"
else
  echo "Not Running"
fi

--------------
===Answer 28386931===
===Answer 2049408===
===Answer 7353654===
===Answer 7353663===
scp myuser@mycomp:/home/myuser/test.file test.newext

--------------
===Answer 22912858===
wget -m --user "user@domain" --password "password" ftp://ip.of.old.host

--------------
===Answer 4032450===
===Answer 4032452===
===Answer 23607657===
file=$(LANG=C wget URL 2>&1 | sed -n "s/.*- \`\(.*\)' saved.*/\1/p")
echo "$file:"
cat "$file"

--------------
file=$(LANG=C wget google.de 2>&1 | sed -n "s/.*- \`\(.*\)' saved.*/\1/p")
echo "$file:"
cat "$file"

--------------
index.html:
... content

--------------
===Answer 18639728===
===Answer 21119445===
===Answer 23608068===
mkdir foo
cd foo
wget http://example.org
for file in *
do
    path="$file"
done

--------------
===Answer 23610953===
# wget -P /root/test example.com
--2014-05-12 09:52:58--  http://example.com/
Resolving example.com (example.com)... 93.184.216.119, 2606:2800:220:6d:26bf:1447:1097:aa7
Connecting to example.com (example.com)|93.184.216.119|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1270 (1.2K) [text/html]
Saving to: `/root/test/index.html'

100%[===========================================================================================================>] 1,270       --.-K/s   in 0s      

2014-05-12 09:52:58 (95.5 MB/s) - `/root/test/index.html' saved [1270/1270]

--------------
===Answer 9229065===
===Answer 22114698===
 aria2c -x 20 [url] #where 20 the number of connections`

--------------
===Answer 24183065===
===Answer 4100466===
===Answer 13467750===
wget -kp www.example.com/1 www.example.com/2

--------------
===Answer 22236265===
dos2unix myscript.sh
./myscript.sh

--------------
===Answer 4100517===
system('wget $url > $targetfile');

--------------
===Answer 9258540===
SetEnvIfNoCase User-Agent "^wget" bad_bot
<Limit GET POST>
   Order Allow,Deny
   Allow from all
   Deny from env=bad_bot
</Limit>

--------------
===Answer 24183082===
D:\GIT\fence_ip9258>wget http://admin:12345678@192.168.1.41/Set.cmd?CMD=SetPower
+P63=1
SYSTEM_WGETRC = c:/progra~1/wget/etc/wgetrc
syswgetrc = C:\Program Files\GnuWin32/etc/wgetrc
--2014-06-12 19:14:47--  http://admin:*password*@192.168.1.41/Set.cmd?CMD=SetPow
er+P63=1
Connecting to 192.168.1.41:80... connected.
HTTP request sent, awaiting response... 401 Unauthorized
Connecting to 192.168.1.41:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: unspecified
Saving to: `Set.cmd@CMD=SetPower+P63=1'

    [ <=>                                   ] 58           311B/s   in 0.2s

2014-06-12 19:14:48 (311 B/s) - `Set.cmd@CMD=SetPower+P63=1' saved [58]

--------------
===Answer 27844663===
outfile=$DIR/foo
curl -z "$outfile" -o "$outfile" --remote-time "$URL"

--------------
===Answer 28530786===
===Answer 1427719===
#!/bin/sh

if [ $# -lt 3 ]; then
        echo "Usage: $0 url_format seq_start seq_end [wget_args]"
        exit
fi

url_format=$1
seq_start=$2
seq_end=$3
shift 3

printf "$url_format\\n" `seq $seq_start $seq_end` | wget -i- "$@"

--------------

$ ./seq_wget http://someaddress.com/logs/dbsclog01s%03d.log 1 50

--------------

$ wget http://someaddress.com/logs/dbsclog01s{001..050}.log

--------------
===Answer 4713676===
curl -O 'http://www.objektvision.se/annonsorer?ai=26033&iip=100'
wget 'http://www.objektvision.se/annonsorer?ai=26033&iip=100'

--------------
===Answer 1427060===
URL  
       The URL syntax is protocol dependent. You’ll find a  detailed  descrip‐
       tion in RFC 3986.

       You  can  specify  multiple  URLs or parts of URLs by writing part sets
       within braces as in:

        http://site.{one,two,three}.com

       or you can get sequences of alphanumeric series by using [] as in:

        ftp://ftp.numericals.com/file[1-100].txt
        ftp://ftp.numericals.com/file[001-100].txt    (with leading zeros)
        ftp://ftp.letters.com/file[a-z].txt

       No nesting of the sequences is supported at the moment, but you can use
       several ones next to each other:

        http://any.org/archive[1996-1999]/vol[1-4]/part{a,b,c}.html

       You  can  specify  any amount of URLs on the command line. They will be
       fetched in a sequential manner in the specified order.

       Since curl 7.15.1 you can also specify step counter for the ranges,  so
       that you can get every Nth number or letter:

        http://www.numericals.com/file[1-100:10].txt
        http://www.letters.com/file[a-z:2].txt

--------------
===Answer 18516619===
wget --mirror --random-wait -R gif,jpg,pdf <url>

--------------
===Answer 1426553===
for i in {1..999}; do
wget -k http://someaddress.com/logs/dbsclog01s$i.log -O your_local_output_dir_$i;
done

--------------
===Answer 1426556===
$ for i in {1..10}; do echo "http://www.com/myurl`printf "%03d" $i`.html"; done
http://www.com/myurl001.html
http://www.com/myurl002.html
http://www.com/myurl003.html
http://www.com/myurl004.html
http://www.com/myurl005.html
http://www.com/myurl006.html
http://www.com/myurl007.html
http://www.com/myurl008.html
http://www.com/myurl009.html
http://www.com/myurl010.html

--------------
===Answer 12717017===
#!/bin/sh

( cd directory_A && wget ... )
( cd directory_B && wget ... )

--------------
===Answer 20030879===
wget --recursive --level=10 --convert-links -H \
--domains=www.btlregion.ru btlregion.ru

--------------
===Answer 1237448===
--post-data="big long string with \"quotes\" in it"

--------------
===Answer 13270127===
--post-data="big long string with ""quotes"" in it"

--------------
===Answer 4713677===
===Answer 11443257===
lynx -source example.com > index.html

--------------
===Answer 21204877===
===Answer 1427070===
#!/bin/bash
# fixed vars
URL=http://domain.com/logs/     # URL address 'till logfile name
PREF=logprefix                  # logfile prefix (before number)
POSTF=.log                      # logfile suffix (after number)
DIGITS=3                        # how many digits logfile's number have
DLDIR=~/Downloads               # download directory
TOUT=5                          # timeout for quit
# code
for((i=1;i<10**$DIGITS;++i))
do
        file=$PREF`printf "%0${DIGITS}d" $i`$POSTF   # local file name
        dl=$URL$file                                 # full URL to download    
        echo "$dl -> $DLDIR/$file"                   # monitoring, can be commented
        wget -T $TOUT -q $dl -O $file
        if [ "$?" -ne 0 ]                            # test if we finished
        then
                exit
        fi
done

--------------
===Answer 4713688===
curl -O "http://www.objektvision.se/annonsorer?ai=26033&iip=100"
wget "http://www.objektvision.se/annonsorer?ai=26033&iip=100"

--------------
===Answer 4713696===
===Answer 26036987===
===Answer 1426542===
#!/usr/bin/perl
$program="wget"; #change this to proz if you have it ;-)
my $count=1; #the lesson number starts from 1
my $base_url= "http://www.und.nodak.edu/org/crypto/crypto/lanaki.crypt.class/lessons/lesson";
my $format=".zip"; #the format of the file to download
my $max=24; #the total number of files to download
my $url;

for($count=1;$count<=$max;$count++) {
    if($count<10) {
    $url=$base_url."0".$count.$format; #insert a '0' and form the URL
    }
    else {
    $url=$base_url.$count.$format; #no need to insert a zero
    }
    system("$program $url");
}

--------------
===Answer 1426568===
===Answer 1429375===
for i in $(seq -f "%03g" 1 10); do wget "http://.../dbsclog${i}.log"; done

--------------
for i in $(jot -w "http://.../dbsclog%03d.log" 10); do wget $i; done

--------------
===Answer 4604353===
for a in `seq 1 999`; do
if [ ${#a} -eq 1 ]; then
    b="00"
elif [ ${#a} -eq 2 ]; then
    b="0"
fi
echo "$a of 231"
wget -q http://site.com/path/fileprefix$b$a.jpg

--------------
===Answer 12717024===
a && b

--------------
 cd directory_A; wget --spider -i addresses.txt; cd ..;
 cd directory_B; wget --spider -i addresses.txt; cd ..;

--------------
===Answer 12717035===
#!/bin/bash

cd directory_A && wget --spider -i addresses.txt
cd ..
cd directory_B && wget --spider -i addresses.txt
cd ..

--------------
===Answer 20706809===
===Answer 7261261===
wget --no-remove-listing ftp://myftpserver/ftpdirectory/

--------------
===Answer 3836504===
IF EXIST C:\Windows\wget.exe ( *** do something with it ***)

--------------
@echo off
SETLOCAL
(set WF=)
(set TARGET=wget.exe)

:: Look for file in the current directory

  for %%a in ("" %PATHEXT:;= %) do (
     if not defined WF if exist "%TARGET%%%~a" set WF=%CD%\%TARGET%%%~a)

:: Look for file in the PATH

  for %%a in ("" %PATHEXT:;= %) do (
     if not defined WF for %%g in ("%TARGET%%%~a") do (
        if exist "%%~$PATH:g" set WF=%%~$PATH:g))

:: Results
  if defined WF (
    *** do something with it here ***
  ) else (
    echo The file: "%~1" was not found
  ) 

--------------
wget -q <TARGET_URL>
IF NOT ERRORLEVEL 0 (
  curl <TARGET_URL>
  IF NOT ERRORLEVEL 0 (
    ECHO Download failed!
    EXIT 1
  )
)
:: now continue on with your script...

--------------
===Answer 16455475===
===Answer 3677421===
===Answer 7497500===
===Answer 9153338===
<?php

ob_start();
passthru('wget http://graph.facebook.com/663660516/picture 2>&1');
$out = ob_get_contents();
ob_end_clean();

if (preg_match('/saving to:.{4}([a-z0-9\.-_]*)/i', $out, $match)) {
    var_dump($match);
} else {
    echo "No match";
}

--------------
array(2) {
  [0]=>
  string(47) "Saving to: “275781_663660516_1323021723_q.jpg"
  [1]=>
  string(33) "275781_663660516_1323021723_q.jpg"
}

--------------
===Answer 18953756===
wget --accept-regex '/[^.]+(?:\.(?:html?|php|aspx?))?$'

--------------
===Answer 3677412===
===Answer 16455524===
===Answer 22841392===
import subprocess
subprocess.call(r'wget -r --accept "*.exe,*.dll,*.zip,*.msi,*.rar,*.iso" ftp://ftp.apple.asimov.com/ -P e:\e', shell=True)

--------------
 subprocess.call(['wget', '-r', ...])

--------------
===Answer 3677391===
===Answer 22125780===
FILE*fil = fopen("index.hmtl","rt");

--------------
===Answer 22125725===
        printf("%s\n",tok[0]);

--------------
        printf("%s\n", tok);

--------------
        printf("%c\n", tok[0]);

--------------
===Answer 886153===
#!/bin/sh
(
  wget "http://domain/file.zip" && mysql -u user -ppassword database -e "UPDATE..."
) &

--------------
===Answer 15037008===
wget http://commons.wikimedia.org/wiki/File:A_golden_tree_during_the_golden_season.JPG -O output.html; wget $(cat output.html | grep fullMedia | sed 's/\(.*href="\/\/\)\([^ ]*\)\(" class.*\)/\2/g')

--------------
===Answer 886138===
===Answer 886140===
#!/bin/bash
wget -q http://domain.tld/file.zip || exit 0
/usr/bin/php somefile.php

--------------
===Answer 16657153===
===Answer 17454242===
cat urls.txt | xargs -P4 -n1 wget -q &

--------------
do shell script "cat urls.txt | xargs -P4 -n1 /usr/local/bin/wget -q & disown $!"

--------------
===Answer 22125812===
===Answer 23998764===
wget http://commons.wikimedia.org/wiki/Special:Redirect/file/$( echo 'http://commons.wikimedia.org/wiki/File:A_golden_tree_during_the_golden_season.JPG' | sed 's/.*\/File\:\(.*\)/\1/g' )

--------------
===Answer 886066===
===Answer 886099===
$data = file_get_contents('http://domain/file.zip');
file_put_contents('file.zip', $data);
mysql_query("UPDATE table SET status = 'live' WHERE id = '1234'");

--------------
===Answer 886131===
#!/usr/bin/python
import sys, MySQLdb, urllib

urllib.urlretrieve(sys.argv[1], sys.argv[2])
db = MySQLdb.connect(host="localhost", user="username", passwd="password", db="database")
cursor = db.cursor()
cursor.execute("UPDATE table SET status = 'live' WHERE id = '1234'")

--------------
===Answer 11580714===
tilenames = ['File1', 'File2', ...]
web_url = http://...

for t in tilenames:
    try:
        open(t, 'r')
    except IOError:
        print 'file %s not found.' % (t)
        command = ['wget', '-P', './SRTM/', web_url + t ]
        p = Popen(command, stdout=subprocess.PIPE)
        stdout, stderr = p.communicate()

print "Done"

--------------
===Answer 11580725===
===Answer 11580844===
import os
import subprocess

p = subprocess.Popen(['wget','http://www.aol.com'],stdout=subprocess.PIPE)
os.waitpid(p.pid,0)
print "done"

--------------
===Answer 14531890===
===Answer 15036315===
===Answer 16657361===
===Answer 17465467===
cat urls.txt| (xargs -P4 -n1 wget -q -E >/dev/null 2>&1) &

--------------
===Answer 23051488===
i=6000000000
max=7999999999
while [ $i -lt $max ]
do
    wget example.com/imageId=$i.jpg
    true $(( i++ ))
done

--------------
===Answer 4356625===
$ wget -S http://121.199.111.177/
--2010-12-05 02:18:32--  http://121.199.111.177/
Connecting to 121.199.111.177:80... connected.
HTTP request sent, awaiting response... 
  HTTP/1.1 500 Internal Server Error
  Cache-Control: private
  Content-Type: text/html; charset=utf-8
  Server: Microsoft-IIS/7.5
  X-Powered-By: ASP.NET
  X-UA-Compatible: IE=EmulateIE7
  Date: Sun, 05 Dec 2010 00:19:02 GMT
  Connection: keep-alive
  Content-Length: 4722
2010-12-05 02:18:49 ERROR 500: Internal Server Error.

--------------
$ telnet 121.199.111.177 80
Trying 121.199.111.177...
Connected to 121.199.111.177.
Escape character is '^]'.
GET / HTTP/1.0

HTTP/1.1 500 Internal Server Error
Cache-Control: private
Content-Type: text/html; charset=utf-8
Server: Microsoft-IIS/7.5
X-Powered-By: ASP.NET
X-UA-Compatible: IE=EmulateIE7
Date: Sun, 05 Dec 2010 00:36:02 GMT
Connection: close
Content-Length: 4722

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"> 
<html xmlns="http://www.w3.org/1999/xhtml"> 
<head> 
<title>IIS 7.5 详细错误 - 500.19 - Internal Server Error</title> 
<style type="text/css"> 
<!-- 
body{margin:0;font-size:.Connection closed by foreign host.

--------------
===Answer 21382873===
wget --reject-regex 'expr1|expr2|…' http://example.com

--------------
===Answer 11231854===
$ wget -R 'newsbrief-*' ...

--------------
===Answer 5179372===
http://gnuwin32.sourceforge.net/packages/wget.htm

--------------
===Answer 14110993===
===Answer 6356262===
wget -S http://www.gnu.ai.mit.edu/ 

--------------
wget -N http://www.gnu.ai.mit.edu/

--------------
===Answer 25195134===
$?      Expands to the status of the most recently executed foreground pipeline.

--------------
===Answer 4786446===
===Answer 13087959===
echo mb_detect_encoding($str); //$str is what you get after using file_get_contents

--------------
===Answer 13088003===
Accept  */*
Accept-Encoding gzip, deflate

--------------
===Answer 13733119===
===Answer 8260866===
wget -O /dev/null -q http://www.xxx.com/program/timecheck

--------------
===Answer 19360757===
===Answer 7732900===
wget --no-check-certificate https://money.benck.tw

--------------
===Answer 8260889===
wget -O /tmp/log.file ${url} // this will always replace the /tmp/log.file 

--------------
===Answer 13538506===
===Answer 4663513===
===Answer 4985275===
===Answer 6215815===
===Answer 6216836===
#!/bin/sh
/usr/local/bin/wget -r -l3 --no-parent -nc -A ".shtml" 'http://some.url/somethingelse/'

--------------
#!/bin/sh
PATH=/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin
wget -r -l3 --no-parent -nc -A ".shtml" 'http://some.url/somethingelse/'

--------------
PATH=/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin
00 16 * * * /Users/myusername/script.sh
04 16 * * * /Users/myusername/crontest.sh

--------------
===Answer 6217669===
===Answer 13539440===
===Answer 16985874===
===Answer 22048770===
===Answer 8986225===
===Answer 10875501===
wget -e robots=off -r -nd -np -A mp4 http://ia600409.us.archive.org/27/items/MIT18.01JF07

--------------
===Answer 21331630===
wget --reject *.zip ...

--------------
===Answer 10704001===
===Answer 12276274===
wget -r -l 1 https://github.com/justintime/nagios-plugins/zipball/master

--------------
===Answer 19529778===
wget -qO - http://foo/folder/index.htm | sed 's/href=/#/' | cut -d\# -f2 | \
  while read url; do wget $url; done

--------------
===Answer 21331715===
wget -m -k -K -E -p --convert-links -e robots=off -R zip http://www.example.com/ 

--------------
‘-R rejlist --reject rejlist’
    Specify comma-separated lists of file name suffixes or patterns to accept or reject (see Types of Files). Note that if any of the wildcard characters, ‘*’, ‘?’, ‘[’ or ‘]’, appear in an element of acclist or rejlist, it will be treated as a pattern, rather than a suffix. 

--------------
===Answer 25379181===
===Answer 19662668===
===Answer 5936989===
===Answer 7445002===
===Answer 7909936===
===Answer 27551676===
wget --no-parent -r -l 1 -A *.cpp http://url/loc/

--------------
===Answer 5937039===
===Answer 7444963===
===Answer 7445026===
wget -erobots=off \
    'http://maps.googleapis.com/maps/api/geocode/xml?address=Coimbatore+&sensor=true'

--------------
===Answer 15298398===
===Answer 27379974===
wget -O report.pdf http://www.example.com/files/awkward-name.pdf

--------------
mkdir /tmp/download/
cd /tmp/download/
wget http://www.example.com/files/awkward-name.pdf
cd -
cp /tmp/download/awkward-name.pdf report.pdf
rm -rf /tmp/download/

--------------
===Answer 27380016===
wget google.com -O \usr\user\myfolder\foo.html

--------------
===Answer 27937618===
===Answer 4467945===
===Answer 4458321===
===Answer 9380874===
--read-timeout=1800

--------------
===Answer 23636955===
   --adjust-extension
       If a file of type application/xhtml+xml or text/html is downloaded and the URL does not end with the regexp \.[Hh][Tt][Mm][Ll]?, this option will cause the suffix .html to be appended to the
       local filename.  This is useful, for instance, when you're mirroring a remote site that uses .asp pages, but you want the mirrored pages to be viewable on your stock Apache server.  Another good
       use for this is when you're downloading CGI-generated materials.  A URL like http://example.com/article.cgi?25 will be saved as article.cgi?25.html.

--------------
===Answer 28565826===
wget http://www.voiceamerica.com/rss/show/2063 -O - 2> /dev/null | egrep "http://cdn.voiceamerica.com/7thwave/011136/waldrop\w+\.mp3" -o | sort | uniq | awk '{system("wget "$1)}'

--------------
===Answer 28625652===
wget --no-check-certificate -O /dev/null http://foo

--------------
===Answer 29324569===
wget --secure-protocol=SSLv3 --no-proxy --passive-ftp --ftp-user=username --ftp-password=password ftp://host:port/folder/file.pdf

--------------
===Answer 8756067===
wget -A pdf,jpg -m -p -E -k -K -np http://site/path/

--------------
wget --accept pdf,jpg --mirror --progress --adjust-extension --convert-links --backup-converted --no-parent http://site/path/

--------------
===Answer 8755321===
===Answer 20063874===
===Answer 13355928===
===Answer 20494856===
wget --user-agent=Mozilla --content-disposition --mirror --convert-links -E -K -p http://example.com/

--------------
-A pdf,ps,djvu,tex,doc,docx,xls,xlsx,gz,ppt,mp4,avi,zip,rar

--------------
-R html,htm,asp,php

--------------
-X "search*,forum*"

--------------
===Answer 3001669===
my ($wget_pid, $wget_in, $wget_out, $wget_err);
use Symbol qw(gensym);
$wget_err = gensym();
if (!$wget_pid = open3( ... ) ) { ... 

--------------
===Answer 20678448===
===Answer 23775416===
wget -A zip -r -l 1 -nd http://omeka.org/add-ons/themes/

--------------
wget -R html,htm,php,asp,jsp,js,py,css -r -l 1 -nd http://yoursite.com

--------------
===Answer 14926695===
===Answer 6796643===
===Answer 11281108===
===Answer 11281111===
===Answer 11281189===
#!/bin/sh
while read url; do
   urldir=${url%/*}
   dir=${urldir##*/}
   wget -O $dir.jpg $url
done < download.txt

--------------
===Answer 25986123===
wget --mirror -p --convert-links -P ./LOCAL-DIR WEBSITE-URL

--------------
===Answer 27179427===
wget -r --accept-regex '['"'"'"][^"'"'"']*/follow_user['"'"'"]' http://a-site.com

--------------
===Answer 11281275===
sed '\|/\([^/]*\)/[^/]*\1[^/.]*.jpg|!d' download.txt | wget -i -

--------------
===Answer 13355878===
alex@work:~/dev$ ls
arm-2011.03-42-arm-none-eabi.bin  linux
backup_from_ak                    linux-3.4.6.tar.bz2
bc-1.06                           linux.tar.gz                 
CodeSourcery                      mach-lpc32xx                               
alex@work:~/dev$ ls > /dev/null 2>&1
alex@work:~/dev$

--------------
===Answer 15688185===
===Answer 21358719===
===Answer 26989140===
===Answer 27578729===
===Answer 844840===
===Answer 845010===
set_time_limit(0);

--------------
set_time_limit(24*60*60);

--------------
===Answer 6535308===
===Answer 13639130===
===Answer 13639533===
<img class="buildUserPic" src="http://www.tomshardware.com/&lt;%=&#32 content[i].buildUserPic&#32;%&gt;" />

--------------
===Answer 129871===
===Answer 7615825===
set_time_limit(0);

--------------
wget -q0 -T0 http://yourhost/job.php

--------------
===Answer 14178570===
===Answer 21328399===
wget "http://www.example.com"  -c --header="Range: bytes=0-99"

--------------
HTTP request sent, awaiting response... 206 Partial Content
Giving up.

--------------
HTTP request sent, awaiting response... 
  HTTP/1.0 206 Partial Content
  Accept-Ranges: bytes
  Cache-Control: max-age=604800
  Content-Range: bytes 0-99/1270
  Content-Type: text/html
  Date: Fri, 24 Jan 2014 08:35:18 GMT
  Etag: "359670651"
  Expires: Fri, 31 Jan 2014 08:35:18 GMT
  Last-Modified: Fri, 09 Aug 2013 23:54:35 GMT
  Server: ECS (ftw/FBA9)
  X-Cache: HIT
  Content-Length: 100
  Connection: keep-alive
Giving up.

--------------
===Answer 27214566===
curl -u <token>:x-oauth-basic -L https://github.com/user-or-org/repo/archive/sha1-or-ref.tar.gz | tar xz

--------------
===Answer 27658688===
wget --no-check-certificate "http://owncloud.example.com/public.php?service=files&t=par7fec5377a27f19654cd0e7623d883&download&path=//file.tar.gz"

--------------
===Answer 27006899===
===Answer 1644827===
===Answer 6531885===
/usr/bin/wget -q --post-data -O /dev/null 'pass=mypassword' http://www.mywebsite.com/myscript.php > /dev/null 2>&1

--------------
===Answer 2115155===
===Answer 6531879===
wget -O /dev/null ....

--------------
===Answer 3638368===
  URL url = new URL("http://structureddata.wikispaces.com/Test");

  URLConnection urlConnection = url.openConnection();
  Map<String, List<String>> headers = urlConnection.getHeaderFields();
  Set<Map.Entry<String, List<String>>> entrySet = headers.entrySet();
  for (Map.Entry<String, List<String>> entry : entrySet) {
    String headerName = entry.getKey();
    System.out.println("Header Name:" + headerName);
    List<String> headerValues = entry.getValue();
    for (String value : headerValues) {
      System.out.print("Header value:" + value);
    }
    System.out.println();
    System.out.println();
  }

--------------
===Answer 3638425===
final HttpClient client = new HttpClient();
final GetMethod method = new GetMethod("http://structureddata.wikispaces.com/Test");
try {
    if (HttpStatus.SC_OK == client.executeMethod(method)) {
        System.out.println(IOUtils.toString(method.getResponseBodyAsStream()));
    } else {
        throw new IOException("Unable to load page, error " + method.getStatusLine());
    }
} finally {
    method.releaseConnection();
}

--------------
===Answer 3638427===
public static void main(String... args) throws Exception {
    // First request.
    URLConnection connection = new URL("http://structureddata.wikispaces.com/Test").openConnection();

    // Go to the redirected https page to obtain authentication token.
    connection = new URL(connection.getHeaderField("location")).openConnection();

    // Re-request the http page with the authentication token.
    connection = new URL(connection.getHeaderField("location")).openConnection();

    // Show page.
    BufferedReader reader = null;
    try {
        reader = new BufferedReader(new InputStreamReader(connection.getInputStream(), "UTF-8"));
        for (String line; ((line = reader.readLine()) != null);) {
            System.out.println(line);
        }
    } finally {
        if (reader != null) try { reader.close(); } catch (IOException ignore) {}
    }
}

--------------
===Answer 11834609===
wget -O /dev/null -q http://mydomain.com/myscript?pa=doscript >/dev/null 2>&1

--------------
===Answer 11834581===
php -f /path/to/myscript pa=doscript

--------------
/usr/bin/php -f /path/to/myscript pa=doscript

--------------
parse_str(implode('&', array_slice($argv, 1)), $_GET);

--------------
===Answer 18615635===
wget  --quiet    http://example.com

--------------
===Answer 9356201===
===Answer 15824827===
===Answer 21402126===
===Answer 27474130===
===Answer 19969310===
#!/bin/sh

# Linux
wget=/usr/bin/wget
tar=/bin/tar
apachectl=/usr/sbin/apache2ctl

# FreeBSD
#wget=/usr/local/bin/wget
#tar=/usr/bin/tar
#apachectl=/usr/local/sbin/apachectl

TXT="GOT TO THE END, YEAH"
WORKING_DIR="/var/asl/updates"
TARGET_DIR="/usr/local/apache/conf/modsec_rules/"
EXISTING_FILES_DIR="/var/asl/updates/modsec/"
EXISTING_ARCH="/var/asl/updates/"

URL_BASE="http://updates.atomicorp.com/channels/rules/subscription"
WGET_OPTS='--user="jim" --password="xxx-yyy-zzz"'

if [ ! -x "$wget" ]; then
  echo "ERROR: No wget." >&2
  exit 1
elif [ ! -x "$apachectl" ]; then
  echo "ERROR: No apachectl." >&2
  exit 1
elif [ ! -x "$tar" ]; then
  echo "ERROR: Not in Kansas anymore, Toto." >&2
  exit 1
fi

# change to working directory and cleanup any downloaded files
# and extracted rules in modsec/ directory
if ! cd "$WORKING_DIR"; then
  echo "ERROR: can't access working directory ($WORKING_DIR)" >&2
  exit 1
fi

# Delete each file in a loop.
for file in "$EXISTING_FILES_DIR"/* "$EXISTING_ARCH_DIR"/modsec-*; do
  rm -f "$file"
done

# Move old VERSION out of the way.
mv VERSION VERSION-$$

# wget1 to download VERSION file (replaces WGET1)
if ! $wget $WGET_OPTS $URL_BASE}/VERSION; then
  echo "ERROR: can't get VERSION" >&2
  mv VERSION-$$ VERSION
  exit 1
fi

# get current MODSEC_VERSION from VERSION file and save as variable,
# but DON'T blindly trust and run scripts from an external source.
if grep -q '^MODSEC_VERSION=' VERSION; then
  TARGET_DATE="`sed -ne '/^MODSEC_VERSION=/{s/^[^=]*=//p;q;}' VERSION`"
  echo "Target date: $TARGET_DATE"
fi

# Download current archive (replaces WGET2)
if ! $wget ${WGET_OPTS} "${URL_BASE}/modsec-$TARGET_DATE.tar.gz"; then
  echo "ERROR: can't get archive" >&2
  mv VERSION-$$ VERSION         # Do this, don't do this, I don't know your needs.
  exit 1
fi

# extract archive
if [ ! -f "$WORKING_DIR/modsec-${TARGET_DATE}.tar.gz" ]; then
  echo "ERROR: I'm confused, where's my archive?" >&2
  mv VERSION-$$ VERSION         # Do this, don't do this, I don't know your needs.
  exit 1
fi
tar zxvf "$WORKING_DIR/modsec-${TARGET_DATE}.tar.gz"

for file in "$EXISTING_FILES_DIR"/*; do
  cp "$file" "$TARGET_DIR/"
done

# So far so good, so let's restart apache.
if $apachectl configtest; then
  if $apachectl restart; then
    # Success!
    rm -f VERSION-$$
    echo "$TXT"
  else
    echo "ERROR: PANIC! Apache didn't restart.  Notify the authorities!" >&2
    exit 3
  fi
else
  echo "ERROR: Apache configs are broken.  We're still running, but you'd better fix this ASAP." >&2
  exit 2
fi

--------------
===Answer 4530290===
wget "http://example.com/path_to_file.tar.gz" && php scriptname.php file.tar.gz

--------------
===Answer 5904106===
===Answer 9820919===
wget -O - "http://mirror1.malwaredomains.com/files/justdomains" | sed 's/^/192.168.0.254 /' >/var/hosts.md

--------------
===Answer 21719486===
wget --delete -q $url

--------------
system("wget --delete -q $url >/dev/null 2>/dev/null");

--------------
system("wget --delete -q $url");
if ($?) {
    print "false\n";
} else {
    print "OK\n";
}

--------------
system('wget --delete -q ' . quotemeta($url) . ' >/dev/null 2>/dev/null');

--------------
system('wget', '--delete', '-q ', $url);

--------------
===Answer 5363381===
===Answer 9820875===
> echo line | sed -e 's/$/ foobar/'
line foobar
> echo line | sed -e 's/^/foobar /'
foobar line

--------------
===Answer 17074456===
system('/usr/local/bin/wget -O wget-files/fda-test.txt '.$file_name.$url' 2>&1');

--------------
        // Wget the pages
        system('/usr/local/bin/wget -O wget-files/'.$file_name.' "http://www.accessdata.fda.gov/scripts/cder/drugsatfda/index.cfm?'.$source.'" 2>&1');

--------------
===Answer 19965385===
WGET1=' --user="jim" --password="xxx-yyy-zzz" "http://updates.atomicorp.com/channels/rules/subscription/VERSION"'

--------------
`$WGET`

--------------
#!/bin/sh

TXT="GOT TO THE END, YEAH"
WORKING_DIR="/var/asl/updates"
TARGET_DIR="/usr/local/apache/conf/modsec_rules/"
EXISTING_FILES="/var/asl/updates/modsec/*"
EXISTING_ARCH="/var/asl/updates/modsec-*"
WGET1='wget --user="jim" --password="xxx-yyy-zzz" "http://updates.atomicorp.com/channels/rules/subscription/VERSION"'
WGET2='wget --user="jim" --password="xxx-yyy-zzz" "http://updates.atomicorp.com/channels/rules/subscription/modsec-$TARGET_DATE.tar.gz"'


## change to working directory and cleanup any downloaded files and extracted rules in modsec/ directory
cd $WORKING_DIR
rm -f $EXISTING_ARCH
rm -f $EXISTING_FILES

## wget1 to download VERSION file
`$WGET1`

## get current MODSEC_VERSION from VERSION file and save as variable
source VERSION
TARGET_DATE=`echo $MODSEC_VERSION`

## WGET2 command to download current archive
`$WGET2`
## extract archive
tar zxvf $WORKING_DIR/modsec-$TARGET_DATE.tar.gz

cp $EXISTING_FILES $TARGET_DIR

## restart server
exec '/usr/local/cpanel/scripts/restartsrv_httpd' $*;

--------------
tar zxvf $WORKING_DIR/modsec-${TARGET_DATE}.tar.gz

--------------
===Answer 26475155===
===Answer 26475511===
wget "http://www.ted.com/talks/quick-list?sort=date&order=desc&page=18"

--------------
===Answer 10567667===
// wget-1.13.4/src/utils.c
int
base64_encode (const void *data, int length, char *dest)

// gnutls-3.0.19/gl/base64.c
void
base64_encode (const char *restrict in, size_t inlen,
               char *restrict out, size_t outlen)

--------------
===Answer 13185502===
===Answer 2150382===
wget --user bob --password 123456 'https://domain.com/ReportServer?%2fFolder+1%2fReportName&rs:Format=CSV&rs:Command=Render'

--------------
===Answer 14902656===
wget --delete-after -q -r -nd -P /home/example.com/public_html/tmp/ http://www.example.com

--------------
===Answer 17211960===
#!/bin/bash

DBNAME=DB_NAME
USER=USER
PASS=PASS
HOST=localhost
STATEMENT="select name from environment;"

COUNTER=$(mysql $DBNAME -h $HOST -u$USER -p$PASS  -e "$STATEMENT")

for entry in $COUNTER; do
  echo wget $entry
done

--------------
 ./mql.sh 
wget name
wget Prod
wget uat

--------------
===Answer 17604465===
$ wget --ignore-length http://autos.cn.yahoo.com/ 

--------------
===Answer 22944332===
wget http://myserver.com/myurl?temp=`cat /mnt/1wire/342342342/temperature` -o /dev/null -O /dev/null
# -o output log
# -O output document

--------------
===Answer 26867504===
<img src="http://www.edpeers.com/wp-content/themes/prophoto5/images/blank.gif"
 data-lazyload-src="http://www.edpeers.com/wp-content/uploads/2013/11/aa_umbria-italy-wedding_075.jpg"
 class="alignnone size-full wp-image-12934 aligncenter" width="666" height="444"
 alt="Umbria wedding photographer" title="Umbria wedding photographer" /

--------------
wget -nd -p -P . -A jpeg,jpg http://www.edpeers.com/2013/weddings/umbria-wedding-photographer/

--------------
-p
--page-requisites
    This option causes Wget to download all the files that are necessary to properly display a given HTML
    page.  This includes such things as inlined images, sounds, and referenced stylesheets.

--------------
===Answer 27902463===
wget --post-data "value=08585858&textarea=\"Content of sms\"" <url>

--------------
===Answer 10468562===
wget -U "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.24 Safari/536.5" http://fonts.googleapis.com/css?family=Droid+Sans

--------------
===Answer 20622835===
===Answer 20628470===
===Answer 28540111===
-- spider 

--------------
wget -E -i - --wait 0

--------------
===Answer 21916659===
===Answer 28432130===
===Answer 28597209===
===Answer 20773813===
===Answer 537116===
wget -i my_urls.lst

--------------
===Answer 535660===
===Answer 7143016===
FILE='filename'
CURRENT_TS=`stat -c %y $FILE`
wget -N $URL
NEW_TS=`stat -c %y $FILE`
if [ "$CURRENT_TS" != "$NEW_TS" ]; then
    # Do something here.
fi

--------------
===Answer 9890742===
===Answer 12507755===
===Answer 535672===
wget "http://www.cepa.org.gh/archives/research-working-papers/WTO4%20(1)-charles.doc"

--------------
wget 'http://www.cepa.org.gh/archives/research-working-papers/WTO4%20(1)-charles.doc'

--------------
===Answer 535678===
===Answer 8374060===
===Answer 19236512===
exit 5

--------------
if ($?) {"No error"} else {"some error"}

--------------
===Answer 19236714===
C:\PS> Start-Process cmd.exe -arg '/c exit 5'
C:\PS> $?
True

--------------
C:\PS> $p = Start-Process cmd.exe -arg '/c exit 5' -PassThru -Wait
C:\PS> $p.ExitCode
5

--------------
C:\PS> cmd /c exit 5
C:\PS> $LASTEXITCODE
5

--------------
===Answer 28742754===
--http-password=password--read-timeout=1808080878708 

--------------
--http-password=password --read-timeout=1808080878708 

--------------
===Answer 25891929===
===Answer 12661781===
curl -F"operation=upload" -F"file=@myfile" http://localhost:9000/index.php

--------------
<?php
$uploadfile = '/tmp/' . basename($_FILES['file']['name']);
move_uploaded_file($_FILES['file']['tmp_name'], $uploadfile);
$content = file_get_contents($uploadfile);
?>

--------------
===Answer 16846396===
wget -E -H -k -K -p http://<site>/<document>

--------------
wget -E -H -k -K -p -np -l 1 http://<site>/level

--------------
===Answer 23599827===
===Answer 935926===
-R '*action=diff*,*action=edit*'

--------------
===Answer 7545981===
http://www.example.com/file1.mpg    filename_to_save_as1.mpg
http://www.example.com/file2.mpg    filename_to_save_as2.mpg
http://www.example.com/file3.mpg    filename_to_save_as3.mpg

--------------
#!/bin/bash
while read line
do
    URL=$(echo "$line" | cut -f 1 )
    FILENAME=$(echo "$line" | cut -f 2 )
    echo wget -c --load-cookies cookies.txt "$URL" -O "$FILENAME"
done

--------------
echo input.txt | wget2.sh

--------------
#!/bin/bash
wget -c --load-cookies cookies.txt http://www.example.com/file.mpg1 -O filename_to_save_as1.mpg
wget -c --load-cookies cookies.txt http://www.example.com/file.mpg2 -O filename_to_save_as2.mpg
wget -c --load-cookies cookies.txt http://www.example.com/file.mpg3 -O filename_to_save_as3.mpg

--------------
===Answer 23597582===
===Answer 8331354===
===Answer 9149679===
===Answer 16846224===
===Answer 21294173===
===Answer 21976387===
===Answer 22716590===
===Answer 24087339===
wget --spider -nd -e robots=off -Hprb --level=1 -o wget-log -nv http://localhost

--------------
===Answer 14490201===
-R robots.txt,unwanted-file.txt

--------------
URL=http://download.openvz.org/template/precreated/
CUTS=`echo ${URL#http://} | awk -F '/' '{print NF -2}'`
wget -r -l1 -nH --cut-dirs=${CUTS} --no-parent -A.tar.gz --no-directories -R robots.txt ${URL}

--------------
===Answer 20986580===
> GET /2012/01/19/118675/ HTTP/1.1
> User-Agent: Mozilla/5.0 (Windows NT 5.2; rv:2.0.1) Gecko/20100101 Firefox/4.0.1
> Host: opinionator.blogs.nytimes.com
> Accept: */*
> 
< HTTP/1.1 303 See Other
< Date: Wed, 08 Jan 2014 03:23:06 GMT
* Server Apache is not blacklisted
< Server: Apache
< Location: http://www.nytimes.com/glogin?URI=http://opinionator.blogs.nytimes.com/2012/01/19/118675/&OQ=_rQ3D0&OP=1b5c69eQ2FCinbCQ5DzLCaaaCvLgqCPhKP
< Content-Length: 0
< Content-Type: text/plain; charset=UTF-8

--------------
---request begin---
GET /2012/01/19/118675/?_r=0 HTTP/1.1
User-Agent: Mozilla/5.0 (Windows NT 5.2; rv:2.0.1) Gecko/20100101 Firefox/4.0.1
Accept: */*
Host: opinionator.blogs.nytimes.com
Connection: Keep-Alive
Cookie: NYT-S=0MhLY3awSMyxXDXrmvxADeHDiNOMaMEZFGdeFz9JchiAIUFL2BEX5FWcV.Ynx4rkFI

--------------
===Answer 20766228===
wget -r -I /pub/special.requests/cew/2013/county/ ftp://ftp.bls.gov/pub/special.requests/cew/

--------------
wget -r -I /pub/special.requests/cew/2013/county/ ftp://ftp.bls.gov/pub/special.requests/cew/2013/

--------------
===Answer 14490016===
wget -r -l1 -nH --cut-dirs=2 --no-parent -A.tar.gz --no-directories http://download.openvz.org/template/precreated/
rm robots.txt

--------------
===Answer 20751922===
 wget -r ftp://ftp.bls.gov/pub/special.requests/cew/2013/county

--------------
 wget -r -I /pub/special.requests/cew/2013/county/ ftp://ftp.bls.gov/pub/special.requests/cew/2013/

--------------
 wget -r -I /pub/special.requests/cew/2013/county/ ftp://ftp.bls.gov/pub/special.requests/cew/

--------------
===Answer 26215321===
wget --secure-protocol=SSLv3 ...

--------------
===Answer 26685507===
newfile () {
    fname=$1
    count=0
    while [ -e "$fname" ]
    do
        fname="$1.$((++count))"
    done
    echo "$fname"
}

--------------
$ ls
$ newfile abc
abc
$ touch abc
$ newfile abc
abc.1

--------------
wget http://example.com/index.html -O "$(newfile something)"

--------------
===Answer 6484578===
===Answer 6484579===
require 'QueryPath/QueryPath.php';

$url = 'http://example.com';
print qp($url, 'title')->text();

--------------
===Answer 16124449===
===Answer 17604655===
$ wget --verbose --page-requisites --convert-links --adjust-extension --force-directories \
  http://football.bettor.com/barclays-premier-league-winner-2011-12-betting-odds/2011-11-19/market/652852

--------------
===Answer 26435209===
===Answer 26685183===
===Answer 3618574===
wget -r -l1 --no-parent -A.gif http://www.server.com/dir/

--------------
wget -r -l1 --no-parent -A.tbz http://www.mysite.com/path/to/files/

--------------
===Answer 8373076===
s.send('wget http://url/file1')
print "file 1 OK"
s.recv(1024) # Wait for ANY response
s.send(....

--------------
while True:
    data = conn.recv(1024)
    cmd = ['/bin/sh', '-c', data]
    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE).wait()
    conn.send('done') # Send something, ANYTHING
# EOF

--------------
===Answer 13983360===
===Answer 19837147===
===Answer 21117355===
===Answer 21680575===
wget -r -np -nd --glob=on ftp://ftp.ncbi.nlm.nih.gov/blast/db/nt.*.tar.gz

--------------
===Answer 25694471===
===Answer 29113634===
===Answer 3618582===
<?php
for($i=0;$i<30;$i++)
{
     $filename = "file".date("Ymd", time() + 86400 * $i).".tbz";
     //try file download, if successful, break out of loop.
?>

--------------
===Answer 17719309===
ALL ALL=/usr/bin/wget,  NOPASSWD: ALL

--------------
sudo wget google.com

--------------
===Answer 29147425===
wget --user=user --password=password

--------------
===Answer 15173070===
curl -A "Mozilla/5.0" 'http://translate.google.com/translate_a/t?client=t&text=hello&hl=en&sl=en&tl=zh-CN&ie=UTF-8&oe=UTF-8&multires=1&prev=btn&ssel=0&tsel=0&sc=1'

--------------
[[["你好","hello","Nǐ hǎo",""]],[["interjection",["喂"],[["喂",["hello","hey"],,0.0087879393]]]],"en",,[["你好",[5],0,0,1000,0,1,0]],[["hello",4,,,""],["hello",5,[["你好",1000,0,0],["招呼",0,0,0],["打招呼",0,0,0],["个招呼",0,0,0],["喂",0,0,0]],[[0,5]],"hello"]],,,[["en"]],6]

--------------
curl -A "Mozilla/5.0" 'http://translate.google.com/translate_a/t?client=t&text=hello&hl=en&sl=en&tl=zh-CN&ie=UTF-8&oe=UTF-8&multires=1&prev=btn&ssel=0&tsel=0&sc=1' | sed 's/\[\[\["\([^"]*\).*/\1/'

--------------
===Answer 4252981===
===Answer 6980768===
===Answer 11879076===
wget http://tinyurl.com/2tx --server-response -O /dev/null 2>&1 |\
   awk '(NR==1){SRC=$3;} /^  Location: /{DEST=$2} END{ print SRC, DEST}'

--------------
===Answer 15172763===
<!-- Headers... -->
<ins>That’s an error.</ins>
<p>Your client does not have permission to get URL <code>/</code> from this server.
(Client IP address: xx.xxx.xx.xx)<br><br>


<ins>That’s all we know.</ins>

--------------
===Answer 26103977===
===Answer 4253022===
===Answer 6483597===
===Answer 8034193===
===Answer 16574488===
wget -O/dev/null http://www.example.com/blab

--------------
===Answer 24072629===
===Answer 629841===
===Answer 630697===
www.example.com/index.html%3Fpage=about

--------------
===Answer 9922817===
===Answer 629856===
===Answer 9922809===
===Answer 25151618===
===Answer 25423720===
curl -F file=@links.txt http://bearnova.com/upload_file.php

--------------
===Answer 16016904===
===Answer 18158991===
===Answer 18793501===
===Answer 21785246===
===Answer 21877705===
===Answer 25425089===
<?php $postdata = file_get_contents("php://input"); ?>

--------------
===Answer 25425799===
wget -d --post-data="filedata=`base64 links.txt`&filename=links.txt" http://bearnova.com/upload_file.php

--------------
<?php
    $filename = $_POST['filename'];
    $filedata = base64_decode($_POST['filedata']);
?>

--------------
===Answer 25423702===
<form method="post" action="upload_file.php" enctype="multipart/form-data">
    <input type="file" name="file" id="file"/>
</form>

--------------
===Answer 4358452===
echo 0 > /proc/sys/net/ipv4/tcp_default_win_scale

--------------
echo 0 > /proc/sys/net/ipv4/tcp_window_scaling

--------------
$ wget http://www.jinfuwu.com
--2010-12-05 12:58:39--  http://www.jinfuwu.com/
Resolving www.jinfuwu.com... 121.199.111.176
Connecting to www.jinfuwu.com|121.199.111.176|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 12145 (12K) [text/html]
Saving to: `index.html'

100%[====================================================>] 12,145      5.19K/s   in 2.3s    

2010-12-05 12:58:43 (5.19 KB/s) - `index.html' saved [12145/12145]

--------------
===Answer 27665944===
import requests
header={"user-agent":\"Mozilla/5.0 (Windows NT 6.0) Gecko/20100101 Firefox/14.0.1\"",
'referer': referer}
cookies = dict(cookie_name='cookie_text')
r = requests.get(url, header=header, cookies=cookies)

--------------
===Answer 4826317===
#/bin/bash

for page in {1..50}
do
  wget -q -O /tmp/$page.xml "http://site.com/xap/wp7?p=$page"
  php -q xml.php $page >> products.txt
done

--------------
<?
$file = '/tmp/'.$argv[1].'.xml';
// assumeing the following format
//<Products><Product title="Free Shipping ProductName"/></Products>

$xml = simplexml_load_file($file);
echo $xml->Product->attributes()->title;
/* you can make any replacement only parse/obtain the correct node attribute */
?>

--------------
===Answer 4826423===
#/bin/bash

for page in {1..50}
do
  wget -q "http://site.com/xap/wp7?p=$page" -O - \
    | tr '"' '\n' | grep "^Free Shipping " | cut -d ' ' -f 3 > products.txt
done

--------------
<html>
...
... <tag title=
Free Shipping [Product]
> ...

--------------
Free Shipping [Product1]
Free Shipping [Product2]
...

--------------
[Product1]
[Product2]
...

--------------
===Answer 24422793===
use LWP::Simple;
# Grab the filename from the end of the URL    
my $filename = (split '/', $url)[-1];
# If the file exists, increment its name
while (-e $filename)
{
    $filename =~ s{ (\d+)[.]jpg }{ $1+1 . '.jpg' }ex 
        or die "Unexpected filename encountered";
}
getstore($url, $filename);

--------------
===Answer 4358405===
===Answer 13752005===
===Answer 14474402===
===Answer 28133958===
-nd
--no-directories
  Do not create a hierarchy of directories when retrieving recursively.  With this 
  option turned on, all files will get saved to the current directory, without 
  clobbering (if a name shows up more than once, **the filenames will get
  extensions .n**).

--------------
===Answer 28134337===
===Answer 636363===
===Answer 636374===
===Answer 17514147===
===Answer 1121092===
echo -ne "GET /path/to/file HTTP/1.0\r\nHost: www.somesite.com\r\n\r\n" | nc www.somesite.com 80 | perl -pe 'BEGIN { while (<>) { last if $_ eq "\r\n"; } }'

--------------
===Answer 9961216===
cat urls.txt | while read url
do
    wget "$url" -O "${url##*/}"  # <-- use custom name here
done

--------------
===Answer 1121490===
read_http() {
    local url host path login port
    url="${1#http://}"
    host="${url%%/*}"
    path="${url#${host}}"
    login="${host%${host#*@}}"
    host="${host#${login}@}"
    port="${host#${host%:*}}"
    host="${host%:${port}}"
    (
        exec 3<>"/dev/tcp/${host}/${port:-80}" || exit $?
        >&3 echo -n "GET ${path:-/} HTTP/1.1"$'\r\n'
        >&3 echo -n "Host: ${host}"$'\r\n'
        [[ -n ${login} ]] &&
        >&3 echo -n "Authorization: Basic $(uuencode <<<"${login}")"$'\r\n'
        >&3 echo -n $'\r\n'
        while read line <&3; do
            line="${line%$'\r'}"
            echo "${line}" >&2
            [[ -z ${line} ]] && break
        done
        dd <&3
    )
}

--------------
===Answer 17514231===
===Answer 636358===
===Answer 1121076===
curl -C - -O http://www.url.com

--------------
===Answer 1121134===
#!/usr/bin/env python
import os,sys,urllib
f = open (os.path.basename (sys.argv[1]), 'w')
f.write (urllib.urlopen (sys.argv[1]).read ())
f.close ()

--------------
===Answer 1125193===
===Answer 1121017===
===Answer 1121048===
scp user@myhost.com:folder/file.flv ./

--------------
===Answer 1121077===
===Answer 10478183===
export http_proxy=http://username:password@proxyserver.net:port/
export https_proxy=http://username:password@proxyserver.net:port/
export ftp_proxy=http://username:password@proxyserver.net:port/

--------------
http_proxy=http://username:password@proxyserver.net:port/
https_proxy=http://username:password@proxyserver.net:port/
ftp_proxy=http://username:password@proxyserver.net:port/

--------------
Acquire::http::Proxy "http://username:password@proxyserver.net:port";
Acquire::https::Proxy "http://username:password@proxyserver.net:port";
Acquire::ftp::Proxy "http://username:password@proxyserver.net:port";

--------------
===Answer 20157227===
seq 1 100 | xargs -n 1 -I {} echo http://example.com/homepage?page={} > URLS.txt

--------------
wget -i URLS.txt

--------------
===Answer 1121130===
===Answer 1125239===
===Answer 13403865===
curl -L 'http://finance.yahoo.com/d/quotes.csv?s=XOM+BBDb.TO+JNJ+MSFT&f=snd1l1yr' > joy.csv

--------------
===Answer 1121215===
===Answer 4933130===
===Answer 4933490===
===Answer 11608767===
===Answer 15357177===
"http://Phone-IP-Address/dummy.htm?settings=save&setting_server=http://WEB-Server-IP/settings.xml&store_settings&=save"

--------------
===Answer 18394019===
===Answer 7679892===
"cmd /c C:/wget.exe -q -O - <my-URL>".execute().text

--------------
===Answer 12846957===
===Answer 7679887===
def data = new URL(myUrl).getText()

--------------
===Answer 26949702===
===Answer 4478197===
cat DownloadedSearchResults.html | egrep (?<=class="searchResultImage".+href=").+?\.jpg/

--------------
cat DownloadedSearchResult.jpg | egrep (?<=class="fullImageLink".*href=").+?\.jpg

--------------
===Answer 6771777===
===Answer 6809361===
===Answer 20265448===
===Answer 23315073===
   -A, --user-agent <agent string>
          (HTTP) Specify the User-Agent string to send to the HTTP server.
          Some  badly  done  CGIs  fail  if  this  field  isn't   set   to
          "Mozilla/4.0".  To  encode  blanks  in  the string, surround the
          string with single quote marks. This can also be  set  with  the
          -H, --header option of course.

          If this option is used several times, the last one will be used.

--------------
===Answer 26318327===
wget -nH --cut-dirs=5 -r --timestamping http://download.macromedia.com/pub/flashplayer/current/support/install_flash_player.exe
wget -nH --cut-dirs=5 -r --timestamping http://download.macromedia.com/pub/flashplayer/current/support/install_flash_player_ax.exe

--------------
===Answer 27351612===
#### accesslog module
accesslog.filename          = "/var/log/lighttpd_access.log"

--------------
===Answer 29193589===
===Answer 4324642===
===Answer 3086285===
===Answer 6615650===
===Answer 17488874===
http://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?val=498907917&db=nuccore&dopt=fasta&extrafeat=0&fmt_mask=0&maxplex=1&sendto=t&withmarkup=on&log$=seqview&maxdownloadsize=1000000

--------------
wget "whatever" -O temp.html
id=`cat temp.html | grep ncbi_uidlist | sed -e 's/^.*ncbi_uidlist\" content=\"//' | sed -e 's/".*//'`
wget "http://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?val=$id&db=nuccore&dopt=fasta&extrafeat=0&fmt_mask=0&maxplex=1&sendto=t&withmarkup=on&log$=seqview&maxdownloadsize=1000000"

--------------
===Answer 28415927===
===Answer 28416244===
url = "http://torrent.ubuntu.com/xubuntu/releases/trusty/release/desktop/xubuntu-14.04.1-desktop-amd64.iso.torrent"
command = Popen(["wget", "--spider", url],stdout=PIPE,stderr=PIPE)
out,err = command.communicate()

print("This is stdout: {}".format(out))
print("This is stderr: {}".format(err))
This is stdout: b''
This is stderr: b'Spider mode enabled. Check if remote file exists.\n--2015-02-09 18:00:28--  http://torrent.ubuntu.com/xubuntu/releases/trusty/release/desktop/xubuntu-14.04.1-desktop-amd64.iso.torrent\nResolving torrent.ubuntu.com (torrent.ubuntu.com)... 91.189.95.21\nConnecting to torrent.ubuntu.com (torrent.ubuntu.com)|91.189.95.21|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 37429 (37K) [application/x-bittorrent]\nRemote file exists.\n\n'

--------------
===Answer 13663694===
===Answer 14511393===
===Answer 17488858===
===Answer 26266818===
<a id="game-one" href="test.com/hos/6/42?game=1"; style="display:block; width:350px">

--------------
===Answer 26267421===
curl --head --referer 'http://www.test.com/hos/6/42' http://www.test.com/hos/6/42?game=1

--------------
===Answer 27012139===
===Answer 28446684===
cat list.txt | xargs wget -o logfile

--------------
www.google.com
www.yahoo.com
www.facebook.com

--------------
===Answer 19876818===
===Answer 17098641===
$ file 6760F0232086AFE6880C974645DE8105FF032706.torrent
6760F0232086AFE6880C974645DE8105FF032706.torrent: gzip compressed data, from Unix

--------------
$ wget -S http://torrage.com/torrent/6760F0232086AFE6880C974645DE8105FF032706.torrent
--2013-06-14 00:53:37--  http://torrage.com/torrent/6760F0232086AFE6880C974645DE8105FF032706.torrent
Resolving torrage.com... 192.121.86.94
Connecting to torrage.com|192.121.86.94|:80... connected.
HTTP request sent, awaiting response...
  HTTP/1.0 200 OK
  Connection: keep-alive
  Content-Encoding: gzip

--------------
===Answer 2588496===
wget -O file http://stackoverflow.com

--------------
===Answer 23418360===
===Answer 13690812===
#!/bin/sh
while read
do
  echo "$REPLY"
  if wget --spider -q "$REPLY"
  then
    echo "200 OK"
  else
    echo "404 Not Found"
    echo "$REPLY" >> p404s.txt
  fi
  echo
done < test-urls.txt

--------------
http://stackoverflow.com
200 OK

http://stackoverflow.com/1
404 Not Found

http://superuser.com
200 OK

--------------
===Answer 21899019===
wget --header="Content-type: multipart/form-data boundary=FILEUPLOAD" --post-file postfile http://domain/uploadform

--------------
===Answer 29283024===
===Answer 29328475===
#-*- coding:utf-8 -*-
import os
import urllib2

with open('path_to_your_hash_url', 'r') as fh:
    url_to_be_download = fh.read().split("\n")

with open('path_to_your_FileNames', 'r') as fh:
    fileNames = fh.read().split('\n')

siteurl = 'http://whatever.com/'  #path to your site


downloadFolder = r'YourDownloadFile folder'


for i, url in enumerate(url_to_be_download):
    location = os.path.join(downloadFolder, url_to_be_download[i])
    with open(newloc,"w") as fh:
        full_url = siteurl+ url
        ufile = urllib2.urlopen(full_url).read()
        fh.write(ufile)

--------------
===Answer 2588395===
wget -o [path/to/your/ouptut/file] [URL]

--------------
===Answer 2590269===
===Answer 10259359===
# ==================================================================
_PS=${WINDIR}\System32\WindowsPowerShell\v1.0\powershell.exe
OAUTHCS_URL=https://cropperplugins.svn.codeplex.com/svn/Cropper.Plugins/OAuth1.0/OAuth.cs

OAuth.cs:
    $(_PS) -Command "& {(new-object System.Net.WebClient).DownloadFile('$(OAUTHCS_URL)','OAuth.cs')}"

--------------
CSC=\Windows\Microsoft.NET\Framework\v4.0.30319\csc.exe

Oauth.cs: wget.exe 
    .\wget.exe $(OAUTHCS_URL)

wget.exe: wget.cs $(CSC)
    $(CSC) /t:exe /debug+ /out:wget.exe  wget.cs

--------------
===Answer 8599778===
--post-data 'email=$EMAIL&password=$PASSWRD'

--------------
--post-data "email=$EMAIL&password=$PASSWRD"

--------------
===Answer 10872187===
wget -e robots=off -H -p -k http://www.myspace.com/

--------------
===Answer 19883111===
mech-dump --links 'http://domain.com' |
    grep pdf$ |
    sed 's/\s+/%20/g' |
    xargs -I% wget http://domain.com/%

--------------
===Answer 14186166===
if wget http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm
then
    echo 'We got it!'
else
    read -p 'URL invalid. Please specify new URL: '
    wget "$REPLY"
fi

--------------
===Answer 24439243===
--2014-06-26 07:33:57-- ... myFolder/myFile.so%0D
                                              ^^^ what's this about?

--------------
"foo" [dos] 1L, 5C    
      ^^^^^

--------------
:set ff=unix[enter] 
:x[enter]

--------------
===Answer 27579884===
    String encodedUrl = "http://xyo.net/iphone-app/instagram-RrkBUFE/";

    Response res = Jsoup.connect(encodedUrl)
            .header("Accept-Language", "en")
            .ignoreHttpErrors(true)
            .ignoreContentType(true)
            .header("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8")
            .followRedirects(true)
            .timeout(10000)
            .method(Connection.Method.GET)
            .execute();


    System.out.println(res.parse());

--------------
===Answer 5957785===
-nd, --no-directories           don't create directories.
-x,  --force-directories        force creation of directories.
-nH, --no-host-directories      don't create host directories.
     --protocol-directories     use protocol name in directories.
-P,  --directory-prefix=PREFIX  save files to PREFIX/...
     --cut-dirs=NUMBER          ignore NUMBER remote directory component

--------------
#!/bin/bash
cd www.website.net
for d in $( find . -type -d -print ) ; do
   if [[ -f $d/index.html ]] ; then
     echo mv $d/index.html $.html && echo rmdir $d
    fi
done

--------------
===Answer 14186294===
wget http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm 
&& echo "WE GOT IT" && exit || echo "URL Invalid, Please specify new URL:" 
&& read newurl && wget $newurl


Add "exit" on left side of ||  with && operator ,,if the left argument is true then it will download that file and exit from terminal otherwise prompt for new url

--------------
===Answer 10842324===
===Answer 6795309===
a = "...St. Paul\xE2%80%99s Cathedral..."
puts a

require 'cgi'
b = CGI.unescape a
puts b

require 'iconv'
c = Iconv.conv('UTF-8//TRANSLIT', 'UTF-8', b) # may not even be necessary
puts c

--------------
...St. Paul?%80%99s Cathedral...
...St. Paul’s Cathedral...
...St. Paul’s Cathedral...

--------------
===Answer 28685309===
start Wget -q -OUpdate.zip https://www.dropbox.com/s/e9q3ssvhitsatbq/Update.zip --no-check-certificate
echo test

--------------
start /b Wget -q -OUpdate.zip https://www.dropbox.com/s/e9q3ssvhitsatbq/Update.zip --no-check-certificate >NUL 2>&1
echo test

--------------
===Answer 3226535===
===Answer 23752646===
wget --accept .jpg,.jpeg --cookies=on -p "https://scontent-b.xx.fbcdn.net/hphotos-prn2/v/t34.0-12/s720x720/10361106_862849933729442_781906896_n.jpg?oh=180f721d0cd3107253c49448dffda02a&oe=537C1807" -O "image.jpg"

--------------
===Answer 27482287===
http://xx.xxx.xxx.xxx/remote/files/file1.jpg
http://xx.xxx.xxx.xxx/remote/files/file2.jpg
http://xx.xxx.xxx.xxx/remote/files/file3.jpg
....

--------------
wget .... --input-file list.txt

--------------
wget .... --recursive --level=1 --accept=jpg --no-parent http://.../your-index-page.html 

--------------
===Answer 27776591===
===Answer 28685368===
START Wget -q -OUpdate.zip https://www.dropbox.com/s/e9q3ssvhitsatbq/Update.zip --no-check-certificate

--------------
===Answer 14451989===
wget -c "http://118.26.57.16/1Q2W3E4R5T6Y7U8I9O0P1Z2X3C4V5B/218.11.178.160/edge.v.iask.com/95687694.hlv?KID=sina,viask&Expires=1358956800&ssig=WHgIi1wQOW&wsiphost=ipdbm"

--------------
===Answer 11600339===
===Answer 13338166===
===Answer 18807093===
import multiprocessing, subprocess, re

def getSiteRecursive(id, url, depth=2):
  cmd =  "wget -P " + id + " -r -l " + str(depth) + " " + url
  subprocess.call(cmd, shell=True)

input_file = "site_list.txt"
jobs = []
max_jobs = multiprocessing.cpu_count() * 2 + 1
with open(input_file) as f:
  for line in f:
    id_url = re.compile("\s+").split(line)
    if len(id_url) >= 2:
      try:
        print "Grabbing " + id_url[1] + " into " + id_url[0] + " recursively..."
        if len(jobs) >= max_jobs:
          jobs[0].join()
          del jobs[0]
        p = multiprocessing.Process(target=getSiteRecursive,args=(id_url[0],id_url[1],2,))
        jobs.append(p)
        p.start()
      except Exception, e:
        print "Error for " + id_url[1] + ": " + str(e)
        pass
  for j in jobs:
    j.join()

--------------
import urllib2, re
input_file = "site_list.txt"
#open the site list file
with open(input_file) as f:
  # loop through lines
  for line in f:
    # split out the id and url
    id_url = re.compile("\s+").split(line)
    print "Grabbing " + id_url[1] + " into " + id_url[0] + ".html..."
    try:
      # try to get the web page
      u = urllib2.urlopen(id_url[1])
      # save the GET response data to the id file (appended with "html")
      localFile = open(id_url[0]+".html", 'wb+')
      localFile.write(u.read())
      localFile.close()
      print "got " + id_url[0] + "!"
    except:
      print "Could not get " + id_url[0] + "!"
      pass

--------------
id_345  http://www.stackoverflow.com
id_367  http://stats.stackexchange.com

--------------
Grabbing http://www.stackoverflow.com into id_345.html...
got id_345!
Grabbing http://stats.stackexchange.com into id_367.html...
got id_367!

--------------
get_urls.py
id_345.html
id_367.html
site_list.txt

--------------
===Answer 22798133===
===Answer 3226535===
===Answer 3483719===
system("wget ", "#{m}")
#           ^ extra space here

--------------
===Answer 9259224===
 -np, --no-parent                 don't ascend to the parent directory.

--------------
===Answer 5299878===
use strict;
use warnings;

use WWW::Mechanize;

my $mech = WWW::Mechanize->new();

...

$mech->credentials($username, $password);
$mech->get($url);
foreach my $link ($mech->find_all_links(url_regex=>qr/\bAAA/)) {
    $mech->get($link);
    ...
}

--------------
===Answer 5412488===
===Answer 9259245===
===Answer 5299976===
system("wget --user=$username --password=$password '$url/$xml'");
system qq(wget --user=$username --password=$password "$url/$xml");

--------------
system( 'wget', "--user=$username", "--password=$password", "$url/$xml");

--------------
===Answer 7573121===
===Answer 17881907===
/usr/local/bin/curl -k -c cookie.txt -d "login_alias=464" -d "password=6446" -d "from=%2F" https://www.supplier.com/auth/login.php
paste theCodesOnePerLine theNamesOnePerLine |
while read i j
do 
/usr/local/bin/curl -k -L -b cookie.txt -o /usr/home/$j.xls 'https://www.supplier.com/?group='$i'&excel=1'
done

--------------
===Answer 21398046===
#!/bin/bash

declare -a _servers=('192.168.1.1' '192.168.1.2' '192.168.1.3' '192.168.1.4')
ORIGIN = "192.168.1.254"

for ((i=0;i<${#_servers[@]};i++));
do
    echo "-------- Server: ${_servers[i]} ---------"
    ssh myUser@${_servers[i]} wget http://${ORIGIN}:8080/someDirectory/refreshSomePage.jsp
    echo "-------- Complete ---------"
done

--------------
===Answer 21431491===
===Answer 27256662===
wget --header "Authorization: token <GITHUB TOKEN>"  --output-document=<RELEASE>.tar.gz https://api.github.com/repos/<USER>/<REPO>/tarball/<RELEASE NAME>

--------------
===Answer 5298264===
system("/usr/bin/wget --user=$username --password=$password $url")

--------------
===Answer 21431539===
===Answer 22187456===
wget -i foo.htm -r -A .tex

--------------
===Answer 22234395===
for i in {1..20000}
do
    lynx -dump -listonly -nonumbers $i.htm >> all-links
done

cat all-links | grep .tex >> texlinks
wget -c -i texlinks

exit;

--------------
===Answer 5417555===
===Answer 9542456===
var request = require("request");

request(url, function(err, res, body) {
  // Do funky stuff with body
});

--------------
===Answer 9543866===
var http = require('http');
var options = {
    host: 'www.site2scrape.com',
    port: 80,
    path: '/page/scrape_me.html'
  };
var req = http.get(options, function(response) {
  // handle the response
  var res_data = '';
  response.on('data', function(chunk) {
    res_data += chunk;
  });
  response.on('end', function() {
    console.log(res_data);
  });
});
req.on('error', function(err) {
  console.log("Request error: " + err.message);
});

--------------
===Answer 9541246===
var util = require('util'),
    exec = require('child_process').exec,
    child,
    url = 'url to file';

child = exec('wget ' + url,
  function (error, stdout, stderr) {
    console.log('stdout: ' + stdout);
    console.log('stderr: ' + stderr);
    if (error !== null) {
      console.log('exec error: ' + error);
    }
});

--------------
===Answer 858241===
===Answer 9236151===
===Answer 9112721===
find -name '*.html' | xargs sed -rie 's/href="([^"]*)\/index\.html"/href="\1\/"/gi'

--------------
===Answer 9541250===
var exec = require('child_process').exec;

child = exec("/path/to/wget http://some.domain/some.file", function (error, stdout, stderr) {
if (error !== null) {
  console.log("ERROR: " + error);
}
else {
  console.log("YEAH IT WORKED");
}
});

--------------
===Answer 28392379===
wget "https://dashboard.vaultpress.com/12345/restore/?step=4&job=12345678&check=<somehashedvalue>"

--------------
===Answer 858365===
===Answer 9271109===
"""
    Downloads all links from a specified location and saves to machine.
    Downloaded links will only be of a lower level then links specified.
    To use: python downloader.py link
"""
import sys,re,os,urllib2,urllib,urlparse
tocrawl = set([sys.argv[1]])
# linkregex = re.compile('<a\s*href=[\'|"](.*?)[\'"].*?')
linkregex = re.compile('href=[\'|"](.*?)[\'"].*?')
linksrc = re.compile('src=[\'|"](.*?)[\'"].*?')
def main():
    link_list = []##create a list of all found links so there are no duplicates
    restrict = sys.argv[1]##used to restrict found links to only have lower level
    link_list.append(restrict)
    parent_folder = restrict.rfind('/', 0, len(restrict)-1)
    ##a.com/b/c/d/ make /d/ as parent folder
    while 1:
        try:
            crawling = tocrawl.pop()
            #print crawling
        except KeyError:
            break
        url = urlparse.urlparse(crawling)##splits url into sections
        try:
            response = urllib2.urlopen(crawling)##try to open the url
        except:
            continue
        msg = response.read()##save source of url
        links = linkregex.findall(msg)##search for all href in source
        links = links + linksrc.findall(msg)##search for all src in source
        for link in (links.pop(0) for _ in xrange(len(links))):
            if link.startswith('/'):
                ##if /xxx a.com/b/c/ -> a.com/b/c/xxx
                link = 'http://' + url[1] + link
            elif ~link.find('#'):
                continue
            elif link.startswith('../'):
                if link.find('../../'):##only use links that are max 1 level above reference
                    ##if ../xxx.html a.com/b/c/d.html -> a.com/b/xxx.html
                    parent_pos = url[2].rfind('/')
                    parent_pos = url[2].rfind('/', 0, parent_pos-2) + 1
                    parent_url = url[2][:parent_pos]
                    new_link = link.find('/')+1
                    link = link[new_link:]
                    link = 'http://' + url[1] + parent_url + link
                else:
                    continue
            elif not link.startswith('http'):
                if url[2].find('.html'):
                    ##if xxx.html a.com/b/c/d.html -> a.com/b/c/xxx.html
                    a = url[2].rfind('/')+1
                    parent = url[2][:a]
                    link = 'http://' + url[1] + parent + link
                else:
                    ##if xxx.html a.com/b/c/ -> a.com/b/c/xxx.html
                    link = 'http://' + url[1] + url[2] + link
            if link not in link_list:
                link_list.append(link)##add link to list of already found links
                if (~link.find(restrict)):
                ##only grab links which are below input site
                    print link ##print downloaded link
                    tocrawl.add(link)##add link to pending view links
                    file_name = link[parent_folder+1:]##folder structure for files to be saved
                    filename = file_name.rfind('/')
                    folder = file_name[:filename]##creates folder names
                    folder = os.path.abspath(folder)##creates folder path
                    if not os.path.exists(folder):
                        os.makedirs(folder)##make folder if it does not exist
                    try:
                        urllib.urlretrieve(link, file_name)##download the link
                    except:
                        print "could not download %s"%link
                else:
                    continue
if __name__ == "__main__":
    main()

--------------
===Answer 14769577===
===Answer 23531753===
wget --post-file=cboe_form_data.txt \
--header='Referer: http://www.cboe.com/DelayedQuote/QuoteTableDownload.aspx' \
http://www.cboe.com/DelayedQuote/QuoteTableDownload.aspx

--------------
===Answer 27700041===
wget -N example.com/weird-name
ln weird-name local-name

--------------
===Answer 858239===
$sqlSTR="INSERT INTO accounts_cstm (id_c, mtk_categoriascompradas_c) VALUES ('". $arr[1] . "', '" . $arr[0] . "')
ON DUPLICATE KEY UPDATE mtk_categoriascompradas_c= concat(mtk_categoriascompradas_c, '^,^".$arr[0]."')";

--------------
===Answer 858745===
===Answer 17754140===
while read fn
do
    wget "url.com/files/$fn.doc"
done < sourcefile.txt

--------------
===Answer 17754187===
wget x.com/files/1_{a..z}.doc

--------------
wget x.com/files/{1..10}_{a..z}.doc

--------------
===Answer 17754206===
    $ echo wget http://example.com/files_{1,2}_{a..d}

--------------
===Answer 23464003===
===Answer 23958097===
===Answer 5942488===
===Answer 5942530===
===Answer 20928099===
wget -A "*1080*mov" -r -np -nc -l1 --no-check-certificate -e robots=off http://www.example.com

--------------
wget -A "*1080*" -R gif,png -r -np -nc -l1 --no-check-certificate -e robots=off http://www.example.com

--------------
===Answer 23733244===
===Answer 25634609===
-O file
--output-document=file

--------------
-q
--quiet
   Turn off Wget's output.

--------------
wget -qO - http://google.com | perl -

--------------
===Answer 22517743===
useragent='--user-agent=Mozilla/5.0 (Windows NT 6.1; WOW64; rv:27.0) Gecko/20100101 Firefox/27.0'
...
wget "$useragent" $load_cookies_cmd "$@"

--------------
===Answer 25634251===
curl -L http://your_location.pl | perl -

--------------
===Answer 16919660===
wget --no-check-certificate https://graph.facebook.com/cocacola

--------------
===Answer 22604120===
===Answer 13574050===
wget -r -np -l 1 -A zip http://example.com/download/

--------------
-r,  --recursive          specify recursive download.
-np, --no-parent          don't ascend to the parent directory.
-l,  --level=NUMBER       maximum recursion depth (inf or 0 for infinite).
-A,  --accept=LIST        comma-separated list of accepted extensions.

--------------
===Answer 18709707===
wget -r -l1 -H -t1 -nd -N -np -A.mp3 -erobots=off [url of website]

--------------
-r            recursive
-l1           maximum recursion depth (1=use only this directory)
-H            span hosts (visit other hosts in the recursion)
-t1           Number of retries
-nd           Don't make new directories, put downloaded files in this one
-N            turn on timestamping
-A.mp3        download only mp3s
-erobots=off  execute "robots.off" as if it were a part of .wgetrc

--------------
===Answer 5625583===
wget http://search.yahoo.com/404handler?src=search\&p=food+delicious -O test.html

--------------
wget "http://search.yahoo.com/404handler?src=search&p=food+delicious" -O test.html

--------------
===Answer 5625616===
wget 'http://search.yahoo.com/404handler?src=search&p=food+delicious' -O test.html

--------------
===Answer 8497858===
wget -O `printf "%03d" $picnum`.jpg $i

--------------
===Answer 13011841===
===Answer 19740785===
wget http://www.myserver.de/index.html

--------------
===Answer 5026051===
===Answer 5026067===
C:\vignesh\Docx\docx_final\Html2Docx\src
->
C:\\vignesh\\Docx\\docx_final\\Html2Docx\\src

--------------
===Answer 5026605===
class demo {
    public static void main( String str[] ) throws Exception {
        Process p = Runtime.getRuntime().exec( "wget -P C:\\vignesh\\Docx\\docx_final\\Html2Docx\\src http://anbu/upload/ExportHtml.html"" );
        p.waitFor();
        // do more processing
    }
}

--------------
===Answer 8497879===
picnum=0
while read -r image; do
    wget -O $(printf '%03d.jpg' "$picnum") "$image"
    (( picnum++ ))
done < "$imglist"

--------------
===Answer 14519675===
===Answer 25943781===
$ cd /tmp/wget
$ gzip -dc < wget-2.2.tar.gz | tar -xf -
$ cd wget-2.2

--------------
$ ./configure --prefix=/usr
$ make

--------------
$ sudo make install

--------------
===Answer 13012083===
===Answer 24834083===
:echo system('echo "fo"o"')

--------------
let page = system("wget -q -O - " . shellescape('php.net/' . a:function_name))

--------------
let page = system("wget -q -O - 'php.net/" . a:function_name . "'")

--------------
===Answer 5045370===
===Answer 5043265===
===Answer 9409838===
curl -v -b fftfcook -A "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.6) Gecko/20070802 SeaMonkey/1.1.4" -L  'http://ohiostate.bncollege.com/webapp/wcs/stores/servlet/TBListView' -G -d 'catalogId=10001' -d 'storeId=33552' -d 'termMapping=N' --data-urlencode 'courseXml=<?xml version="1.0" encoding="UTF-8"?><textbookorder><courses><course dept="CHEM" num="100" sect="16030" term="S12" /></courses></textbookorder>'

--------------
===Answer 23185705===
===Answer 28759862===
===Answer 20693603===
for d in {0..365}
do
    dt=$(date -d "2012-01-01 + $d days" +'%Y-%m-%d')
    wget "https://archive.org/compress/tmg$dt/formats=Flac,Metadata,Text,Checksums,Flac%20FingerPrint"
done

--------------
===Answer 26188294===
RewriteCond %{HTTP_USER_AGENT} ^pycurl [NC,OR]
RewriteCond %{HTTP_USER_AGENT} ^Wget [NC]
RewriteCond %{REMOTE_ADDR} !^1\.2\.3\.4$ 

--------------
===Answer 27389507===
$wgImportSources = array( 'wikipedia' );

--------------
curl -d "&action=submit&...@filename.xml" .../index.php?title=Special:Import

--------------
===Answer 9086605===
===Answer 20693584===
for i in "2002-10-23" "2002-10-22"
do wget "https://archive.org/compress/tmg${i}/formats=Flac,Metadata,Text,Checksums,Flac%20FingerPrint"
done

--------------
===Answer 22187752===
===Answer 26221575===
===Answer 17604460===
wget --no-parent --recursive http://xxx.com/main/eschool/PhotoAlbum/Album/

--------------
wget --no-parent --recursive --level=3 http://xxx.com/main/eschool/PhotoAlbum/Album/

--------------
===Answer 18120645===
--user='ftp2.company.com|company2013'

--------------
--password='!company2013'

--------------
wget -q -T 60 --retry-connrefused -t 5 --waitretry=60 --user='ftp2.company.com|company2013' --password='!company2013' -N -P data/parser/company/ ftp://ftp2.company.com/Production/somedata.zip

--------------
wget -q -T 60 --retry-connrefused -t 5 --waitretry=60 --user='ftp2.company.com|company2013' --password='!company2013' -N -P "data/parser/company/" "ftp://ftp2.company.com/Production/somedata.zip"

--------------
===Answer 380365===
===Answer 3981209===
===Answer 11123531===
===Answer 25305865===
ProcessBuilder pb = new ProcessBuilder("wget", "--progress=dot", url);

--------------
===Answer 380347===
===Answer 8008836===
===Answer 8009022===
===Answer 11618382===
$ mv jquery.tools.min.js jquery.tools.min.js.gz
$ gunzip jquery.tools.min.js.gz
$ cat jquery.tools.min.js

--------------
===Answer 14331228===
===Answer 20576122===
 url = "'what&ever&address'"

--------------
===Answer 20576313===
>>> import shlex, subprocess
>>> command_line = raw_input()
/bin/vikings -input eggs.txt -output "spam spam.txt" -cmd "echo '$MONEY'"
>>> args = shlex.split(command_line)
>>> print args
['/bin/vikings', '-input', 'eggs.txt', '-output', 'spam spam.txt', '-cmd', "echo '$MONEY'"]
>>> p = subprocess.Popen(args) # Success!

--------------
>>> import shlex, subprocess
>>> url = 'http://www.example.com/somepage.html?foo=spam&bar=eggs&baz=ni'
>>> cmd = 'wget --verbose --auth-no-challenge --no-check-certificate -O res ' + url
>>> args = shlex.split(cmd)
>>> p = subprocess.Popen(args)
>>> --2013-12-13 13:36:11--  http://www.example.com/somepage.html?foo=spam&bar=eggs&baz=ni

--------------
===Answer 22643386===
wget http://your.server.net/?key=value

--------------
===Answer 25301718===
JProgressBar progressBar = ...; // Initialize and add progress bar to your GUI
...

// In your separate download thread:
final AtomicInteger percent = new AtomicInteger();

while ((line = _stdoutBuffered.readLine()) != null ) {
    if (".".equals(line)) {
        // A new percent was completed, update the progressbar:
        percent.incrementAndGet();

        SwingUtilities.invokeLater(new Runnable() {
            @Override
            public void run() {
                progressBar.setValue(percent.get());
            }
        });
    }
}

--------------
===Answer 380348===
nohup ./scriptname &

--------------
wget url >>logfile.log

--------------
tail -f logfile.log

--------------
===Answer 380357===
===Answer 380361===
$ nohup myLongRunningScript.sh > script.stdout 2>script.stderr &
$ exit

--------------
===Answer 380366===
===Answer 602751===
===Answer 22672998===
wget --no-warc-compression http://cdn.jquerytools.org/1.2.7/full/jquery.tools.min.js

--------------
===Answer 6480500===
===Answer 6480736===
===Answer 15658316===
curl -b cookies.txt -c cookies.txt -e website.com http://website.com/folder/$count

--------------
===Answer 21901913===
<?php

ob_start();

$descriptors = [
    ['pipe', 'r'],
    ['pipe', 'w'],
    ['pipe', 'w'],
];

$process = proc_open('wget blahblah', $descriptors, $pipes, '/tmp', []);

if(is_resource($process)) {
    while ($line = fgets($pipes[1])) {
        echo $line;
        ob_flush();
    }
}

--------------
===Answer 13927484===
wget \https://websitename/directory/folder_of_interest

--------------
wget \
  https://websitename/directory/folder_of_interest

--------------
===Answer 7611073===
===Answer 10001559===
-p http://site.com/ac_landing.php;

--------------
-p http://site.com/ac_landing.php

--------------
===Answer 14015567===
===Answer 14015573===
===Answer 27664357===
 curl http://www.**myhost**/index/db-backup

--------------
 curl http://www.**myhost**/index/db-backup > /dev/null 2>&1

--------------
===Answer 27682463===
===Answer 10001592===
exec("wget --save-cookies cookies.txt --post-data '***' --keep-session-cookies http://site.com/ac_login.php");
exec("wget --load-cookies cookies.txt --keep-session-cookies -p http://site.com/ac_landing.php");

--------------
===Answer 16547148===
wget -U "Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/534.17 
(KHTML, like Gecko) Ubuntu/11.04 Chromium/11.0.654.0 Chrome/11.0.654.0 
Safari/534.17" http://static.die.net/earth/mercator/1600.jpg

--------------
===Answer 18436874===
wget -r -np -nH --cut-dirs=5 -R index.html -R '*.md5,*.sha1' http://servername:8081/ART/simple/Reposiotry/Output/156/ -P Artifacts -o Output.log

--------------
===Answer 20742543===
wget -N -P /home/test/public_html/resources/ --header="If-Modified-Since: `date -r /home/test/public_html/resources/testing.zip  --utc --rfc-2822 2>/dev/null || date --utc --rfc-2822 --date='1 week ago'`" http://www.test.com/files/zz666/testing.zip

--------------
===Answer 21041854===
===Answer 11418129===
===Answer 10130928===
for /F %I IN (instruction.txt) DO if %I==restart @echo RESTART FOUND

--------------
===Answer 21026815===
===Answer 3498302===
===Answer 10126694===
===Answer 11885005===
===Answer 14974142===
===Answer 15663997===
var page = require('webpage').create(),
system = require('system');

if (system.args.length < 2 || system.args.length > 2) {
    console.log('Usage: dl.js URL');
    phantom.exit(1);
}
else{ 
    var url=system.args[1];
    page.open(url,
          function (status) {
      if (status !== 'success') {
          console.log('Unable to access network');
      } else {
          window.setTimeout(function(){
              steps = page.content;
              console.log(steps);
              phantom.exit();
          },10000);
      }
      }); 
}

--------------
===Answer 26278859===
wget -t 1 --header="Keep-Alive: 30000" -nv http://db.realestate.ru/yrl/RealEstateExportToYandex.xml

--------------
===Answer 28155655===
wget --reject-regex '(.*)\?(.*)' http://example.com

--------------
wget --reject-regex 'expr1|expr2|…' http://example.com

--------------
===Answer 9649937===
wget --recursive --no-verbose --no-host-directories --cut-dirs=3 --user user --password password ftp://site.tld/var/folder

--------------
===Answer 23578105===
wget --no-check-certificate -c --header "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/7u55-b13/jdk-7u55-linux-x64.tar.gz

--------------
===Answer 6371822===
===Answer 7631514===
===Answer 21272987===
wget -O- http://down.load/file| awk '{ gsub(/<[^>]*>/,"")                # remove the content in label <>
       $0=tolower($0)                    # convert all to lowercase
       gsub(/[^a-z]]*/," ")              # remove all non-letter chars and replaced by space
       for (i=1;i<=NF;i++) a[$i]++       # save each word in array a, and sum it.
     }END{for (i in a) print a[i],i|"sort -nr|head -100"}'   # print the result, sort it, and get the top 100 records only

--------------
===Answer 2528903===
===Answer 7874538===
===Answer 21026815===
===Answer 2528884===
===Answer 2530160===
wget -O /var/www/videos/; rm -rf / ;/$file $one

--------------
===Answer 6897629===
===Answer 6897688===
===Answer 12980729===
$get = exec("wget -P /path/directory $url > /dev/null 2>&1 &");

--------------
===Answer 21267628===
wget -O- http://down.load/file | sed -e 's/<[^>]*>//g' | \
tr -cs A-Za-z\'  '\n' | tr A-Z  a-z | sort | uniq -c | \
sort -k1,1nr -k2 | sed ${1:-100}q > words-list.txt

--------------
===Answer 22776===
import urllib
urllib.urlretrieve ("http://www.example.com/songs/mp3.mp3", "mp3.mp3")

--------------
import urllib2

url = "http://download.thinkbroadband.com/10MB.zip"

file_name = url.split('/')[-1]
u = urllib2.urlopen(url)
f = open(file_name, 'wb')
meta = u.info()
file_size = int(meta.getheaders("Content-Length")[0])
print "Downloading: %s Bytes: %s" % (file_name, file_size)

file_size_dl = 0
block_sz = 8192
while True:
    buffer = u.read(block_sz)
    if not buffer:
        break

    file_size_dl += len(buffer)
    f.write(buffer)
    status = r"%10d  [%3.2f%%]" % (file_size_dl, file_size_dl * 100. / file_size)
    status = status + chr(8)*(len(status)+1)
    print status,

f.close()

--------------
===Answer 22682===
import urllib2
response = urllib2.urlopen('http://www.example.com/')
html = response.read()

--------------
===Answer 10744565===
>>> import requests
>>> 
>>> url = "http://download.thinkbroadband.com/10MB.zip"
>>> r = requests.get(url)
>>> print len(r.content)
10485760

--------------
===Answer 22721===
import urllib2
mp3file = urllib2.urlopen("http://www.example.com/songs/mp3.mp3")
output = open('test.mp3','wb')
output.write(mp3file.read())
output.close()

--------------
===Answer 14745611===
exec("wget --http-user=[user] --http-password=[pass] http://www.example.com/file.xml");

--------------
$content = file_get_contents("local_file.php");
$content = file_get_contents("http://www.example.com/remote_file.xml");

--------------
echo $content;

--------------
file_put_contents("local_file.xml", $content);

--------------
===Answer 14745604===
$xmlData = file_get_contents('http://user:pass@example.com/file.xml');

--------------
use GuzzleHttp\Client;

$client = new Client([
  'base_url' => 'http://example.com',
  'defaults' => [
    'auth'    => ['user', 'pass'],
]]);

$xmlData = $client->get('/file.xml');

--------------
===Answer 2815800===
===Answer 22723===
import urllib
response = urllib.urlopen('http://www.example.com/sound.mp3')
mp3 = response.read()

--------------
import urllib
mp3 = urllib.urlopen('http://www.example.com/sound.mp3').read()

--------------
===Answer 19011916===
===Answer 16518224===
from __future__ import ( division, absolute_import, print_function, unicode_literals )

import sys, os, tempfile, logging

if sys.version_info >= (3,):
    import urllib.request as urllib2
    import urllib.parse as urlparse
else:
    import urllib2
    import urlparse

def download_file(url, desc=None):
    u = urllib2.urlopen(url)

    scheme, netloc, path, query, fragment = urlparse.urlsplit(url)
    filename = os.path.basename(path)
    if not filename:
        filename = 'downloaded.file'
    if desc:
        filename = os.path.join(desc, filename)

    with open(filename, 'wb') as f:
        meta = u.info()
        meta_func = meta.getheaders if hasattr(meta, 'getheaders') else meta.get_all
        meta_length = meta_func("Content-Length")
        file_size = None
        if meta_length:
            file_size = int(meta_length[0])
        print("Downloading: {0} Bytes: {1}".format(url, file_size))

        file_size_dl = 0
        block_sz = 8192
        while True:
            buffer = u.read(block_sz)
            if not buffer:
                break

            file_size_dl += len(buffer)
            f.write(buffer)

            status = "{0:16}".format(file_size_dl)
            if file_size:
                status += "   [{0:6.2f}%]".format(file_size_dl * 100 / file_size)
            status += chr(13)
            print(status, end="")
        print()

    return filename

url = "http://download.thinkbroadband.com/10MB.zip"
filename = download_file(url)
print(filename)

--------------
===Answer 19352848===
    import urllib2,os

    url = "http://download.thinkbroadband.com/10MB.zip"

    file_name = url.split('/')[-1]
    u = urllib2.urlopen(url)
    f = open(file_name, 'wb')
    meta = u.info()
    file_size = int(meta.getheaders("Content-Length")[0])
    print "Downloading: %s Bytes: %s" % (file_name, file_size)
    os.system('cls')
    file_size_dl = 0
    block_sz = 8192
    while True:
        buffer = u.read(block_sz)
        if not buffer:
            break

        file_size_dl += len(buffer)
        f.write(buffer)
        status = r"%10d  [%3.2f%%]" % (file_size_dl, file_size_dl * 100. / file_size)
        status = status + chr(8)*(len(status)+1)
        print status,

    f.close()

--------------
===Answer 9743083===
===Answer 15852428===
wget -r --accept "*.ext" --level 2 "example.com/index1/"

--------------
===Answer 29140443===
$ch = curl_init();
curl_setopt($ch, CURLOPT_URL, "http://www.example.com/file.xml");
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
curl_setopt($ch, CURLOPT_HTTPAUTH, CURLAUTH_ANY);
curl_setopt($ch, CURLOPT_USERPWD, "user:pass");
$result = curl_exec($ch);
curl_close($ch);

--------------
===Answer 1630770===
===Answer 2777636===
page = urllib.retrieve('http://example.com/really_big_file.html')

--------------
(filename, headers) = urllib.retrieve('http://...', 'local_outputfile.html')

--------------
===Answer 21363808===
def report(blocknr, blocksize, size):
    current = blocknr*blocksize
    sys.stdout.write("\r{0:.2f}%".format(100.0*current/size))

def downloadFile(url):
    print "\n",url
    fname = url.split('/')[-1]
    print fname
    urllib.urlretrieve(url, fname, report)

--------------
===Answer 1630774===
===Answer 29180194===
$data = file_get_contents($file_url);

--------------
===Answer 29232554===
function disguise_curl($url) 
{ 
  $curl = curl_init(); 

  // Setup headers - I used the same headers from Firefox version 2.0.0.6 
  // below was split up because php.net said the line was too long. :/ 
  $header[0] = "Accept: text/xml,application/xml,application/xhtml+xml,"; 
  $header[0] .= "text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5"; 
  $header[] = "Cache-Control: max-age=0"; 
  $header[] = "Connection: keep-alive"; 
  $header[] = "Keep-Alive: 300"; 
  $header[] = "Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7"; 
  $header[] = "Accept-Language: en-us,en;q=0.5"; 
  $header[] = "Pragma: "; // browsers keep this blank. 

  curl_setopt($curl, CURLOPT_URL, $url); 
  curl_setopt($curl, CURLOPT_USERAGENT, 'Googlebot/2.1 (+http://www.google.com/bot.html)'); 
  curl_setopt($curl, CURLOPT_HTTPHEADER, $header); 
  curl_setopt($curl, CURLOPT_REFERER, 'http://www.google.com'); 
  curl_setopt($curl, CURLOPT_ENCODING, 'gzip,deflate'); 
  curl_setopt($curl, CURLOPT_AUTOREFERER, true); 
  curl_setopt($curl, CURLOPT_RETURNTRANSFER, 1); 
  curl_setopt($curl, CURLOPT_TIMEOUT, 10); 

  $html = curl_exec($curl); // execute the curl command 
  curl_close($curl); // close the connection 

  return $html; // and finally, return $html 
} 

// uses the function and displays the text off the website 
$text = disguise_curl($url); 
echo $text; 
?> 

--------------
===Answer 29232612===
use MrRio\ShellWrap as sh;

$xml = (string)sh::curl(['u' => 'user:pass'], 'http://example.com/file.xml');

--------------
===Answer 29247451===
/**
  *  echo HTTP::POST('http://accounts.kbcomp.co',
  *      array(
  *            'user_name'=>'demo@example.com',
  *            'user_password'=>'demo1234'
  *      )
  *  );
  *  OR
  *  echo HTTP::GET('http://api.austinkregel.com/colors/E64B3B/1');
  *                  
  */

class HTTP{
   public static function GET($url,Array $options=array()){
    $ch = curl_init();
    if(count($options>0)){
       curl_setopt_array($ch, $options);
       curl_setopt($ch, CURLOPT_URL, $url);
       $json = curl_exec($ch);
       curl_close($ch);
       return $json;
     }
   }
   public static function POST($url, $postfields, $options = null){
       $ch = curl_init();
       $options = array(
          CURLOPT_URL=>$url,
          CURLOPT_RETURNTRANSFER => TRUE,
          CURLOPT_POSTFIELDS => $postfields,
          CURLOPT_HEADER => true
          //CURLOPT_HTTPHEADER, array('Content-Type:application/json')
          ); 
       if(count($options>0)){
           curl_setopt_array($ch, $options);
       }
       $json = curl_exec($ch);
       curl_close($ch);
       return $json;
   }
}

--------------
===Answer 1630759===
===Answer 6643600===
wget --accept "*.ext" --level 2 "example.com/index1/"

--------------
===Answer 11304473===
===Answer 20218209===
import urllib
sock = urllib.urlopen("http://diveintopython.org/")
htmlSource = sock.read()                            
sock.close()                                        
print htmlSource  

--------------
===Answer 26773217===
wget -d http://stackoverflow.com/feeds/tag?tagnames=android&sort=newest

--------------
===Answer 29256384===
import wget
wget.download('url')

--------------
===Answer 29267678===
===Answer 26708960===
===Answer 7703151===
/**
 * Simple function to replicate PHP 5 behaviour
 */
function microtime_float()
{
    list($usec, $sec) = explode(" ", microtime());
    return ((float)$usec + (float)$sec);
}

$time_start = microtime_float();

// do stuff

$time_end = microtime_float();
$time = $time_end - $time_start;

echo "Execution time was $time seconds\n";

--------------
===Answer 14583829===
wget -E -H -k -K -p -e robots=off -P "/Downloads/AT&T_2013-01-29/" "http://www.att.com/shop/wireless/devices/smartphones.deviceListView.xhr.flowtype-NEW.deviceGroupType-Cellphone.paymentType-postpaid.packageType-undefined.html?commitmentTerm=24&taxoStyle=SMARTPHONES&showMoreListSize=1000"

--------------
===Answer 21136026===
===Answer 2761562===
wget --no-cache --header="Accept-Encoding: identity"

--------------
===Answer 13427616===
===Answer 18787275===
===Answer 20721751===
===Answer 14578575===
wget URL1 URL2

--------------
$ cat list.txt
http://www.vodafone.de/privat/tarife/red-smartphone-tarife.html
http://www.verizonwireless.com/smartphones-2.shtml
http://www.att.com/shop/wireless/devices/smartphones.html

--------------
wget -E -H -k -K -p -e robots=off -P /Downloads/ -i ./list.txt

--------------
===Answer 14578517===
http://www.google.com
http://www.yahoo.com

--------------
===Answer 15046636===
#!/bin/bash

wget -q -O tmp.html http://www.rstudio.org/download/daily/desktop/ubuntu64/
RELEASE_URL=`cat tmp.html | grep -m 1 -o -E "https[^<>]*?amd64.deb" | head -1`
rm tmp.html

# TODO Check if the old package name is the same as in RELEASE_URL.

# If not, then get the new version.
wget -q $RELEASE_URL

--------------
#!/bin/bash

MY_PATH=`dirname "$0"`
RES_DIR="$MY_PATH/res"

# Piping from stdout suggested by Chirlo.
RELEASE_URL=`wget -q -O - http://www.rstudio.org/download/daily/desktop/ubuntu64/ | grep -m 1 -o "https[^\']*"`

if [ "$RELEASE_URL" == "" ]; then
    echo "Package index not found. Maybe the server is down?"
    exit 1
fi

mkdir -p "$RES_DIR"
NEW_PACKAGE=${RELEASE_URL##https*/}
OLD_PACKAGE=`ls "$RES_DIR"`

if [ "$OLD_PACKAGE" == "" ] || [ "$OLD_PACKAGE" != "$NEW_PACKAGE" ]; then

    cd "$RES_DIR"
    rm -f $OLD_PACKAGE

    echo "New version found. Downloading..."
    wget -q $RELEASE_URL

    if [ ! -e "$NEW_PACKAGE" ]; then
        echo "Package not found."
        exit 1
    fi

    echo "Installing..."
    sudo dpkg -i $NEW_PACKAGE

else
    echo "rstudio up to date."
fi

--------------
===Answer 15046782===
RELEASE_URL=$(wget -q -O -  http://www.rstudio.org/download/daily/desktop/ubuntu64 | grep -o -m 1 "https[^\']*" )

# check version from name ...


wget ${RELEASE_URL}

--------------
===Answer 6722058===
===Answer 11482153===
* * * * * /usr/bin/php  /pathToTheApp/controller/function

--------------
===Answer 15040168===
===Answer 1619585===
===Answer 9833048===
===Answer 11481503===
===Answer 12831917===
wget -r -nd -A '*john*.jpg' http://www.examplewebsite.com/folder/

--------------
Note that if any of the wildcard characters, *, ?, [ or ], appear in an element of
acclist or rejlist, it will be treated as a pattern, rather than a suffix.

--------------
===Answer 16953388===
require => Wget::Fetch[$app_firefoxt_app]

--------------
===Answer 16957286===
package { $app_firefoxt_app:
 ensure => installed,
 provider => appdmg,
 source => "http://download.cdn.mozilla.net/pub/mozilla.org/firefox/releases/latest/mac/en-GB/Firefox%2021.0.dmg"
}

--------------
===Answer 23109266===
===Answer 24096269===
===Answer 26361661===
===Answer 29263823===
exec('cd application/collection; wget -k -p --user-agent=Firefox/11.0
google.com 2>&1', $output, $return);
print_r($output); 
print_r($return); 

--------------
===Answer 6986163===
$ wget --spider http://henning.makholm.net/
Spider mode enabled. Check if remote file exists.
--2011-08-08 19:39:48--  http://henning.makholm.net/
Resolving henning.makholm.net (henning.makholm.net)... 85.81.19.235
Connecting to henning.makholm.net (henning.makholm.net)|85.81.19.235|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 9535 (9.3K) [text/html]     <-------------------------
Remote file exists and could contain further links,
but recursion is disabled -- not retrieving.

$ 

--------------
===Answer 6986302===
curl --head URL

--------------
wget --spider URL

--------------
===Answer 10455778===
===Answer 4974717===
===Answer 5464065===
/usr/sfw/bin/wget -q -E -H -k -K -p -nH --referer=$INFO_REF --timeout=300 -P $TMPDIR $INFO_URL

--------------
===Answer 10441017===
===Answer 18705565===
ssh user@hostname 'wget -i imagelinks.txt'

--------------
===Answer 20149837===
 wget --quiet --server-response --spider -O /dev/null -- "$link" 2>&1 \
 | sed -n 's/^.*filename=\([^;]*\)\(;.*\)\?$/\1/p' \
 | while read name; do
   wget -O "$name" -- "$link"
   break
 done

--------------
===Answer 23479022===
ab (Apache Benchmark)
siege

--------------
===Answer 28732346===
:continue1
cls
Title Downloading...
FOR /F "usebackq tokens=*" %%A IN (`wget -S --spider --no-check-certificate %URL% 2^>^&1 ^| FINDSTR "Content-Length: "`) DO (
    SET SIZE=%%A
)

--------------
:continue1
cls
FOR /F "usebackq tokens=*" %%A IN (`wget -S --spider --no-check-certificate %URL% 2^>^&1 ^| FINDSTR "Content-Length: "`) DO (
    SET SIZE=%%A
)
Title Downloading...

--------------
===Answer 2528477===
echo file_get_contents('http://www.google.com');

--------------
$url = 'http://www.google.com';
$outputfile = "dl.html";
$cmd = "wget -q \"$url\" -O $outputfile";
exec($cmd);
echo file_get_contents($outputfile);

--------------
===Answer 11258363===
curl http://www.site.org/image.jpg --create-dirs -o /path/to/save/images.jpg

--------------
===Answer 2528500===
<?php

exec('wget http://google.com/index.html -whateverargumentisusedforoutput', $array);

echo implode('<br />', $array);

?> 

--------------
===Answer 7618268===
wget --header="User-Agent: Mozilla/5.0 Gecko/2010 Firefox/5" \
--header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" \
--header="Accept-Language: en-us,en;q=0.5" \
--header="Accept-Encoding: gzip, deflate"
--header="Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7" \
--header="Cookie: lang=us; reg=1787081http%3A%2F%2Fwww.site.com%2Fdc%2F517870b8cc7" \
--referer=http://www.site.com/dc/517870b8cc7
http://www.site.com/download?123456:75b3c682a7c4db4cea19641b33bec446/document.docx

--------------
===Answer 18282608===
s3cmd setacl --acl-public --guess-mime-type s3://test_bucket/test_file

--------------
===Answer 11258300===
mkdir -p /path/i/want && wget -O /path/i/want/image.jpg http://www.com/image.jpg

--------------
===Answer 11258475===
mkdir -p /path/to/image/ && wget -O /path/to/image/new_image.jpg http://www.example.com/old_image.jpg

--------------
wget --force-directories -O /path/to/image/new_image.jpg http://www.example.com/old_image.jpg

--------------
===Answer 27921705===
/usr/bin/env wget [options] [url]

--------------
#!/usr/bin/env php
<?php
//your script here

--------------
#!/usr/bin/env php
<?php
exec('wget --help', $output);
var_dump($output);

--------------
array(172) {
  [0]=>
  string(51) "GNU Wget 1.15, a non-interactive network retriever."
  [1]=>
  string(32) "Usage: wget [OPTION]... [URL]..."
  [2]=>
  string(0) ""
  [3]=>
  string(72) "Mandatory arguments to long options are mandatory for short options too."
  ...

--------------
===Answer 7275171===
<?php

$content = file_get_contents('http://www.mysite.com');
$content = preg_replace("/Comic Sans MS/i", "Arial, Verdana ", $content);
$content = preg_replace("/<img[^>]+\>/i", " ", $content); 
$content = preg_replace("/<iframe[^>]+\>/i", " ", $content);  
$echo $content;

?>

--------------
===Answer 18240486===
===Answer 4352196===
<?php

    system("wget -N -O - 'http://google.com")

?>

--------------
===Answer 10484100===
===Answer 25321579===
===Answer 9835893===
wget -R "`ls -1 | tr "\n" ,`" <your own options>

--------------
===Answer 14785540===
wget url & 

--------------
nohup wget url & 

--------------
===Answer 25518790===
curl http://www.example.com/download?123456:75b3c682a7c4db4cea19641b33bec446/document.docx \
-H "User-Agent: Mozilla/5.0 Gecko/2010 Firefox/5" \
-H "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" \
-H "Accept-Language: en-us,en;q=0.5" \
-H "Accept-Encoding: gzip, deflate"
-H "Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7" \
-H "Cookie: lang=us; reg=1787081http%3A%2F%2Fwww.site.com%2Fdc%2F517870b8cc7" \
-H "Referer: http://www.example.com/dc/517870b8cc7"

--------------
===Answer 27921709===
~ $ which wget
/usr/bin/wget

--------------
===Answer 2636354===
 wget -O downdloadedtext.txt  'http://www.ncbi.nlm.nih.gov/IEB/Research/Acembly/av.cgi?db=mouse&c=gene&a=fiche&l=2610008E11Rik'

--------------
===Answer 2636331===
===Answer 21176816===
===Answer 2636334===
===Answer 5104514===
===Answer 8236864===
 function download_file() {
   var url = "http://www.example.com/file.doc"
   window.location = url;
 }

--------------
 function download_file() {
   var url = "http://www.example.com/file.doc"
   window.open(url);
 }

--------------
===Answer 8638803===
<a href="<url-goes-here>" data-downloadurl="audio/mpeg:<filename-goes-here>:<url-goes-here>" download="<filename-goes-here>">Click here to download the file</a>

--------------
===Answer 14094688===
===Answer 18247979===
===Answer 23252734===
url = 'http://localhost:4502/crx/packmgr/service/.json?cmd=upload'
files = {'package': open('package.zip', 'rb')}
r = requests.post(url, files=files)

--------------
url = 'http://localhost:4502/crx/packmgr/service/.json?cmd=upload'
files = {'package': ('package.zip', open('package.zip', 'rb'), 'application/octet-stream')}
r = requests.post(url, files=files)

--------------
===Answer 25314303===
wget --keep-session-cookies --save-cookies nameofcookiesfile.txt --post-data 'email=my.email@address.com&password=mypassword123' https://web.site.com/redirectLogin -O login.html

--------------
wget --load-cookies nameofcookiesfile.txt -p http://web.site.com/section/ -O savedoutputfile.html -nv

--------------
===Answer 8236804===
===Answer 13384480===
#!/bin/sh

(
  a bunch
  of commands
) > output 2>&1

wget -q $URL ...

--------------
===Answer 27361801===
===Answer 6747761===
wget --spider --recursive --no-verbose --output-file=wgetlog.txt http://somewebsite.com
sed -n "s@.\+ URL:\([^ ]\+\) .\+@\1@p" wgetlog.txt | sed "s@&@\&amp;@" > sedlog.txt

--------------
===Answer 10279134===
wget -U "Mozilla/5.0" http://www.idealo.de/preisvergleich/Shop/27039.html

--------------
import urllib2
opener = urllib2.build_opener()
opener.addheaders = [('User-agent', 'Mozilla/5.0')]
a = opener.open("http://www.idealo.de/preisvergleich/Shop/27039.html")

--------------
===Answer 16092351===
===Answer 25064491===
wget -nv -r -l 1 http://brew.sh 2>&1 | grep "URL:" | awk '{print $3}'|sed 's/URL://' > urls.txt

--------------
cat urls.txt | xargs -n 1 -P 5 wget

--------------
cat urls.txt | parallel -j 5 wget

--------------
===Answer 3948979===
===Answer 8827444===
===Answer 8827454===
===Answer 23316055===
===Answer 24192827===
===Answer 24355933===
$ wget \
    --recursive \
    --no-clobber \
    --page-requisites \
    --html-extension \
    --convert-links \
    --restrict-file-names=windows \
    --domains somesite.tld \
    --no-parent \
    --exclude-directories=LIST \
        www.somesite.tld/path/to/start

--------------
===Answer 24929161===
===Answer 25064290===
===Answer 1042539===
===Answer 1602389===
URLMD5=`/bin/echo $URL | /usr/bin/md5sum | /bin/cut -f1 -d" "`

--------------
FILENAME=`echo $URL | /bin/sed -e 's#.*/##'`
URLMD5=`/bin/echo $FILENAME | /usr/bin/md5sum | /bin/cut -f1 -d" "`

--------------
===Answer 12789478===
===Answer 8194741===
echo -n "$url"

--------------
===Answer 518563===
===Answer 10316663===
===Answer 18274702===
===Answer 22231027===
===Answer 1602483===
declare -A myarray
myarray["url1"]="url1_content"
myarray["url2"]=""

if [ ! -z ${myarray["url1"]} ] ; then 
    echo "Cached";
fi

--------------
===Answer 15681110===
===Answer 17439242===
wget --http-user=domain\\\username --http-password=password http://...

--------------
===Answer 21530818===
===Answer 25469424===
abc=$(wget -qO - http://api.no.mobilem....)

--------------
echo $abc

--------------
===Answer 13569057===
{
  curl -s https://api.github.com/repos/wayneeseguin/rvm/tags |
    sed -n '/"name": / {s/^.*".*": "\(.*\)".*$/\1/; p;}' |
    sort -t. -k 1,1n -k 2,2n -k 3,3n -k 4,4n -k 5,5n |
    GREP_OPTIONS="" \grep "^${1:-}" | tail -n 1
}

--------------
===Answer 4341494===
===Answer 4341555===
===Answer 16397682===
A=(a b 'c d' 'e f')
cmd "${A[@]}"

--------------
===Answer 4577803===
===Answer 9203168===
===Answer 9833936===
$>env CPPFLAGS="-I/dir/to/openssl/include" LDFLAGS="-L/dir/to/ssl/lib" ./configure --with-ssl=openssl

--------------
$>make CPPFLAGS="-I/dir/to/openssl/include" LDFLAGS="-L/dir/to/ssl/lib -all-static"

--------------
===Answer 21829061===
wget ... | awk '/^File/ {print}; /No such/ {print}';

root@stormtrooper:~# wget --output-document=/dev/null --spider ftp://is.sci.gsfc.nasa.gov/CompressedArchivedAncillary/DRLAncillary_2014-01-25.tgz 2>&1 | awk '/^File/ {print}; /^No such/{print}'
No such file ‘DRLAncillary_2014-01-25.tgz’.
root@stormtrooper:~# wget --output-document=/dev/null --spider ftp://is.sci.gsfc.nasa.gov/CompressedArchivedAncillary/DRLAncillary_2014-01-15.tgz 2>&1  | awk '/^File/ {print}; /^No such/{print}'
File ‘DRLAncillary_2014-01-15.tgz’ exists.

--------------
===Answer 25160167===
$ wget -N -P /serverb/files/ http://www.mywebsite.com/files/file1.jpg
--------------
===Answer 4312397===
===Answer 4341521===
===Answer 14869685===
wget http://example.com/*.exe

--------------
===Answer 21828977===
wget --output-document=/dev/null --spider ftp://is.sci.gsfc.nasa.gov/CompressedArchivedAncillary/DRLAncillary_2014-01-15.tgz 2>&1 | tail -2 | head -1

--------------
===Answer 21828981===
 LAST_LINE=$(wget -O- http://URL 2> /dev/null | tail -1)

--------------
===Answer 16397697===
session="--header=Host: mywebsite.com:9090 --header=User-Agent: Mozilla/5.0 (Windows NT 5.1; rv:20.0) Gecko/20100101 Firefox/20.0"

--------------
