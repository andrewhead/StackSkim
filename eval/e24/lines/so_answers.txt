wget -r --no-parent http://mysite.com/configs/.vim/
wget -r --no-parent --reject "index.html*" http://mysite.com/configs/.vim/
wget --save-cookies cookies.txt      --post-data 'user=foo&password=bar'      http://server.com/auth.php
wget --load-cookies cookies.txt      -p http://server.com/interesting/article.php
wget ... -e use_proxy=yes -e http_proxy=127.0.0.1:8080 ...
wget -r -nH --cut-dirs=2 --no-parent --reject="index.html*" http://mysite.com/dir1/dir2/data
wget -N http://site.org/images/misc/pic.png
wget -O - -q -t 1 --timeout=600 http://www.example.com/cron/run
wget -e robots=off http://www.example.com/
wget <url>
wget -r -np -N [url] &
wget -r -np -N [url] &
wget -r -np -N [url] &
wget -r -np -N [url]
wget http://example.com/textfile.txt
wget -m http://example.com/configs/.vim/
wget -m -e robots=off --no-parent http://example.com/configs/.vim/
wget -r http://mysite.com/configs/.vim/
wget http://username:password@www.domain.com/page.html
wget -r http://stackoverflow.com/
wget --include downloads/good --mirror --execute robots=off --no-host-directories --cut-dirs=1 --reject="index.html*" --continue http://<host>/downloads/good
wget --mirror www.example.com
wget --mirror example.com
wget --mirror --domains=example.com example.com
wget --login yourlogin --password yourpassword yoururl
wget -r --user=prayagupd --password='/aGBLB8HTPx8c' --no-parent http://pseudononymous.com/repos/projects/k2c/docs/requirements/
wget -r -P /save/location -A jpeg,jpg,bmp,gif,png http://www.domain.com
wget -nd -r -l 2 -A jpg,jpeg,png,gif http://t.co
wget -nd -H -p -A jpg,jpeg,png,gif -e robots=off example.tumblr.com/page/{1..2}
wget -q -O /dev/stdout <URL>
wget -r -l1 --no-parent -A.cpp http://abc.com/programs/
wget --quiet -O - http://bit.ly/rQyhG5   | sed -n -e 's!.*<title>\(.*\)</title>.*!\1!p'
wget --quiet -O - http://bit.ly/rQyhG5   | paste -s -d " "    | sed -e 's!.*<head>\(.*\)</head>.*!\1!'   | sed -e 's!.*<title>\(.*\)</title>.*!\1!'
wget --quiet -O - http://bit.ly/rQyhG5   | paste -s -d " "    | sed -e 's!.*<head[^>]*>\(.*\)</head>.*!\1!'   | sed -e 's!.*<title>\(.*\)</title>.*!\1!'
wget --quiet -O - http://bit.ly/rQyhG5   | paste -s -d " "    | sed -n -e 's!.*<head[^>]*>\(.*\)</head>.*!\1!p'   | sed -n -e 's!.*<title>\(.*\)</title>.*!\1!p'
wget --quiet -O - http://bit.ly/rQyhG5   | sed -n -e 'H;${x;s!.*<head[^>]*>\(.*\)</head>.*!\1!;T;s!.*<title>\(.*\)</title>.*!\1!p}'
wget --quiet -O - http://bit.ly/rQyhG5   | sed -n -e 'H;${x;s!.*<head[^>]*>\(.*\)</head>.*!\1!;tnext;b;:next;s!.*<title>\(.*\)</title>.*!\1!p}'
wget --quiet -O - http://bit.ly/rQyhG5   | sed -n -e 'H;${x;s!.*<head[^>]*>\(.*\)</head>.*!\1!;tnext};b;:next;s!.*<title>\(.*\)</title>.*!\1!p'
wget --quiet -O - http://bit.ly/rQyhG5   | sed -n -f script
wget -k -m -r -q -t 1 -O - http://www.web.com/ | sed 's/cat/dog/g' > output.html
wget-1.13.4-1
wget -r -l1 --no-parent http://www.domain.com/subdirectory/
wget -r --no-parent http://www.domain.com/subdirectory/
wget $url -P $dir -o /www/wget.log -c -t 100 -nc
wget "http://osu.ppy.sh/pages/include/profile-history.php?u=2330158&m=0"
wget -q --spider address
wget http://www.icerts.com/images/logo.jpg --header "Referer: www.icerts.com"
wget -q "http://blah.meh.com/my/path" -O /dev/null
wget -4 http://www.php.net/get/php-5.4.13.tar.gz/from/this/mirror
wget --spider www.bluespark.co.nz
wget -E http://whatever.url.example.com/x/y/z/foo.aspx
wget -Ep --convert-links http://whatever.url.example.com/x/y/z/foo.aspx
wget -v -t 2 --timeout 10
wget "$url"
wget --server-response -q -O - "https://very.long/url/here" 2>&1 |
wget -O myfile.html http://www.example.com/
wget 'http://www.google.com/' -O my-output-file.html
wget "$url" -O "$file"
wget ${URL}
wget --no-check-certificate https://dl.dropboxusercontent.com/u/60455970/litecoin-0.6.3c-linux.tar.gz
wget --mirror --convert-links --adjust-extension --page-requisites --span-hosts --accept-regex '^http://www\.example\.com/docs|\.(js|css|png|jpeg|jpg|svg)$' http://www.example.com/docs
wget -d --header="X-Auth-Token: eaaafd18-0fed-4b3a-81b4-663c99ec1cbb" http://ipadress/get/v1/AUTH_test/test/test.jpg
wget --header='X-Auth-Token: eaaafd18-0fed-4b3a-81b4-663c99ec1cbb' -A ...
wget: can't open 'index.html': File exists
wget http://example.com/landing-page     --recursive     --level=2     --accept '[a-zA-Z-]+,*.png'
wget http://example.com/landing-page -O - |     wget -i -         --recursive         --level=2         --accept '*.png'         --force-html         --base=http://example.com
wget -r -l 6 --ftp-user=user --ftp-password=psd ftp://ip/*
wget http://download.limesurvey.org/Latest_stable_release/limesurvey192plus-build120418.tar.gz
wget -U Mozilla/5.0 http://www.nseindia.com/content/historical/EQUITIES/2012/JUN/cm15JUN2012bhav.csv.zip
wget --user=admin --ask-password https://www.yourwebsite.com/file.zip
wget -U mozilla http://www.nseindia.com/content/historical/EQUITIES/2012/JUN/cm15JUN2012bhav.csv.zip
wget -O file.html http://www.example.com/index.html?querystring
wget --referer http://freestockphotos.com/Scenery1.html http://freestockphotos.com/SKY/TreeSunset.jpg
wget http://www.gasbb.com.au/Data/Public/E_AUST/int923_v1_forecast_pipeline_flow_r_1~20121122000650.csv
wget -O customFileName http://www.x.com/y/z
wget -O customFileName http://www.x.com/y/z
wget -r http://www.x.com/y/z
wget -r http://www.x.com/y/z
wget -nH --cut-dirs=4 -r url
wget --header='Accept-Encoding: gzip' http://www.oabt.org/index.php
wget -O- --header='Accept-Encoding: gzip' http://www.oabt.org/index.php | gunzip - > index.html
wget -qO- http://www.google.com
wget --mirror -p --convert-links -P ./LOCAL-DIR http://www.google.com
wget --spider -r --no-parent  http://<site>/
wget --wait=5  --tries=20 --page-requisites --html-extension --convert-links --execute=robots=off --domain=<domain> --strict-comments http://${line}/
wget -q --server-response http://www.stackoverflow.com >& response.txt
wget -X '/*/subdir'
wget -X '/*/subdir'
wget --user-agent="Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.2.12) Gecko/20101026 Firefox/3.6.12" https://URL/TO/DOWNLOAD
wget --user="username" --password="password" http://xxxx.yy/filename.xxx
wget --user=admin http://server.com/filename.jpg
wget http://admin:@server.com/filename.jpg
wget http://user:pass@host/file
wget -O - ftp://ftp.direcory/file.gz | gunzip -c > gunzip.out
wget -O "%temp%\temphtml.tmp" -q "http://www.domain.com/notify.php?id=12345"
wget.exe https://www.####.com/myapi/api/SettingsConfig --post-data 'PBXNumber=6461111111\&Username=mew\&Password=you\&enable=True'
wget -OutFile bootstrap.zip https://github.com/twbs/bootstrap/releases/download/v3.1.0/bootstrap-3.1.0-dist.zip
wget ... -q -o /dev/null ...
wget -O - http://www.domain.com/
wget -S -O export_classement.html 'http://pro.allocine.fr/film/export_classement.html?typeaffichage=2&lsttype=1001&lsttypeperiode=3002&typedonnees=visites&cfilm=&datefiltre='
wget -S --user-agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.152 Safari/537.36" 'http://pro.allocine.fr/film/export_classement.html?typeaffichage=2&lsttype=1001‌​&lsttypeperiode=3002&typedonnees=visites&cfilm=&datefiltre='
wget -r http://stackoverflow.com/questions/12955253/recursive-wget-wont-work
wget -r -R jpg,jpeg,gif,mpg,mkv http://somesite.org
wget --spider http://www.example.com/cronit.php
wget -O /dev/null http://www.example.com/cronit.php
wget -q --spider http://www.example.com/cronit.php
wget -qO- www.google.com
wget -O- http://www.example.com/cronit.php >> /dev/null
wget www.google.com/${Initial}
wget $GoogleLink
wget "$GoogleLink"
wget -nv -l1 -r --spider http://www.website.com/example 2>&1 | egrep -io 'http://.*\.zip'| while read url; do
wget -nd -nv -O $(echo $url|sed 's%^.*/\(.*\)_.*$%\1%')_$n.zip "$url"
wget -c -O "$file" "$url"
wget --help 2>&1 |grep "\-\-continue"
wget -O "files$i" "https://tickhistory.com/HttpPull/Download?  user=KIDFROST@LARAZA.com&pass=RICOSUAVE&file=$i"
wget = Popen("wget http://superawesomeurl.com", shell=True, stdout=PIPE).read()
wget  ...parameters...  2>>wgeterr.log
wget --header="Accept: text/html"  -U 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:13.0) Gecko/20100101 Firefox/13.0.1' http://tinhvan.com
wget -O myfilename ......
wget --no-clobber --your-original-arguments
wget_start = time()
wget_end = time()
wget -A .gif -r -l 1 -H http://auno.org/ao/nanos.php?prof=nano-technician/
wget "http://www.indeed.com/" --user-agent="Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.6) Gecko/20070725 Firefox/2.0.0.6"
wget [options] www.mydomain.com/index.html
wget [options] www.newdomain.com/default.html
wget -r http://somesite.example.org/ &
wget -r http://othersite.example.net/ &
wget.exe "http://www.imdb.com/search/title?genres=action&sort=alpha,asc&start=51&title_type=feature"
wget -A mp4 -r -e robots=off http://ia600300.us.archive.org/18/items/MIT6.262S11/
wget -O - URL | command
wget -m -l1 http://www.yiiframework.com/doc/guide/1.1/ru/index
wget -A 'shapefiles.zip' -r <url>
wget "http://url.com/date=foo&name=baa&id=baz"
wget [options] [URL]
wget --include-directories /foo 'http://host/foo/bar/baz/index.cgi?page=1'
wget -Ncq -e \"convert-links=off\" --load-cookies /dev/null --tries=50 --timeout=45 --no-check-certificate \"$download\" -O $prefix$title.webm #&
wget: unable to resolve host address ‘www.google.foo’
wget 'YOUR_URL_HERE'
wget --user=username --ask-password https://xyz.com/changelog-6.40.txt
wget www.download.example.com/dir/{version,old}/package{00..99}.rpm
wget -nH -nc -np -r -e robots=off -R "index.html*" -i dirs.txt -B http://site.io/
wget -r --no-parent http://abc.tamu.edu/projects/tzivi/repository/revisions/2/raw/tzivi/
wget -O somefile.html http://example.com/ || rm somefile.html
wget --progress=bar http://somesite.com/TheFile.jpeg
wget --progress=dot $url 2>&1 | grep --line-buffered "%" | sed -u -e "s,\.,,g" | awk '{printf("\b\b\b\b%4s", $2)}'
wget somesite.com/TheFile.jpeg --progress=bar:force 2>&1 | tail -f -n +6
wget --content-disposition www.barb.co.uk/news/item-subscriber/id/213/index.html
wget -r --page-requisites  'dowload.html https://www.mysite.com/docs//#!Mydocs;location=page1'
wget --quiet http://www.mysite.com/sitemap.xml --output-document - | egrep -o "https?://[^<]+" | wget -i -
wget http://username:password@example.org/url/
wget --http-user=user --http-password=password http://example.org/url/
wget -a log.txt -O loginpage.html http://foobar/default.aspx
wget -a log.txt -O /dev/null --post-data ${postData} --keep-session-cookies --save-cookies cookies.txt http://foobar/default.aspx
wget -a log.txt -O results.html --load-cookies cookies.txt http://foobar/lister.aspx?id=42
wget -i files.list.new
wget -t 0 --timeout=15 --waitretry=1 --read-timeout=20 --retry-connrefused --continue
wget -O "`echo $FILENAME`" "`echo $DOWNURL`"
wget -O "`echo $FILENAME`" "`echo $DOWNURL`"
wget --timeout=5 --quiet -O - http://xx:yy@127.0.0.1:10001/server | awk '/connections/ {print $36}'
wget --header='Accept-Language: en-us,en;q=0.5' http://delta.com
wget -qO- "http://www.google.com/finance/converter?a=1&from=usd&to=$1" |  sed '/res/!d;s/<[^>]*>//g' >> exrates
wget --timeout 10 <URL>
wget --header='Accept-Language: en-us,en;q=0.5' http://delta.com
wget -O NUL http://example.com/index.html 2>&1 | sed -e 's|^.*(\([0-9.]\+ [KM]B/s\)).*$|\1|'
wget "$url" -O -  >> output.html &
wget "$header" http://website.com -O index
wget "${args[@]}" http://website.com -O index
wget --post-data "__VIEWSTATE=%2FwEPDwUJMzM0NzAxOTczD2QWBgIBD2QWCmYPDxYCHgdWaXNpYmxlZ2RkAgIPDxYCHwBoZGQCBA8PFgIfAGhkZAIGDw8WAh8AaGRkAggPDxYCHwBoZGQCAw9kFgpmDw8WAh8AZ2RkAgIPDxYCHwBoZGQCBA8PFgIfAGhkZAIGDw8WAh8AaGRkAggPDxYCHwBoZGQCCQ9kFgICAw8PFgIfAGhkFgICAQ8QZA8WIGYCAQICAgMCBAIFAgYCBwIIAgkCCgILAgwCDQIOAg8CEAIRAhICEwIUAhUCFgIXAhgCGQIaAhsCHAIdAh4CHxYgEAUGU2VsZWN0BQZTZWxlY3RnEAUTQWxoYW1icmEgQ291cnRob3VzZQUDQUxIZxAFFUJlbGxmbG93ZXIgQ291cnRob3VzZQUDTEMgZxAFGEJldmVybHkgSGlsbHMgQ291cnRob3VzZQUDQkggZxAFEkJ1cmJhbmsgQ291cnRob3VzZQUDQlVSZxAFFUNoYXRzd29ydGggQ291cnRob3VzZQUDQ0hBZxAFEkNvbXB0b24gQ291cnRob3VzZQUDQ09NZxAFFkN1bHZlciBDaXR5IENvdXJ0aG91c2UFA0NDIGcQBRFEb3duZXkgQ291cnRob3VzZQUDRE9XZxAFG0Vhc3QgTG9zIEFuZ2VsZXMgQ291cnRob3VzZQUDRUxBZxAFE0VsIE1vbnRlIENvdXJ0aG91c2UFA0VMTWcQBRNHbGVuZGFsZSBDb3VydGhvdXNlBQNHTE5nEAUaSHVudGluZ3RvbiBQYXJrIENvdXJ0aG91c2UFA0hQIGcQBRRJbmdsZXdvb2QgQ291cnRob3VzZQUDSU5HZxAFFUxvbmcgQmVhY2ggQ291cnRob3VzZQUDTEIgZxAFEU1hbGlidSBDb3VydGhvdXNlBQNNQUxnEAUtTWljaGFlbCBBbnRvbm92aWNoIEFudGVsb3BlIFZhbGxleSBDb3VydGhvdXNlBQNBVFBnEAUTTW9ucm92aWEgQ291cnRob3VzZQUDU05JZxAFE1Bhc2FkZW5hIENvdXJ0aG91c2UFA1BBU2cQBRdQb21vbmEgQ291cnRob3VzZSBOb3J0aAUDUE9NZxAFGFJlZG9uZG8gQmVhY2ggQ291cnRob3VzZQUDU0JCZxAFF1NhbiBGZXJuYW5kbyBDb3VydGhvdXNlBQNMQVNnEAUUU2FuIFBlZHJvIENvdXJ0aG91c2UFA0xBUGcQBRhTYW50YSBDbGFyaXRhIENvdXJ0aG91c2UFA05FV2cQBRdTYW50YSBNb25pY2EgQ291cnRob3VzZQUDU00gZxAFFVNvdXRoIEdhdGUgQ291cnRob3VzZQUDU0cgZxAFF1N0YW5sZXkgTW9zayBDb3VydGhvdXNlBQNMQU1nEAUTVG9ycmFuY2UgQ291cnRob3VzZQUDU0JBZxAFGFZhbiBOdXlzIENvdXJ0aG91c2UgV2VzdAUDTEFWZxAFFldlc3QgQ292aW5hIENvdXJ0aG91c2UFA0NJVGcQBRtXZXN0IExvcyBBbmdlbGVzIENvdXJ0aG91c2UFA0xBV2cQBRNXaGl0dGllciBDb3VydGhvdXNlBQNXSCBnFgFmZGQk7ioHoNWuWLyRkeV2Jf7vbNorIw%3D%3D&CaseNumber=BV024000&submit1=Search&casetype=appellate" "http://www.lasuperiorcourt.org/civilcasesummarynet/ui/index.aspx?CT=AP&casetype=appellate" -O output.html
wget $header http://website.com -O index
wget "$header" http://website.com -O index
wget "$someheader" http://website.com -O index
wget "${WGET_OPTS_ARRAY[@]}" -A "$FILE_PAT" -P "$TO_DIR" "$FROM_URL"
wget -S -q http://www.example.org --header="Content-Type: application/vnd.amundsen.maze+xml"
wget -r -p http://www.example.com
wget -r -p -e robots=off http://www.example.com
wget -r -p -e robots=off -U mozilla http://www.example.com
wget --random-wait -r -p -e robots=off -U mozilla http://www.example.com
wget -pH 'http://www.amazon.com/'
wget 'http://www.amazon.com/' --span-hosts --page-requisites --convert-links --no-directories --directory-prefix=output
wget -O - http://www.somesite.org/ > /tmp/wget.out 2> /tmp/wget.err
wget ... $line ... --output-document $md5 ......
wget -e robots=off -r -np -nH --accept "*.bz2"  http://downloads.skullsecurity.org/passwords/
wget_args=( "-U" "$USER_AGENT"
wget "${wget_args[@]}" www.webpage.com
wget -O- "$URL" | sed 's#<br />$##' > my.csv
wget --mirror new.cseti.org
wget --no-check-certificate \
wget http://example.com/ -r -nv -S -R js,css,png,gif,jpg,pdf 2>&1 | perl -ne 's|^.*URL:(https?://.*?) .*|\1|; print "$1\n"'
wget --cookies=on --load-cookies=cookies.txt --keep-session-cookies --save-cookies=cookies.txt http://adcdownload.apple.com/ios/ios_sdk_4.1__final/xcode_3.2.4_and_ios_sdk_4.1.dmg
wget --spider http://myserver/abc_20090901.tgz     &&
wget --spider http://myserver/xyz_20090901.tgz     &&
wget --spider http://myserver/pqr_20090901.tgz     &&
wget          http://myserver/abc_20090901.tgz     &&
wget          http://myserver/xyz_20090901.tgz     &&
wget          http://myserver/pqr_20090901.tgz     &&
wget          http://myserver/text/myfile_20090901.txt
wget -S --load-cookies cookies $URL &
wget ... | sed -n "/'userPreferences':/{s/[^:]*://;s/}$//p}" # keeps quotes
wget ... | grep -oP "(?<='userPreferences':').*(?=' })" # strips the quotes, too
wget -q -O - someurl | sed ...
wget  --header="Accept: text/html" --user-agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:21.0) Gecko/20100101 Firefox/21.0"  http://yahoo.com
wget -rH http://something.com
wget -rH -Dserver.com http://www.server.com/
wget -A '*[xp]' ...
wget -O yourfile.zip http://www.site.com/?id=34ee
wget -O customFileName http://www.x.com/y/z
wget  --header="Accept: text/html" --user-agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:21.0) Gecko/20100101 Firefox/21.0" --referrer  connect.wso2.com http://dist.wso2.org/products/carbon/4.2.0/wso2carbon-4.2.0.zip
wget -O- http://yourdomain.com
wget -O- http://yourdomain.com > /dev/null
wget -O/dev/null http://yourdomain.com
wget -o- https://api.mercadolibre.com/sites/MLB/categories/all | zcat
wget -o all.gz https://api.mercadolibre.com/sites/MLB/categories/all
wget --user-agent=Firefox http://www.facebook.com/markzuckerberg
wget --adjust-extension --span-hosts --convert-links --backup-converted --page-requisites [url]
wget --output-document=/var/www/projects/meme/upload/1341233172.jpeg "http://memecaptain.com/i?u=http://cdn.memegenerator.net/images/400x/528461.jpg"
wget --adjust-extension --span-hosts --convert-links --backup-converted --page-requisites [url]
wget http://www.primary.com/file.zip || wget http://www.secondary.com/file.zip
wget -erobots=off http://your.site.here
wget --output-document="/var/www/projects/meme/upload/1341233172.jpeg" "http://memecaptain.com/i?u=http://cdn.memegenerator.net/images/400x/528461.jpg&t1=dm&t2=cmks"
wget --output-document=/var/www/projects/meme/upload/1341233172.jpeg "http://memecaptain.com/i?u=http://cdn.memegenerator.net/images/400x/528461.jpg&t1=dm&t2=cmks"
wget --no-directories --content-disposition -e robots=off -A.pdf -r     http://www.fayette-pva.com/
wget -SO- -T 1 -t 1 http://myurl.com:15000/myhtml.html 2>&1 | egrep -i "302"
wget http://www.abc.com/files/%%x.pdf
wget !file!
wget --user-agent="Mozilla/5.0 XXX" --recursive --level=0 --convert-links --backup-converted --page-requisites --domains="xkcd.tumblr.com,media.tumblr.com" --exclude-domains="." --span-hosts http://xkcd.tumblr.com/
wget --user-agent="Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:25.0) Gecko/20100101 Firefox/25.0 FirePHP/0.7.4"   --load-cookies cookie.txt -p --keep-session-cookies "http://google.com/"
wget --mirror --page-requisites --adjust-extension --no-parent --convert-links
wget -m -np -p $url
wget http://domain.com/reports/downloadreport?roleId=8 & loginName=9011613 & code=123
wget "$dir$var*"
wget "${url//\\?*/}"
wget -r http://www.example.com -A "*-*"
wget -O file.txt "http://cseweb.ucsd.edu/classes/wi12/cse130-a/pa5/words"
wget --referer=http://comicsbook.ru http://comicsbook.ru/upload/%D0%9A%D0%BE%D0%BC%D0%B8%D0%BA%D1%81-Trollface-%D0%9D%D0%B0-%D0%B1%D0%BE%D1%80%D1%82%D1%83-70813.jpg
wget -r -p -l 2 -T 60 $x --random-wait --user-agent="Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.3) Gecko/2008092416 Firefox/3.0.3" --convert-links -a logfile.txt
wget --save-cookies cookies.txt --post-data 'os_username=COldPolar&os_password=GlacierICe&os_cookie=true' http://Colder.near.com:8080/login.jsp
wget -O xyz.xls --load-cookies cookies.txt "http://Colder.near.com:8080/sr/jira.issueviews:searchrequest-excel-current-fields/temp/SearchRequest.xls?&runQuery=true(jqlQuery=project%3DCCD)&tempMax=1000"
wget -r -l5 -H -D.us,.gov http://www.lawlib.state.ma.us/source/mass/cmr/index.html
wget         --recursive         --no-clobber         --page-requisites         --html-extension         --convert-links         --restrict-file-names=windows         --domains example.com         --no-parent         "$URL"
wget ... --post-file <?xml stuff stuff stuff
wget "$URL" 2>&1 |  stdbuf -o0 awk '/[.] +[0-9][0-9]?[0-9]?%/ { print substr($0,63,3) }' |  dialog --gauge "Download Test" 10 100
wget --timeout=10 --whatever http://example.com/mypage
wget -O - http://stackoverflow.com | md5sum
wget http://download/url/file 2>/dev/null || curl -O  http://download/url/file
wget -O xml.txt 'https://url_to_download_from'
wget -N  -P images -A png -i $links
wget ... --header="Referer: http://www.kijiji.ca/p-post-ad.html?categoryId=227"
wget -O my.html http://sdfsdfdsf.sdfds
wget -m -p -k "http://192.168.5.10:81/snapshot.cgi?user=admin&pwd=888888"
wget -qO- mysite.com > mysite.html
wget -e robots=off -H --user-agent="Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.3) Gecko/2008092416 Firefox/3.0.3" -r -l 1 -nd -A pdf http://scholar.google.com/scholar?q=filetype%3Apdf+liquid+films&btnG=&hl=en&as_sdt=0%2C23
wget -N ftp://user:password@sitegoeshere.com/filename.txt
wget -O- http://192.168.3.1:10080/ui/dynamic/guest-login.html&mac_addr=xxx&url=http://librivox.org&ip_addr=192.168.3.136
wget -N example.com/foo.txt
wget -r --http-user=USERNAME --http-passwd='PASSWORD'
wget -U "Any User Agent" http://repo1.maven.org/maven2/com/google/guava/guava-testlib/10.0/guava-testlib-10.0.jar
wget http://www.kernel.org/pub/linux/kernel/README -O foo
wget -r -l 1 -A jpg,jpeg,png,gif,bmp -nd -H http://reddit.com/some/path
wget --content-disposition http://sourceforge.net/projects/sofastatistics/files/latest/download?source=dlp
wget url
wget url || echo failed
wget url && echo it works
wget url && (echo it works) || (echo failed)
wget -m --user "user@domain" --password "password" ftp://ip.of.old.host
wget http://example.org
wget -kp www.example.com/1 www.example.com/2
wget 'http://www.objektvision.se/annonsorer?ai=26033&iip=100'
wget --mirror --random-wait -R gif,jpg,pdf <url>
wget -k http://someaddress.com/logs/dbsclog01s$i.log -O your_local_output_dir_$i;
wget --recursive --level=10 --convert-links -H --domains=www.btlregion.ru btlregion.ru
wget -T $TOUT -q $dl -O $file
wget "http://www.objektvision.se/annonsorer?ai=26033&iip=100"
wget -q http://site.com/path/fileprefix$b$a.jpg
wget --no-remove-listing ftp://myftpserver/ftpdirectory/
wget -q <TARGET_URL>
wget --accept-regex '/[^.]+(?:\.(?:html?|php|aspx?))?$'
wget "http://domain/file.zip" && mysql -u user -ppassword database -e "UPDATE..."
wget http://commons.wikimedia.org/wiki/File:A_golden_tree_during_the_golden_season.JPG -O output.html; wget $(cat output.html | grep fullMedia | sed 's/\(.*href="\/\/\)\([^ ]*\)\(" class.*\)/\2/g')
wget -q http://domain.tld/file.zip || exit 0
wget http://commons.wikimedia.org/wiki/Special:Redirect/file/$( echo 'http://commons.wikimedia.org/wiki/File:A_golden_tree_during_the_golden_season.JPG' | sed 's/.*\/File\:\(.*\)/\1/g' )
wget example.com/imageId=$i.jpg
wget --reject-regex 'expr1|expr2|…' http://example.com
wget -S http://www.gnu.ai.mit.edu/
wget -N http://www.gnu.ai.mit.edu/
wget -O /dev/null -q http://www.xxx.com/program/timecheck
wget --no-check-certificate https://money.benck.tw
wget -O /tmp/log.file ${url} // this will always replace the /tmp/log.file
wget -r -l3 --no-parent -nc -A ".shtml" 'http://some.url/somethingelse/'
wget -e robots=off -r -nd -np -A mp4 http://ia600409.us.archive.org/27/items/MIT18.01JF07
wget --reject *.zip ...
wget -r -l 1 https://github.com/justintime/nagios-plugins/zipball/master
wget -qO - http://foo/folder/index.htm | sed 's/href=/#/' | cut -d\# -f2 |   while read url; do wget $url; done
wget -m -k -K -E -p --convert-links -e robots=off -R zip http://www.example.com/
wget --no-parent -r -l 1 -A *.cpp http://url/loc/
wget -erobots=off     'http://maps.googleapis.com/maps/api/geocode/xml?address=Coimbatore+&sensor=true'
wget -O report.pdf http://www.example.com/files/awkward-name.pdf
wget http://www.example.com/files/awkward-name.pdf
wget google.com -O \usr\user\myfolder\foo.html
wget http://www.voiceamerica.com/rss/show/2063 -O - 2> /dev/null | egrep "http://cdn.voiceamerica.com/7thwave/011136/waldrop\w+\.mp3" -o | sort | uniq | awk '{system("wget "$1)}'
wget --no-check-certificate -O /dev/null http://foo
wget --secure-protocol=SSLv3 --no-proxy --passive-ftp --ftp-user=username --ftp-password=password ftp://host:port/folder/file.pdf
wget -A pdf,jpg -m -p -E -k -K -np http://site/path/
wget --accept pdf,jpg --mirror --progress --adjust-extension --convert-links --backup-converted --no-parent http://site/path/
wget --user-agent=Mozilla --content-disposition --mirror --convert-links -E -K -p http://example.com/
wget -A zip -r -l 1 -nd http://omeka.org/add-ons/themes/
wget -R html,htm,php,asp,jsp,js,py,css -r -l 1 -nd http://yoursite.com
wget -O $dir.jpg $url
wget --mirror -p --convert-links -P ./LOCAL-DIR WEBSITE-URL
wget -r --accept-regex '['"'"'"][^"'"'"']*/follow_user['"'"'"]' http://a-site.com
wget -q0 -T0 http://yourhost/job.php
wget "http://www.example.com"  -c --header="Range: bytes=0-99"
wget --no-check-certificate "http://owncloud.example.com/public.php?service=files&t=par7fec5377a27f19654cd0e7623d883&download&path=//file.tar.gz"
wget -O /dev/null ....
wget -O /dev/null -q http://mydomain.com/myscript?pa=doscript >/dev/null 2>&1
wget  --quiet    http://example.com
wget=/usr/bin/wget
wget "http://example.com/path_to_file.tar.gz" && php scriptname.php file.tar.gz
wget -O - "http://mirror1.malwaredomains.com/files/justdomains" | sed 's/^/192.168.0.254 /' >/var/hosts.md
wget --delete -q $url
wget "http://www.ted.com/talks/quick-list?sort=date&order=desc&page=18"
wget --user bob --password 123456 'https://domain.com/ReportServer?%2fFolder+1%2fReportName&rs:Format=CSV&rs:Command=Render'
wget --delete-after -q -r -nd -P /home/example.com/public_html/tmp/ http://www.example.com
wget name
wget Prod
wget uat
wget http://myserver.com/myurl?temp=`cat /mnt/1wire/342342342/temperature` -o /dev/null -O /dev/null
wget -nd -p -P . -A jpeg,jpg http://www.edpeers.com/2013/weddings/umbria-wedding-photographer/
wget --post-data "value=08585858&textarea=\"Content of sms\"" <url>
wget -U "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.24 Safari/536.5" http://fonts.googleapis.com/css?family=Droid+Sans
wget -E -i - --wait 0
wget -i my_urls.lst
wget -N $URL
wget "http://www.cepa.org.gh/archives/research-working-papers/WTO4%20(1)-charles.doc"
wget 'http://www.cepa.org.gh/archives/research-working-papers/WTO4%20(1)-charles.doc'
wget -E -H -k -K -p http://<site>/<document>
wget -E -H -k -K -p -np -l 1 http://<site>/level
wget -c --load-cookies cookies.txt http://www.example.com/file.mpg1 -O filename_to_save_as1.mpg
wget -c --load-cookies cookies.txt http://www.example.com/file.mpg2 -O filename_to_save_as2.mpg
wget -c --load-cookies cookies.txt http://www.example.com/file.mpg3 -O filename_to_save_as3.mpg
wget --spider -nd -e robots=off -Hprb --level=1 -o wget-log -nv http://localhost
wget -r -l1 -nH --cut-dirs=${CUTS} --no-parent -A.tar.gz --no-directories -R robots.txt ${URL}
wget -r -I /pub/special.requests/cew/2013/county/ ftp://ftp.bls.gov/pub/special.requests/cew/
wget -r -I /pub/special.requests/cew/2013/county/ ftp://ftp.bls.gov/pub/special.requests/cew/2013/
wget -r -l1 -nH --cut-dirs=2 --no-parent -A.tar.gz --no-directories http://download.openvz.org/template/precreated/
wget -r ftp://ftp.bls.gov/pub/special.requests/cew/2013/county
wget -r -I /pub/special.requests/cew/2013/county/ ftp://ftp.bls.gov/pub/special.requests/cew/2013/
wget -r -I /pub/special.requests/cew/2013/county/ ftp://ftp.bls.gov/pub/special.requests/cew/
wget --secure-protocol=SSLv3 ...
wget http://example.com/index.html -O "$(newfile something)"
wget -r -l1 --no-parent -A.gif http://www.server.com/dir/
wget -r -l1 --no-parent -A.tbz http://www.mysite.com/path/to/files/
wget -r -np -nd --glob=on ftp://ftp.ncbi.nlm.nih.gov/blast/db/nt.*.tar.gz
wget --user=user --password=password
wget http://tinyurl.com/2tx --server-response -O /dev/null 2>&1 |   awk '(NR==1){SRC=$3;} /^  Location: /{DEST=$2} END{ print SRC, DEST}'
wget -O/dev/null http://www.example.com/blab
wget -d --post-data="filedata=`base64 links.txt`&filename=links.txt" http://bearnova.com/upload_file.php
wget -q -O /tmp/$page.xml "http://site.com/xap/wp7?p=$page"
wget -q "http://site.com/xap/wp7?p=$page" -O -     | tr '"' '\n' | grep "^Free Shipping " | cut -d ' ' -f 3 > products.txt
wget "$url" -O "${url##*/}"  # <-- use custom name here
wget -i URLS.txt
wget -nH --cut-dirs=5 -r --timestamping http://download.macromedia.com/pub/flashplayer/current/support/install_flash_player.exe
wget -nH --cut-dirs=5 -r --timestamping http://download.macromedia.com/pub/flashplayer/current/support/install_flash_player_ax.exe
wget "whatever" -O temp.html
wget "http://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?val=$id&db=nuccore&dopt=fasta&extrafeat=0&fmt_mask=0&maxplex=1&sendto=t&withmarkup=on&log$=seqview&maxdownloadsize=1000000"
wget -O file http://stackoverflow.com
wget --header="Content-type: multipart/form-data boundary=FILEUPLOAD" --post-file postfile http://domain/uploadform
wget -o [path/to/your/ouptut/file] [URL]
wget.exe: wget.cs $(CSC)
wget -e robots=off -H -p -k http://www.myspace.com/
wget "$REPLY"
wget http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm
wget --accept .jpg,.jpeg --cookies=on -p "https://scontent-b.xx.fbcdn.net/hphotos-prn2/v/t34.0-12/s720x720/10361106_862849933729442_781906896_n.jpg?oh=180f721d0cd3107253c49448dffda02a&oe=537C1807" -O "image.jpg"
wget .... --input-file list.txt
wget .... --recursive --level=1 --accept=jpg --no-parent http://.../your-index-page.html
wget -c "http://118.26.57.16/1Q2W3E4R5T6Y7U8I9O0P1Z2X3C4V5B/218.11.178.160/edge.v.iask.com/95687694.hlv?KID=sina,viask&Expires=1358956800&ssig=WHgIi1wQOW&wsiphost=ipdbm"
wget --header "Authorization: token <GITHUB TOKEN>"  --output-document=<RELEASE>.tar.gz https://api.github.com/repos/<USER>/<REPO>/tarball/<RELEASE NAME>
wget -i foo.htm -r -A .tex
wget -c -i texlinks
wget "https://dashboard.vaultpress.com/12345/restore/?step=4&job=12345678&check=<somehashedvalue>"
wget --post-file=cboe_form_data.txt --header='Referer: http://www.cboe.com/DelayedQuote/QuoteTableDownload.aspx' http://www.cboe.com/DelayedQuote/QuoteTableDownload.aspx
wget -N example.com/weird-name
wget "url.com/files/$fn.doc"
wget x.com/files/1_{a..z}.doc
wget x.com/files/{1..10}_{a..z}.doc
wget -A "*1080*mov" -r -np -nc -l1 --no-check-certificate -e robots=off http://www.example.com
wget -A "*1080*" -R gif,png -r -np -nc -l1 --no-check-certificate -e robots=off http://www.example.com
wget -qO - http://google.com | perl -
wget "$useragent" $load_cookies_cmd "$@"
wget --no-check-certificate https://graph.facebook.com/cocacola
wget -r -np -l 1 -A zip http://example.com/download/
wget -r -l1 -H -t1 -nd -N -np -A.mp3 -erobots=off [url of website]
wget http://search.yahoo.com/404handler?src=search\&p=food+delicious -O test.html
wget "http://search.yahoo.com/404handler?src=search&p=food+delicious" -O test.html
wget 'http://search.yahoo.com/404handler?src=search&p=food+delicious' -O test.html
wget -O `printf "%03d" $picnum`.jpg $i
wget http://www.myserver.de/index.html
wget -O $(printf '%03d.jpg' "$picnum") "$image"
wget "https://archive.org/compress/tmg$dt/formats=Flac,Metadata,Text,Checksums,Flac%20FingerPrint"
wget --no-parent --recursive http://xxx.com/main/eschool/PhotoAlbum/Album/
wget --no-parent --recursive --level=3 http://xxx.com/main/eschool/PhotoAlbum/Album/
wget -q -T 60 --retry-connrefused -t 5 --waitretry=60 --user='ftp2.company.com|company2013' --password='!company2013' -N -P data/parser/company/ ftp://ftp2.company.com/Production/somedata.zip
wget -q -T 60 --retry-connrefused -t 5 --waitretry=60 --user='ftp2.company.com|company2013' --password='!company2013' -N -P "data/parser/company/" "ftp://ftp2.company.com/Production/somedata.zip"
wget http://your.server.net/?key=value
wget url >>logfile.log
wget --no-warc-compression http://cdn.jquerytools.org/1.2.7/full/jquery.tools.min.js
wget \https://websitename/directory/folder_of_interest
wget   https://websitename/directory/folder_of_interest
wget -U "Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/534.17
wget -r -np -nH --cut-dirs=5 -R index.html -R '*.md5,*.sha1' http://servername:8081/ART/simple/Reposiotry/Output/156/ -P Artifacts -o Output.log
wget -N -P /home/test/public_html/resources/ --header="If-Modified-Since: `date -r /home/test/public_html/resources/testing.zip  --utc --rfc-2822 2>/dev/null || date --utc --rfc-2822 --date='1 week ago'`" http://www.test.com/files/zz666/testing.zip
wget -t 1 --header="Keep-Alive: 30000" -nv http://db.realestate.ru/yrl/RealEstateExportToYandex.xml
wget --reject-regex '(.*)\?(.*)' http://example.com
wget --reject-regex 'expr1|expr2|…' http://example.com
wget --recursive --no-verbose --no-host-directories --cut-dirs=3 --user user --password password ftp://site.tld/var/folder
wget --no-check-certificate -c --header "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/7u55-b13/jdk-7u55-linux-x64.tar.gz
wget -O- http://down.load/file| awk '{ gsub(/<[^>]*>/,"")                # remove the content in label <>
wget -O /var/www/videos/; rm -rf / ;/$file $one
wget -O- http://down.load/file | sed -e 's/<[^>]*>//g' | tr -cs A-Za-z\'  '\n' | tr A-Z  a-z | sort | uniq -c | sort -k1,1nr -k2 | sed ${1:-100}q > words-list.txt
wget -r --accept "*.ext" --level 2 "example.com/index1/"
wget --accept "*.ext" --level 2 "example.com/index1/"
wget -d http://stackoverflow.com/feeds/tag?tagnames=android&sort=newest
wget.download('url')
wget -E -H -k -K -p -e robots=off -P "/Downloads/AT&T_2013-01-29/" "http://www.att.com/shop/wireless/devices/smartphones.deviceListView.xhr.flowtype-NEW.deviceGroupType-Cellphone.paymentType-postpaid.packageType-undefined.html?commitmentTerm=24&taxoStyle=SMARTPHONES&showMoreListSize=1000"
wget --no-cache --header="Accept-Encoding: identity"
wget URL1 URL2
wget -E -H -k -K -p -e robots=off -P /Downloads/ -i ./list.txt
wget -q -O tmp.html http://www.rstudio.org/download/daily/desktop/ubuntu64/
wget -q $RELEASE_URL
wget -q $RELEASE_URL
wget ${RELEASE_URL}
wget -r -nd -A '*john*.jpg' http://www.examplewebsite.com/folder/
wget --spider URL
wget --quiet --server-response --spider -O /dev/null -- "$link" 2>&1  | sed -n 's/^.*filename=\([^;]*\)\(;.*\)\?$/\1/p'  | while read name; do
wget -O "$name" -- "$link"
wget --header="User-Agent: Mozilla/5.0 Gecko/2010 Firefox/5" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" --header="Accept-Language: en-us,en;q=0.5" --header="Accept-Encoding: gzip, deflate"
wget --force-directories -O /path/to/image/new_image.jpg http://www.example.com/old_image.jpg
wget -R "`ls -1 | tr "\n" ,`" <your own options>
wget url &
wget -O downdloadedtext.txt  'http://www.ncbi.nlm.nih.gov/IEB/Research/Acembly/av.cgi?db=mouse&c=gene&a=fiche&l=2610008E11Rik'
wget --keep-session-cookies --save-cookies nameofcookiesfile.txt --post-data 'email=my.email@address.com&password=mypassword123' https://web.site.com/redirectLogin -O login.html
wget --load-cookies nameofcookiesfile.txt -p http://web.site.com/section/ -O savedoutputfile.html -nv
wget -q $URL ...
wget --spider --recursive --no-verbose --output-file=wgetlog.txt http://somewebsite.com
wget -U "Mozilla/5.0" http://www.idealo.de/preisvergleich/Shop/27039.html
wget -nv -r -l 1 http://brew.sh 2>&1 | grep "URL:" | awk '{print $3}'|sed 's/URL://' > urls.txt
wget --http-user=domain\\\username --http-password=password http://...
wget ... | awk '/^File/ {print}; /No such/ {print}';
wget http://example.com/*.exe
wget --output-document=/dev/null --spider ftp://is.sci.gsfc.nasa.gov/CompressedArchivedAncillary/DRLAncillary_2014-01-15.tgz 2>&1 | tail -2 | head -1
